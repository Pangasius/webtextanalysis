{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will use an RNN architecture to build a Machine Translation model.\n",
    "\n",
    "It will use as a corpus wikipedia dumps.\n",
    "\n",
    "Either the source or the target will be English. We will, in our case, try English to French Translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "!pip3 install numpy\n",
    "!pip3 install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cu117\n",
    "#or any nightly version so long as pytorch > 1.11 https://pytorch.org/\n",
    "!pip3 install nltk gensim transformers d2l==1.0.0a1.post0\n",
    "\n",
    "#In pytorch functional.py, change PILLOW_VERSION to __version__\n",
    "#there are two places to change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go.\tVa !\n",
      "Hi.\tSalut !\n",
      "Run!\tCours !\n",
      "Run!\tCourez !\n",
      "Who?\tQui ?\n",
      "Wow!\tÇa alors !\n",
      "\n",
      "go .\tva !\n",
      "hi .\tsalut !\n",
      "run !\tcours !\n",
      "run !\tcourez !\n",
      "who ?\tqui ?\n",
      "wow !\tça alors !\n",
      "source: tensor([[ 84,   5,   2,   3,   4,   4,   4,   4,   4],\n",
      "        [ 84,  61,  91,   2,   3,   4,   4,   4,   4],\n",
      "        [161,   0,   3,   4,   4,   4,   4,   4,   4]], dtype=torch.int32)\n",
      "decoder input: tensor([[  3, 108,   6,   2,   4,   5,   5,   5,   5],\n",
      "        [  3, 105,  51,   2,   4,   5,   5,   5,   5],\n",
      "        [  3,   6,   0,   4,   5,   5,   5,   5,   5]], dtype=torch.int32)\n",
      "source len excluding pad: tensor([4, 5, 3], dtype=torch.int32)\n",
      "label: tensor([[108,   6,   2,   4,   5,   5,   5,   5,   5],\n",
      "        [105,  51,   2,   4,   5,   5,   5,   5,   5],\n",
      "        [  6,   0,   4,   5,   5,   5,   5,   5,   5]], dtype=torch.int32)\n",
      "source: ['hi', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "target: ['<bos>', 'salut', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "#Test samples location and preprocessing\n",
    "import os\n",
    "import torch\n",
    "from d2l import torch as d2l\n",
    "\n",
    "class MTFraEng(d2l.DataModule):  #@save\n",
    "    def _download(self):\n",
    "        d2l.extract(d2l.download(\n",
    "            d2l.DATA_URL+'fra-eng.zip', self.root,\n",
    "            '94646ad1522d915e7b0f9296181140edcf86a4f5'))\n",
    "        with open(self.root + '/fra-eng/fra.txt', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "\n",
    "data = MTFraEng()\n",
    "raw_text = data._download()\n",
    "print(raw_text[:75])\n",
    "\n",
    "@d2l.add_to_class(MTFraEng)  #@save\n",
    "def _preprocess(self, text):\n",
    "    # Replace non-breaking space with space\n",
    "    text = text.replace('\\u202f', ' ').replace('\\xa0', ' ')\n",
    "    # Insert space between words and punctuation marks\n",
    "    no_space = lambda char, prev_char: char in ',.!?' and prev_char != ' '\n",
    "    out = [' ' + char if i > 0 and no_space(char, text[i - 1]) else char\n",
    "           for i, char in enumerate(text.lower())]\n",
    "    return ''.join(out)\n",
    "\n",
    "text = data._preprocess(raw_text)\n",
    "print(text[:80])\n",
    "\n",
    "@d2l.add_to_class(MTFraEng)  #@save\n",
    "def _tokenize(self, text, max_examples=None):\n",
    "    src, tgt = [], []\n",
    "    for i, line in enumerate(text.split('\\n')):\n",
    "        if max_examples and i > max_examples: break\n",
    "        parts = line.split('\\t')\n",
    "        if len(parts) == 2:\n",
    "            # Skip empty tokens\n",
    "            src.append([t for t in f'{parts[0]} <eos>'.split(' ') if t])\n",
    "            tgt.append([t for t in f'{parts[1]} <eos>'.split(' ') if t])\n",
    "    return src, tgt\n",
    "\n",
    "src, tgt = data._tokenize(text)\n",
    "src[:6], tgt[:6]\n",
    "\n",
    "@d2l.add_to_class(MTFraEng)  #@save\n",
    "def __init__(self, batch_size, num_steps=9, num_train=512, num_val=128):\n",
    "    super(MTFraEng, self).__init__()\n",
    "    self.save_hyperparameters()\n",
    "    self.arrays, self.src_vocab, self.tgt_vocab = self._build_arrays(\n",
    "        self._download())\n",
    "\n",
    "\n",
    "@d2l.add_to_class(MTFraEng)  #@save\n",
    "def _build_arrays(self, raw_text, src_vocab=None, tgt_vocab=None):\n",
    "    def _build_array(sentences, vocab, is_tgt=False):\n",
    "        pad_or_trim = lambda seq, t: (\n",
    "            seq[:t] if len(seq) > t else seq + ['<pad>'] * (t - len(seq)))\n",
    "        sentences = [pad_or_trim(s, self.num_steps) for s in sentences]\n",
    "        if is_tgt:\n",
    "            sentences = [['<bos>'] + s for s in sentences]\n",
    "        if vocab is None:\n",
    "            vocab = d2l.Vocab(sentences, min_freq=2)\n",
    "        array = torch.tensor([vocab[s] for s in sentences])\n",
    "        valid_len = (array != vocab['<pad>']).type(torch.int32).sum(1)\n",
    "        return array, vocab, valid_len\n",
    "    src, tgt = self._tokenize(self._preprocess(raw_text),\n",
    "                              self.num_train + self.num_val)\n",
    "    src_array, src_vocab, src_valid_len = _build_array(src, src_vocab)\n",
    "    tgt_array, tgt_vocab, _ = _build_array(tgt, tgt_vocab, True)\n",
    "    return ((src_array, tgt_array[:,:-1], src_valid_len, tgt_array[:,1:]),\n",
    "            src_vocab, tgt_vocab)\n",
    "\n",
    "@d2l.add_to_class(MTFraEng)  #@save\n",
    "def get_dataloader(self, train):\n",
    "    idx = slice(0, self.num_train) if train else slice(self.num_train, None)\n",
    "    return self.get_tensorloader(self.arrays, train, idx)\n",
    "\n",
    "data = MTFraEng(batch_size=3)\n",
    "src, tgt, src_valid_len, label = next(iter(data.train_dataloader()))\n",
    "print('source:', src.type(torch.int32))\n",
    "print('decoder input:', tgt.type(torch.int32))\n",
    "print('source len excluding pad:', src_valid_len.type(torch.int32))\n",
    "print('label:', label.type(torch.int32))\n",
    "\n",
    "@d2l.add_to_class(MTFraEng)  #@save\n",
    "def build(self, src_sentences, tgt_sentences):\n",
    "    raw_text = '\\n'.join([src + '\\t' + tgt for src, tgt in zip(\n",
    "        src_sentences, tgt_sentences)])\n",
    "    arrays, _, _ = self._build_arrays(\n",
    "        raw_text, self.src_vocab, self.tgt_vocab)\n",
    "    return arrays\n",
    "\n",
    "src, tgt, _,  _ = data.build(['hi .'], ['salut .'])\n",
    "print('source:', data.src_vocab.to_tokens(src[0].type(torch.int32)))\n",
    "print('target:', data.tgt_vocab.to_tokens(tgt[0].type(torch.int32)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embedding\n",
    "\n",
    "# We will use three different types of word embeddings:\n",
    "# 1. Word2Vec\n",
    "# 2. GloVe\n",
    "# 3. FastText\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"## Word2Vec\"\"\"\n",
    "\n",
    "import io\n",
    "import os\n",
    "\n",
    "# Python program to generate word vectors using Word2Vec\n",
    " \n",
    "# importing all necessary modules\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import download\n",
    "import warnings\n",
    " \n",
    "warnings.filterwarnings(action = 'ignore')\n",
    " \n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gille\\AppData\\Local\\Temp\\ipykernel_9472\n",
      "c:\\Users\\gille\\OneDrive\\Desktop\\web\\webtextanalysis\n",
      "c:\\Users\\gille\\miniconda3\\envs\\thesis\\lib\\site-packages\\ipykernel_launcher.py\n"
     ]
    }
   ],
   "source": [
    "#keep in mind you have to launch the notebook inside the git folder to make this work (second one)\n",
    "from inspect import getsourcefile\n",
    "import sys\n",
    "print(os.path.dirname(getsourcefile(lambda:0)))\n",
    "print(sys.path[0])\n",
    "print(os.path.abspath(sys.argv[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to D:\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We need to download the punkt package for tokenizing sentences\n",
    "download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(path = sys.path[0] + \"\\\\samples\\\\alice.txt\"):\n",
    "    #  Reads ‘alice.txt’ file\n",
    "    with io.open(path, 'r',encoding='utf8') as sample :\n",
    "        s = sample.read()\n",
    "        \n",
    "        # Replaces escape character with space\n",
    "        f = s.replace(\"\\n\", \" \")\n",
    "        \n",
    "        data = []\n",
    "        \n",
    "        # iterate through each sentence in the file\n",
    "        for i in sent_tokenize(f):\n",
    "            temp = []\n",
    "            \n",
    "            # tokenize the sentence into words\n",
    "            for j in word_tokenize(i):\n",
    "                temp.append(j.lower())\n",
    "        \n",
    "            data.append(temp)\n",
    "\n",
    "    return data\n",
    "\n",
    "def test_similarity(model, word1, word2, model_name):\n",
    "    print(\"Cosine similarity between '\" + word1 + \"' and '\"+ word2 +\"' - \" + model_name + \" : \" + str(model.similarity(word1, word2)))\n",
    "    \n",
    "def Fast_similarity(model, word1, word2):\n",
    "    print(\"Cosine similarity between '\" + word1 + \"' and '\"+ word2 +\"' - \" + \"FastText\" + \" : \" + str(model.wv.similarity(word1, word2)))\n",
    "\n",
    "embedding_data = tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(sys.path[0] + \"\\\\samples\\\\alice_tokenised.txt\", 'w',encoding='utf8') as f:\n",
    "    for line in embedding_data:\n",
    "        f.write(\" \".join(line) + \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'alice' and 'wonderland' - CBOW : 0.9727886\n",
      "Cosine similarity between 'alice' and 'machines' - CBOW : 0.8968569\n",
      "Cosine similarity between 'alice' and 'wonderland' - SkipGram : 0.6006849\n",
      "Cosine similarity between 'alice' and 'machines' - SkipGram : 0.81720406\n"
     ]
    }
   ],
   "source": [
    "\"\"\"## Word2Vec\"\"\"\n",
    " \n",
    "# Create CBOW model\n",
    "w2v_model_cbow = gensim.models.Word2Vec(embedding_data, min_count = 1,\n",
    "                              vector_size = 100, window = 5)\n",
    " \n",
    "# Print results\n",
    "test_similarity(w2v_model_cbow.wv, 'alice', 'wonderland', \"CBOW\")\n",
    "     \n",
    "test_similarity(w2v_model_cbow.wv, 'alice', 'machines', \"CBOW\")\n",
    " \n",
    "# Create Skip Gram model\n",
    "w2v_model_skip = gensim.models.Word2Vec(embedding_data, min_count = 1, vector_size = 100,\n",
    "                                             window = 5, sg = 1)\n",
    " \n",
    "# Print results\n",
    "test_similarity(w2v_model_skip.wv, 'alice', 'wonderland', \"SkipGram\")\n",
    "     \n",
    "test_similarity(w2v_model_skip.wv, 'alice', 'machines', \"SkipGram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"## GloVe\"\"\"\n",
    "\n",
    "# coding: utf-8\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3390, 300)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Only do this once\n",
    "input_file = sys.path[0] + '\\\\models\\\\alice_glove.txt'\n",
    "output_file = sys.path[0] + '\\\\models\\\\gensim_alice_glove.txt'\n",
    "glove2word2vec(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#once we have the tokenized file, we can call the glove model\n",
    "\n",
    "####CALL FROM BASH glove_run.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model, can take a bit of time\n",
    "output_file = sys.path[0] + '\\\\models\\\\gensim_alice_glove.txt'\n",
    "glove_model = KeyedVectors.load_word2vec_format(output_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'alice' and 'wonderland' - GloVe : -0.18298031\n",
      "Cosine similarity between 'alice' and 'machines' - GloVe : -0.8092102\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test the model\n",
    "test_similarity(glove_model, 'alice', 'wonderland', \"GloVe\")\n",
    "test_similarity(glove_model, 'alice', 'machines', \"GloVe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'alice' and 'wonderland' - FastText : 0.18689592\n",
      "Cosine similarity between 'alice' and 'machine' - FastText : -0.12982063\n"
     ]
    }
   ],
   "source": [
    "\"\"\"## FastText\"\"\"\n",
    "from gensim.models import FastText\n",
    "from gensim.test.utils import common_texts  # some example sentences\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "model = FastText(vector_size=100, window=5, min_count=1)  # instantiate\n",
    "model.build_vocab(corpus_iterable=embedding_data)\n",
    "model.train(corpus_iterable=embedding_data, total_examples=len(embedding_data), epochs=100)  \n",
    "Fast_similarity(model,'alice','wonderland')\n",
    "Fast_similarity(model,'alice','machine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can create the RNN model that will translate from english to french using one of the previous embeddings\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From this model we can create a loss function and an optimizer\n",
    "\n",
    "def loss_function(tag_scores, gold_tags):\n",
    "    loss_function = nn.NLLLoss()\n",
    "    loss = loss_function(tag_scores, gold_tags)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can train the model\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = RNN(100, 128, 100, 100).to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "n_epoch = 100\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    for sentence, tags in data:\n",
    "        sentence = torch.tensor(sentence, dtype=torch.long).to(device)\n",
    "        tags = torch.tensor(tags, dtype=torch.long).to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        tag_scores = model(sentence)\n",
    "\n",
    "        loss = loss_function(tag_scores, tags)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    #here we can use the test data to evaluate the model\n",
    "    \n",
    "    losses = torch.zeros(len(test_data))\n",
    "    \n",
    "    for sentence, tags in test_data:\n",
    "        sentence = torch.tensor(sentence, dtype=torch.long).to(device)\n",
    "        tags = torch.tensor(tags, dtype=torch.long).to(device)\n",
    "\n",
    "        tag_scores = model(sentence)\n",
    "\n",
    "        loss = loss_function(tag_scores, tags)\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "    print(\"Epoch \" + str(epoch) + \" : \" + str(losses.mean()))\n",
    "    print(\"Std : \" + str(losses.std()))\n",
    "        \n",
    "\n",
    "    print(\"Epoch: {}/{}.............\".format(epoch, n_epoch), end=\" \")\n",
    "    print(\"Loss: {:.4f}\".format(loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this model we can now try to add contextual embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for contextual embedding we will use BERT\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "#we use bert and we will train it on the data\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "#we will use the tokenizer to tokenize the sentences\n",
    "sentences = [\"I love machine learning\", \"I love coding in python\"]\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "fefcaccf17e39639418275c4a17d5ca0413e9c7c1af2b5e38e9064532ca76b63"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
