{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will use an RNN architecture to build a Machine Translation model.\n",
    "\n",
    "It will use as a corpus wikipedia dumps.\n",
    "\n",
    "Either the source or the target will be English. We will, in our case, try English to French Translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "!pip3 install numpy\n",
    "!pip3 install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cu117\n",
    "#or any nightly version so long as pytorch > 1.11 https://pytorch.org/\n",
    "!pip3 install gensim transformers d2l==1.0.0a1.post0\n",
    "\n",
    "#In pytorch functional.py, change PILLOW_VERSION to __version__\n",
    "#there are two places to change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test samples location and preprocessing\n",
    "\n",
    "#cell almost entirely from https://d2l.ai/chapter_recurrent-modern/machine-translation-and-dataset.html\n",
    "import os\n",
    "import torch\n",
    "from d2l import torch as d2l\n",
    "\n",
    "class MTFraEng(d2l.DataModule):  #@save\n",
    "    def _download(self):\n",
    "        d2l.extract(d2l.download(\n",
    "            d2l.DATA_URL+'fra-eng.zip', self.root,\n",
    "            '94646ad1522d915e7b0f9296181140edcf86a4f5'))\n",
    "        with open(self.root + '/fra-eng/fra.txt', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "\n",
    "@d2l.add_to_class(MTFraEng)  #@save\n",
    "def _preprocess(self, text):\n",
    "    # Replace non-breaking space with space\n",
    "    text = text.replace('\\u202f', ' ').replace('\\xa0', ' ')\n",
    "    # Insert space between words and punctuation marks\n",
    "    no_space = lambda char, prev_char: char in ',.!?' and prev_char != ' '\n",
    "    out = [' ' + char if i > 0 and no_space(char, text[i - 1]) else char\n",
    "           for i, char in enumerate(text.lower())]\n",
    "    return ''.join(out)\n",
    "\n",
    "@d2l.add_to_class(MTFraEng)  #@save\n",
    "def _tokenize(self, text, max_examples=None):\n",
    "    src, tgt = [], []\n",
    "    for i, line in enumerate(text.split('\\n')):\n",
    "        if max_examples and i > max_examples: break\n",
    "        parts = line.split('\\t')\n",
    "        if len(parts) == 2:\n",
    "            # Skip empty tokens\n",
    "            src.append([t for t in f'{parts[0]} <eos>'.split(' ') if t])  # src.append(EOS_token) ? \n",
    "            tgt.append([t for t in f'{parts[1]} <eos>'.split(' ') if t])\n",
    "    return src, tgt\n",
    "\n",
    "@d2l.add_to_class(MTFraEng)  #@save\n",
    "def __init__(self, batch_size, num_steps=15, num_train=162000, num_test=4000):  #15, 162000\n",
    "    super(MTFraEng, self).__init__()\n",
    "    self.save_hyperparameters()\n",
    "    self.arrays, self.src_vocab, self.tgt_vocab = self._build_arrays(\n",
    "        self._download())\n",
    "\n",
    "\n",
    "@d2l.add_to_class(MTFraEng)  #@save\n",
    "def _build_arrays(self, raw_text, src_vocab=None, tgt_vocab=None):\n",
    "    def _build_array(sentences, vocab, is_tgt=False):\n",
    "        pad_or_trim = lambda seq, t: (\n",
    "            seq[:t] if len(seq) > t else seq + ['<pad>'] * (t - len(seq)))\n",
    "        sentences = [pad_or_trim(s, self.num_steps) for s in sentences]\n",
    "        if is_tgt:\n",
    "            sentences = [['<bos>'] + s for s in sentences]\n",
    "        if vocab is None:\n",
    "            vocab = d2l.Vocab(sentences, min_freq=2)\n",
    "        array = torch.tensor([vocab[s] for s in sentences])\n",
    "        valid_len = (array != vocab['<pad>']).type(torch.int32).sum(1)\n",
    "        return array, vocab, valid_len\n",
    "    src, tgt = self._tokenize(self._preprocess(raw_text),\n",
    "                              self.num_train)\n",
    "    src_array, src_vocab, src_valid_len = _build_array(src, src_vocab)\n",
    "    tgt_array, tgt_vocab, _ = _build_array(tgt, tgt_vocab, True)\n",
    "    return ((src_array, tgt_array[:,:-1], src_valid_len, tgt_array[:,1:]),\n",
    "            src_vocab, tgt_vocab)\n",
    "\n",
    "@d2l.add_to_class(MTFraEng)  #@save\n",
    "def get_dataloader(self, train):\n",
    "    idx = slice(0, self.num_train - self.num_test) if train else slice(self.num_train - self.num_test, self.num_train)\n",
    "    return self.get_tensorloader(self.arrays, train, idx)\n",
    "\n",
    "@d2l.add_to_class(MTFraEng)  #@save\n",
    "def build(self, src_sentences, tgt_sentences):\n",
    "    raw_text = '\\n'.join([src + '\\t' + tgt for src, tgt in zip(\n",
    "        src_sentences, tgt_sentences)])\n",
    "    arrays, _, _ = self._build_arrays(\n",
    "        raw_text, self.src_vocab, self.tgt_vocab)\n",
    "    return arrays\n",
    "\n",
    "#src, tgt, _,  _ = data.build(['hi .'], ['salut .'])\n",
    "#print('source:', data.src_vocab.to_tokens(src[0].type(torch.int32)))\n",
    "#print('target:', data.tgt_vocab.to_tokens(tgt[0].type(torch.int32)))\n",
    "\n",
    "@d2l.add_to_class(MTFraEng)  #@save\n",
    "def shuffle(self, train, seed, maxi):\n",
    "    if (maxi > self.num_train):\n",
    "        raise ValueError(\"maxi must be less than the length of the dataset\")\n",
    "    \n",
    "    for array in self.arrays:\n",
    "        array = array[0:maxi]\n",
    "        \n",
    "    self.num_train = maxi\n",
    "    self.num_test = int(maxi * 0.3)\n",
    "    \n",
    "    idx = torch.randperm(generator=torch.Generator().manual_seed(seed), n=maxi)\n",
    "    if (not train):\n",
    "        idx = idx[int(maxi * 0.7):]\n",
    "    else :\n",
    "        idx = idx[:int(maxi * 0.7)]\n",
    "    return self.get_tensorloader(self.arrays, train, idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "\n",
    "data = MTFraEng(batch_size=batch_size)\n",
    "\n",
    "usage_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "31600\n",
      "140\n",
      "60\n",
      "60\n",
      "140\n"
     ]
    }
   ],
   "source": [
    "print(len(data.get_dataloader(train=False)))\n",
    "print(len(data.get_dataloader(train=True)))\n",
    "print(len(data.shuffle(train=True, seed=0, maxi=usage_size)))\n",
    "print(len(data.shuffle(train=False, seed=0, maxi=usage_size)))\n",
    "print(len(data.get_dataloader(train=False)))\n",
    "print(len(data.get_dataloader(train=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embedding\n",
    "\n",
    "# We will use three different types of word embeddings:\n",
    "# 1. Word2Vec\n",
    "# 2. GloVe\n",
    "# 3. FastText\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"## Word2Vec\"\"\"\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4268\n",
      "/home/lize/Desktop/MASTER2/Web_Text_Analysis/webtextanalysis\n",
      "/home/lize/.local/lib/python3.10/site-packages/ipykernel_launcher.py\n"
     ]
    }
   ],
   "source": [
    "#keep in mind you have to launch the notebook inside the git folder to make this work (second one)\n",
    "from inspect import getsourcefile\n",
    "import sys\n",
    "print(os.path.dirname(getsourcefile(lambda:0)))\n",
    "print(sys.path[0])\n",
    "print(os.path.abspath(sys.argv[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: tensor([[3681,   72,  187,  188,  188,  188,  188,  188,  188,  188,  188,  188,\n",
      "          188,  188,  188]], dtype=torch.int32)\n",
      "decoder input: tensor([[  136, 15923,     0,   137,   138,   138,   138,   138,   138,   138,\n",
      "           138,   138,   138,   138,   138]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "src, tgt, src_valid_len, label = next(iter(data.shuffle(train=False, seed=0, maxi=1)))\n",
    "print('source:', src.type(torch.int32))\n",
    "print('decoder input:', tgt.type(torch.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def save_split():\n",
    "    data = MTFraEng(batch_size=5)\n",
    "    with open(\"samples/source.txt\", \"w\") as f:\n",
    "        for i in range(0, data.num_train):\n",
    "            for word in data.arrays[0][i].numpy() :\n",
    "                f.write(str(word) + \" \")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "    with open(\"samples/target.txt\", \"w\") as f:\n",
    "        for i in range(0, data.num_train):\n",
    "            for word in data.arrays[1][i].numpy() :\n",
    "                f.write(str(word) + \" \")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "def load_source():\n",
    "    return np.loadtxt(\"samples/source.txt\", dtype=str)\n",
    "\n",
    "def load_target():\n",
    "    return np.loadtxt(\"samples/target.txt\", dtype=str)\n",
    "\n",
    "def word_to_token(word, src=True):\n",
    "    if src :\n",
    "        return data.src_vocab[word]\n",
    "    else :\n",
    "        return data.tgt_vocab[word]\n",
    "\n",
    "def token_to_word(token, src=True):\n",
    "    if src :\n",
    "        return data.src_vocab.to_tokens(token)\n",
    "    else :\n",
    "        return data.tgt_vocab.to_tokens(token)\n",
    "\n",
    "def test_similarity(model, word1, word2, model_name, src=True):\n",
    "    print(\"Cosine similarity between '\" + word1 + \"' and '\"+ word2 +\"' - \" + model_name + \" : \" + str(model.similarity(word_to_token(word1, src), word_to_token(word2, src))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['go', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['hi', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['run', '!', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['run', '!', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['who', '?', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<bos>', 'va', '!', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<bos>', 'salut', '!', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<bos>', 'cours', '!', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<bos>', 'courez', '!', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<bos>', 'qui', '?', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "#print a few samples\n",
    "for i in range(5):\n",
    "    print(token_to_word(data.arrays[0][i].numpy(), True))\n",
    "\n",
    "for i in range(5):\n",
    "    print(token_to_word(data.arrays[1][i].numpy(), False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(sys.path[0] + \"/samples/source.txt\") or not os.path.exists(sys.path[0] + \"/samples/target.txt\"):\n",
    "    save_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['3681'], ['72'], ['187'], ['188']]\n",
      "['go', '.', '<eos>', '<pad>']\n",
      "[['136'], ['15923'], ['0'], ['137']]\n",
      "['<bos>', 'va', '!', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "source_text = load_source()\n",
    "one_line_source = source_text.reshape([np.prod(source_text.shape)])\n",
    "\n",
    "#format to be accepted by Word2Vec\n",
    "one_line_source = [str(i).split() for i in one_line_source]\n",
    "\n",
    "print(one_line_source[:4])\n",
    "#print in words \n",
    "print([token_to_word(int(i[0]), src=True) for i in one_line_source[:4]])\n",
    "\n",
    "\n",
    "\n",
    "target_text = load_target()\n",
    "one_line_target = target_text.reshape([np.prod(target_text.shape)])\n",
    "\n",
    "#format to be accepted by Word2Vec\n",
    "one_line_target = [str(i).split() for i in one_line_target]\n",
    "\n",
    "print(one_line_target[:4])\n",
    "#print in words \n",
    "print([token_to_word(int(i[0]), src=False) for i in one_line_target[:4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['3681'], ['72'], ['187'], ['188'], ['188']]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"## Word2Vec\"\"\"\n",
    "\n",
    "print(one_line_source[:5])\n",
    "\n",
    "if not os.path.exists(sys.path[0] + \"/models/source_w2v_cbow.txt\"):\n",
    "    # Create CBOW model\n",
    "    source_w2v_model_cbow = gensim.models.Word2Vec(one_line_source, min_count = 3,\n",
    "                                vector_size = 100, window = 5).wv\n",
    "\n",
    "if not os.path.exists(sys.path[0] + \"/models/source_w2v_skip.txt\"):\n",
    "    # Create Skip Gram model\n",
    "    source_w2v_model_skip = gensim.models.Word2Vec(one_line_source, min_count = 3, vector_size = 100,\n",
    "                                                window = 5, sg = 1).wv\n",
    "    \n",
    "if not os.path.exists(sys.path[0] + \"/models/target_w2v_cbow.txt\"):\n",
    "    # Create CBOW model\n",
    "    target_w2v_model_cbow = gensim.models.Word2Vec(one_line_target, min_count = 3,\n",
    "                                vector_size = 100, window = 5).wv\n",
    "\n",
    "if not os.path.exists(sys.path[0] + \"/models/target_w2v_skip.txt\"):\n",
    "    # Create Skip Gram model\n",
    "    target_w2v_model_skip = gensim.models.Word2Vec(one_line_target, min_count = 3, vector_size = 100,\n",
    "                                                window = 5, sg = 1).wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the models\n",
    "if not os.path.exists(sys.path[0] + \"/models/source_w2v_cbow.txt\"):\n",
    "    source_w2v_model_cbow.save_word2vec_format(sys.path[0] + \"/models/source_w2v_cbow.txt\", binary=False)\n",
    "    \n",
    "if not os.path.exists(sys.path[0] + \"/models/source_w2v_skip.txt\"):\n",
    "    source_w2v_model_skip.save_word2vec_format(sys.path[0] + \"/models/source_w2v_skip.txt\", binary=False)\n",
    "    \n",
    "if not os.path.exists(sys.path[0] + \"/models/target_w2v_cbow.txt\"):\n",
    "    target_w2v_model_cbow.save_word2vec_format(sys.path[0] + \"/models/target_w2v_cbow.txt\", binary=False)\n",
    "\n",
    "if not os.path.exists(sys.path[0] + \"/models/target_w2v_skip.txt\"):\n",
    "    target_w2v_model_skip.save_word2vec_format(sys.path[0] + \"/models/target_w2v_skip.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the models\n",
    "source_w2v_model_cbow = gensim.models.KeyedVectors.load_word2vec_format(sys.path[0] + \"/models/source_w2v_cbow.txt\", binary=False)\n",
    "source_w2v_model_skip = gensim.models.KeyedVectors.load_word2vec_format(sys.path[0] + \"/models/source_w2v_skip.txt\", binary=False)\n",
    "target_w2v_model_cbow = gensim.models.KeyedVectors.load_word2vec_format(sys.path[0] + \"/models/target_w2v_cbow.txt\", binary=False)\n",
    "target_w2v_model_skip = gensim.models.KeyedVectors.load_word2vec_format(sys.path[0] + \"/models/target_w2v_skip.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'hi' and '.' - CBOW : -0.03504046\n",
      "Cosine similarity between 'hi' and 'run' - CBOW : -0.0359637\n",
      "Cosine similarity between 'hi' and '.' - SkipGram : -0.03504046\n",
      "Cosine similarity between 'hi' and 'run' - SkipGram : -0.0359637\n",
      "Cosine similarity between 'bonjour' and '.' - CBOW : 0.24336207\n",
      "Cosine similarity between 'bonjour' and 'cours' - CBOW : 1.0\n",
      "Cosine similarity between 'bonjour' and '.' - CBOW : 0.24336207\n",
      "Cosine similarity between 'bonjour' and 'cours' - CBOW : 1.0\n"
     ]
    }
   ],
   "source": [
    "test_similarity(source_w2v_model_cbow, 'hi', '.', \"CBOW\")\n",
    "test_similarity(source_w2v_model_cbow, 'hi', 'run', \"CBOW\")\n",
    "\n",
    "test_similarity(source_w2v_model_skip, 'hi', '.', \"SkipGram\")\n",
    "test_similarity(source_w2v_model_skip, 'hi', 'run', \"SkipGram\")\n",
    "\n",
    "test_similarity(target_w2v_model_cbow, 'bonjour', '.', \"CBOW\")\n",
    "test_similarity(target_w2v_model_cbow, 'bonjour', 'cours', \"CBOW\")\n",
    "\n",
    "test_similarity(target_w2v_model_skip, 'bonjour', '.', \"CBOW\")\n",
    "test_similarity(target_w2v_model_skip, 'bonjour', 'cours', \"CBOW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"## GloVe\"\"\"\n",
    "\n",
    "# coding: utf-8\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#once we have the tokenized file, we can call the glove model\n",
    "\n",
    "####CALL FROM BASH glove_run.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only do this once (depends on if windows or linux sometimes)\n",
    "#source_file = sys.path[0] + '\\\\models\\\\source_glove.txt'\n",
    "#target_file = sys.path[0] + '\\\\models\\\\target_glove.txt'\n",
    "\n",
    "source_file = sys.path[0] + '/models/source_glove.txt'\n",
    "target_file = sys.path[0] + '/models/target_glove.txt'\n",
    "# Load the model, can take a bit of time\n",
    "source_glove_model = KeyedVectors.load_word2vec_format(source_file, binary=False, no_header=True)\n",
    "target_glove_model = KeyedVectors.load_word2vec_format(source_file, binary=False, no_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'hi' and '.' - GloVe : -0.010622903\n",
      "Cosine similarity between 'hi' and 'run' - GloVe : 0.21301486\n",
      "Cosine similarity between 'bonjour' and '.' - GloVe : -0.04953654\n",
      "Cosine similarity between 'bonjour' and 'cours' - GloVe : 0.504102\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test the model\n",
    "test_similarity(source_glove_model, 'hi', '.', \"GloVe\", src=True)\n",
    "test_similarity(source_glove_model, 'hi', 'run', \"GloVe\", src=True)\n",
    "\n",
    "test_similarity(target_glove_model, 'bonjour', '.', \"GloVe\", src=False)\n",
    "test_similarity(target_glove_model, 'bonjour', 'cours', \"GloVe\", src=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"## FastText\"\"\"\n",
    "from gensim.models import FastText\n",
    "\n",
    "#if not saved yet we train it\n",
    "if not os.path.exists(sys.path[0] + \"/models/source_fast.txt\"):\n",
    "    source_fast_model = FastText(vector_size=100, window=5, min_count=3)\n",
    "    source_fast_model.build_vocab(corpus_file=sys.path[0] + '/samples/source.txt')\n",
    "    source_fast_model.train(corpus_file=sys.path[0] + '/samples/source.txt', epochs=10, total_examples=source_fast_model.corpus_count, total_words=source_fast_model.corpus_total_words)\n",
    "    source_fast_model = source_fast_model.wv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(sys.path[0] + \"/models/target_fast.txt\"):\n",
    "    target_fast_model = FastText(vector_size=100, window=5, min_count=3)\n",
    "    target_fast_model.build_vocab(corpus_file=sys.path[0] + '/samples/target.txt')\n",
    "    target_fast_model.train(corpus_file=sys.path[0] + '/samples/target.txt', epochs=10, total_examples=target_fast_model.corpus_count, total_words=target_fast_model.corpus_total_words)\n",
    "    target_fast_model = target_fast_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(sys.path[0] + \"/models/source_fast.txt\"):\n",
    "    source_fast_model.save_word2vec_format(sys.path[0] + \"/models/source_fast.txt\", binary=False)\n",
    "if not os.path.exists(sys.path[0] + \"/models/target_fast.txt\"):\n",
    "    target_fast_model.save_word2vec_format(sys.path[0] + \"/models/target_fast.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if saved we load it\n",
    "source_fast_model = KeyedVectors.load_word2vec_format(sys.path[0] + \"/models/source_fast.txt\", binary=False)\n",
    "target_fast_model = KeyedVectors.load_word2vec_format(sys.path[0] + \"/models/target_fast.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'hi' and '.' - FastText : 0.1312445\n",
      "Cosine similarity between 'hi' and 'run' - FastText : 0.55649316\n",
      "Cosine similarity between 'bonjour' and '.' - FastText : 0.11566037\n",
      "Cosine similarity between 'bonjour' and 'cours' - FastText : 0.61512697\n"
     ]
    }
   ],
   "source": [
    "test_similarity(source_fast_model,'hi', '.', \"FastText\", src=True)\n",
    "test_similarity(source_fast_model,'hi', 'run', \"FastText\", src=True)\n",
    "\n",
    "test_similarity(target_fast_model,'bonjour', '.', \"FastText\", src=False)\n",
    "test_similarity(target_fast_model,'bonjour', 'cours', \"FastText\", src=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can create the RNN model that will translate from english to french using one of the previous embeddings\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNN_encode(nn.Module):\n",
    "    def __init__(self, embedding_model_input, embedding_model_output):\n",
    "        super(RNN_encode, self).__init__()\n",
    "\n",
    "        self.embedding_in = embedding_model_input\n",
    "        self.embedding_out = embedding_model_output\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.embedding_dim_in = embedding_model_input.vector_size\n",
    "        self.lstm_in = nn.LSTM(self.embedding_dim_in, self.embedding_dim_in, bidirectional=True)\n",
    "        self.hidden_in = nn.Linear(self.embedding_dim_in * 2, self.embedding_dim_in)\n",
    "\n",
    "\n",
    "    def forward(self, input_sentence):\n",
    "        #words_embeddings is a gensim model\n",
    "        embeds = torch.tensor(np.array([[self.embedding_in[int(word.item())] if int(word.item()) in self.embedding_in else self.embedding_in.vectors.mean(axis=0) for word in sentence] for sentence in input_sentence]), requires_grad=True).to(device)\n",
    "        \n",
    "        #encoder\n",
    "        output_lstm_1, _ = self.lstm_in(embeds.view(input_sentence.shape[0], input_sentence.shape[1], self.embedding_dim_in))\n",
    "        output_hidden_1 = self.hidden_in(output_lstm_1.view(input_sentence.shape[0], input_sentence.shape[1], self.embedding_dim_in * 2))\n",
    "        \n",
    "        return F.log_softmax(output_hidden_1, dim=2)\n",
    "        \n",
    "\n",
    "    \n",
    "class RNN_decode(nn.Module):\n",
    "    def __init__(self, embedding_model_input, embedding_model_output):\n",
    "        super(RNN_decode, self).__init__()\n",
    "\n",
    "        self.embedding_in = embedding_model_input\n",
    "        self.embedding_out = embedding_model_output\n",
    "        \n",
    "        self.embedding_dim_in = embedding_model_input.vector_size\n",
    "        self.embedding_dim_out = embedding_model_input.vector_size\n",
    "        print(self.embedding_dim_out)\n",
    "        self.lstm_out = nn.LSTM(self.embedding_dim_in, self.embedding_dim_out, bidirectional=True)\n",
    "        self.hidden_out = nn.Linear(self.embedding_dim_out * 2, self.embedding_dim_out)\n",
    "        \n",
    "    def forward(self, hidden_sentence):\n",
    "        #decoder\n",
    "        output_lstm_2, _ = self.lstm_out(hidden_sentence.view(hidden_sentence.shape[0], hidden_sentence.shape[1], self.embedding_dim_in))\n",
    "        output_hidden_2 = self.hidden_out(output_lstm_2.view(hidden_sentence.shape[0], hidden_sentence.shape[1], self.embedding_dim_out * 2))\n",
    "        \n",
    "        out = torch.matmul(output_hidden_2, self.embedding_out.transpose(0,1))\n",
    "        \n",
    "        #where output greater than length of embedding_out[1] we set it to unknown\n",
    "        return torch.argmax(out, dim=2)\n",
    "    \n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, embedding_model_input, embedding_model_output):\n",
    "        super(RNN, self).__init__()\n",
    "        self.encoder = RNN_encode(embedding_model_input, embedding_model_output)\n",
    "        self.decoder = RNN_decode(embedding_model_input, embedding_model_output)\n",
    "        \n",
    "    def forward(self, input_sentence):\n",
    "        hidden_sentence = self.encoder(input_sentence)\n",
    "        output_sentence = self.decoder(hidden_sentence)\n",
    "        return output_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From this model we can create a loss function and an optimizer\n",
    "\n",
    "def loss_function(predicted_sentence, target_sentence, embedder):\n",
    "    \n",
    "    loss = torch.zeros(1).to(device)\n",
    "    for i in range(target_sentence.shape[0]):\n",
    "        for j in range(target_sentence.shape[1]):\n",
    "            if int(target_sentence[i][j]) < embedder.shape[1] and int(predicted_sentence[i][j]) < embedder.shape[1]:\n",
    "                loss += torch.abs(torch.matmul(embedder[int(predicted_sentence[i][j])], embedder[int(target_sentence[i][j])]))\n",
    "\n",
    "    return (loss / (predicted_sentence.shape[0] * predicted_sentence.shape[1])).clone().detach().requires_grad_(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "Ready\n",
      "None\n",
      "Nonestep :  1 / 141  loss :  2.0033464431762695  estimated time : 0.03156781196594238\n",
      "Nonestep :  2 / 141  loss :  1.2006044387817383  estimated time : 0.04937377691268921\n",
      "Nonestep :  3 / 141  loss :  1.6136871576309204  estimated time : 0.06258080446720124\n",
      "Nonestep :  4 / 141  loss :  1.2238258123397827  estimated time : 0.07311320843100548\n",
      "Nonestep :  5 / 141  loss :  1.6369084119796753  estimated time : 0.08298328195720911\n",
      "Nonestep :  6 / 141  loss :  1.266610026359558  estimated time : 0.09247572562073171\n",
      "Nonestep :  7 / 141  loss :  1.2006044387817383  estimated time : 0.10246755136972442\n",
      "Nonestep :  8 / 141  loss :  1.126074194908142  estimated time : 0.11015646637707438\n",
      "Nonestep :  9 / 141  loss :  1.181767225265503  estimated time : 0.1263578277048271\n",
      "Nonestep :  10 / 141  loss :  2.0033464431762695  estimated time : 0.14021072508453936\n",
      "Nonestep :  11 / 141  loss :  1.6136871576309204  estimated time : 0.1482123338727318\n",
      "Nonestep :  12 / 141  loss :  1.6136871576309204  estimated time : 0.15413474931029028\n",
      "Nonestep :  13 / 141  loss :  1.6369084119796753  estimated time : 0.15853250403227578\n",
      "Nonestep :  14 / 141  loss :  0.994063138961792  estimated time : 0.1629661345488932\n",
      "Nonestep :  15 / 141  loss :  1.6136871576309204  estimated time : 0.16628017294473957\n",
      "Nonestep :  16 / 141  loss :  1.0172843933105469  estimated time : 0.17672523612443738\n",
      "Nonestep :  17 / 141  loss :  1.8202284574508667  estimated time : 0.18498459733457293\n",
      "Nonestep :  18 / 141  loss :  1.6136871576309204  estimated time : 0.19347142093464115\n",
      "Nonestep :  19 / 141  loss :  0.994063138961792  estimated time : 0.20359683441549697\n",
      "New step :  20 / 141  loss :  1.3883086442947388  estimated time : 0.21287194398103804\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [37], line 68\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLoss: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(epoch_loss[epoch]))\n\u001b[1;32m     66\u001b[0m     \u001b[39mreturn\u001b[39;00m epoch_loss\n\u001b[0;32m---> 68\u001b[0m epoch_loss \u001b[39m=\u001b[39m train()\n",
      "Cell \u001b[0;32mIn [37], line 33\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m tgt\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     31\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 33\u001b[0m tag_scores \u001b[39m=\u001b[39m model(src)\n\u001b[1;32m     35\u001b[0m loss \u001b[39m=\u001b[39m loss_function(tag_scores, tgt, embedding_out)\n\u001b[1;32m     37\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1423\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1418\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1419\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1420\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1421\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1422\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1423\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1424\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1425\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [26], line 62\u001b[0m, in \u001b[0;36mRNN.forward\u001b[0;34m(self, input_sentence)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_sentence):\n\u001b[0;32m---> 62\u001b[0m     hidden_sentence \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(input_sentence)\n\u001b[1;32m     63\u001b[0m     output_sentence \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(hidden_sentence)\n\u001b[1;32m     64\u001b[0m     \u001b[39mreturn\u001b[39;00m output_sentence\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1423\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1418\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1419\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1420\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1421\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1422\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1423\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1424\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1425\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [26], line 25\u001b[0m, in \u001b[0;36mRNN_encode.forward\u001b[0;34m(self, input_sentence)\u001b[0m\n\u001b[1;32m     22\u001b[0m embeds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(np\u001b[39m.\u001b[39marray([[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_in[\u001b[39mint\u001b[39m(word\u001b[39m.\u001b[39mitem())] \u001b[39mif\u001b[39;00m \u001b[39mint\u001b[39m(word\u001b[39m.\u001b[39mitem()) \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_in \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_in\u001b[39m.\u001b[39mvectors\u001b[39m.\u001b[39mmean(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m sentence] \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m input_sentence]), requires_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     24\u001b[0m \u001b[39m#encoder\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m output_lstm_1, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm_in(embeds\u001b[39m.\u001b[39;49mview(input_sentence\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m], input_sentence\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding_dim_in))\n\u001b[1;32m     26\u001b[0m output_hidden_1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_in(output_lstm_1\u001b[39m.\u001b[39mview(input_sentence\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], input_sentence\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_dim_in \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m))\n\u001b[1;32m     28\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mlog_softmax(output_hidden_1, dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1423\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1418\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1419\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1420\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1421\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1422\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1423\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1424\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1425\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/rnn.py:776\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    774\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    775\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 776\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m    777\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[1;32m    778\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    779\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    780\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Now we can train the model\n",
    "import torch\n",
    "import time\n",
    "\n",
    "embed_in = source_fast_model\n",
    "embed_out = target_fast_model\n",
    "\n",
    "embedding_out = torch.tensor(embed_out.vectors, requires_grad=False).to(device)\n",
    "\n",
    "model = RNN(embedding_model_input=embed_in, embedding_model_output=embedding_out).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "n_epoch = 5\n",
    "\n",
    "size_per_epoch = int(usage_size * 0.7 / batch_size) + 1\n",
    "\n",
    "print(\"Ready\")\n",
    "\n",
    "def train() :\n",
    "    epoch_loss = np.zeros(n_epoch)\n",
    "    for epoch in range(n_epoch):\n",
    "        counter = 0\n",
    "        time_avg = 0\n",
    "        for src, tgt, src_valid_len, label in data.shuffle(train=True, seed=0, maxi=usage_size):\n",
    "            time_start = time.time()\n",
    "            \n",
    "            src.to(device)\n",
    "            tgt.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            tag_scores = model(src)\n",
    "\n",
    "            loss = loss_function(tag_scores, tgt, embedding_out)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "            print(model.decoder.lstm_out.weight_hh_l0.grad)\n",
    "\n",
    "            counter += 1\n",
    "            time_avg = time_avg * 0.95 + (time.time() - time_start) * (size_per_epoch - counter) * 0.01\n",
    "            print(\"New step : \", counter, \"/\", size_per_epoch, \" loss : \", loss.item(), \" estimated time :\", time_avg , end=\"\\r\")\n",
    "            \n",
    "        #here we can use the test data to evaluate the model\n",
    "        with torch.no_grad() :\n",
    "            losses = torch.zeros(int(usage_size * 0.3 / batch_size) + 1)\n",
    "            counter = 0\n",
    "            for src, tgt, src_valid_len, label in data.shuffle(train=False, seed=0, maxi=usage_size):\n",
    "                src.to(device)\n",
    "                tgt.to(device)\n",
    "\n",
    "                tag_scores = model(src)\n",
    "\n",
    "                loss = loss_function(tag_scores, tgt, embedding_out)\n",
    "                \n",
    "                losses[counter] = loss.item()\n",
    "                counter += 1\n",
    "\n",
    "            epoch_loss[epoch] = losses.mean()\n",
    "            print(\"Epoch: {}/{}.............\".format(epoch, n_epoch), end=\" \")\n",
    "            print(\"Loss: \" + str(epoch_loss[epoch]))\n",
    "            \n",
    "    return epoch_loss\n",
    "            \n",
    "epoch_loss = train()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20e439e02b0>]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "Original sentence : \n",
      "don't lie . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
      "2602 4866 72 187 188 188 188 188 188 188 188 188 188 188 188 \n",
      "Translated sentence : \n",
      "3000 3000 3000 3000 3000 3000 3000 3000 3000 3000 3000 3000 3000 3000 3000 \n",
      "106 106 106 106 106 106 106 106 106 106 106 106 106 106 106 \n",
      "Target sentence : \n",
      "<bos> ne mens pas . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
      "136 10230 9441 10939 27 137 138 138 138 138 138 138 138 138 138 \n",
      "ne mens pas . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
      "10230 9441 10939 27 137 138 138 138 138 138 138 138 138 138 138 \n"
     ]
    }
   ],
   "source": [
    "#sample a sentence from the test set\n",
    "src, tgt, src_valid_len, label = next(iter(data.shuffle(train=False, seed=0, maxi=usage_size)))\n",
    "\n",
    "#translate the sentence\n",
    "\n",
    "sentence = src.to(device)\n",
    "\n",
    "tag_scores = model(sentence)\n",
    "\n",
    "#print the original sentence\n",
    "print(\"Original sentence : \")\n",
    "for word in src[0]:\n",
    "    print(token_to_word(word.item(), src=True), end=\" \")\n",
    "print()\n",
    "for word in src[0]:\n",
    "    print(word.item(), end=\" \")\n",
    "print()\n",
    "\n",
    "#print the translated sentence\n",
    "print(\"Translated sentence : \")\n",
    "for word in tag_scores[0]:\n",
    "    print(token_to_word(int(word.item()), src=False), end=\" \")\n",
    "print()\n",
    "for word in tag_scores[0]:\n",
    "    print(int(word.item()), end=\" \")\n",
    "print()\n",
    "\n",
    "#print the target sentence\n",
    "print(\"Target sentence : \")\n",
    "for word in tgt[0]:\n",
    "    print(token_to_word(word.item(), src=False), end=\" \")\n",
    "print()\n",
    "for word in tgt[0]:\n",
    "    print(word.item(), end=\" \")\n",
    "print()\n",
    "\n",
    "for word in label[0]:\n",
    "    print(token_to_word(word.item(), src=False), end=\" \")\n",
    "print()\n",
    "for word in label[0]:\n",
    "    print(word.item(), end=\" \")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this model we can now try to add contextual embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for contextual embedding we will use BERT\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "#we use bert and we will train it on the data\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "#we will use the tokenizer to tokenize the sentences\n",
    "sentences = [\"I love machine learning\", \"I love coding in python\"]\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
