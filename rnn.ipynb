{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will use an RNN architecture to build a Machine Translation model.\n",
    "\n",
    "It will use as a corpus wikipedia dumps.\n",
    "\n",
    "Either the source or the target will be English. We will, in our case, try English to French Translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "!pip3 install numpy\n",
    "!pip3 install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cu117\n",
    "#or any nightly version so long as pytorch > 1.11 https://pytorch.org/\n",
    "!pip3 install gensim transformers d2l==1.0.0a1.post0\n",
    "\n",
    "#In pytorch functional.py, change PILLOW_VERSION to __version__\n",
    "#there are two places to change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test samples location and preprocessing\n",
    "\n",
    "#cell almost entirely from https://d2l.ai/chapter_recurrent-modern/machine-translation-and-dataset.html\n",
    "import os\n",
    "import torch\n",
    "from d2l import torch as d2l\n",
    "\n",
    "class MTFraEng(d2l.DataModule):  #@save\n",
    "    def _download(self):\n",
    "        d2l.extract(d2l.download(\n",
    "            d2l.DATA_URL+'fra-eng.zip', self.root,\n",
    "            '94646ad1522d915e7b0f9296181140edcf86a4f5'))\n",
    "        with open(self.root + '/fra-eng/fra.txt', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "\n",
    "@d2l.add_to_class(MTFraEng)  #@save\n",
    "def _preprocess(self, text):\n",
    "    # Replace non-breaking space with space\n",
    "    text = text.replace('\\u202f', ' ').replace('\\xa0', ' ')\n",
    "    # Insert space between words and punctuation marks\n",
    "    no_space = lambda char, prev_char: char not in 'aàâbcdeéèêâfghiîjklmnoôpqrstuûvwxyz ' and prev_char != ' '\n",
    "    out = [' ' + char if i > 0 and no_space(char, text[i - 1]) else char\n",
    "           for i, char in enumerate(text.lower())]\n",
    "    \n",
    "    #insert space after punctuation marks\n",
    "    no_space = lambda char, next_char: char not in 'aàâbcdeéèêâfghiîjklmnoôpqrstuûvwxyz ' and next_char != ' '\n",
    "    out = [char + ' ' if i < len(text) - 1 and no_space(char, text[i + 1]) else char\n",
    "              for i, char in enumerate(out)]\n",
    "    return ''.join(out)\n",
    "\n",
    "@d2l.add_to_class(MTFraEng)  #@save\n",
    "def _tokenize(self, text, max_examples=None):\n",
    "    src, tgt = [], []\n",
    "    for i, line in enumerate(text.split('\\n')):\n",
    "        if max_examples and i > max_examples: break\n",
    "        parts = line.split('\\t')\n",
    "        if len(parts) == 2:\n",
    "            # Skip empty tokens\n",
    "            src.append([t for t in f'{parts[0]} <eos>'.split(' ') if t])  # src.append(EOS_token) ? \n",
    "            tgt.append([t for t in f'{parts[1]} <eos>'.split(' ') if t])\n",
    "    return src, tgt\n",
    "\n",
    "@d2l.add_to_class(MTFraEng)  #@save\n",
    "def __init__(self, batch_size=10, num_steps=15, num_train=162000):  #15, 162000\n",
    "    super(MTFraEng, self).__init__()\n",
    "    self.save_hyperparameters()\n",
    "    self.arrays, self.src_vocab, self.tgt_vocab = self._build_arrays(\n",
    "        self._download())\n",
    "\n",
    "\n",
    "@d2l.add_to_class(MTFraEng)  #@save\n",
    "def _build_arrays(self, raw_text, src_vocab=None, tgt_vocab=None):\n",
    "    def _build_array(sentences, vocab, is_tgt=False):\n",
    "        pad_or_trim = lambda seq, t: (\n",
    "            seq[:t] if len(seq) > t else seq + ['<pad>'] * (t - len(seq)))\n",
    "        sentences = [pad_or_trim(s, self.num_steps) for s in sentences]\n",
    "        if is_tgt:\n",
    "            sentences = [['<bos>'] + s for s in sentences]\n",
    "        if vocab is None:\n",
    "            vocab = d2l.Vocab(sentences, min_freq=3)\n",
    "        array = torch.tensor([vocab[s] for s in sentences])\n",
    "        valid_len = (array != vocab['<pad>']).type(torch.int64).sum(1)\n",
    "        return array, vocab, valid_len\n",
    "    src, tgt = self._tokenize(self._preprocess(raw_text),\n",
    "                              self.num_train)\n",
    "    \n",
    "    src_array, src_vocab, src_valid_len = _build_array(src, src_vocab)\n",
    "    tgt_array, tgt_vocab, _ = _build_array(tgt, tgt_vocab, True)\n",
    "\n",
    "    \n",
    "    return ((src_array, tgt_array[:,:-1], src_valid_len, tgt_array[:,1:]),\n",
    "            src_vocab, tgt_vocab)\n",
    "\n",
    "@d2l.add_to_class(MTFraEng)  #@save\n",
    "def build(self, src_sentences, tgt_sentences):\n",
    "    raw_text = '\\n'.join([src + '\\t' + tgt for src, tgt in zip(\n",
    "        src_sentences, tgt_sentences)])\n",
    "    arrays, _, _ = self._build_arrays(\n",
    "        raw_text, self.src_vocab, self.tgt_vocab)\n",
    "    return arrays\n",
    "\n",
    "#src, tgt, _,  _ = data.build(['hi .'], ['salut .'])\n",
    "#print('source:', data.src_vocab.to_tokens(src[0].type(torch.int64)))\n",
    "#print('target:', data.tgt_vocab.to_tokens(tgt[0].type(torch.int64)))\n",
    "\n",
    "@d2l.add_to_class(MTFraEng)  #@save\n",
    "def get_dataloader(self, train, seed=0, maxi=158140):\n",
    "    if (maxi > self.num_train):\n",
    "        raise ValueError(\"maxi must be less than the length of the dataset\")\n",
    "    \n",
    "    for array in self.arrays:\n",
    "        array = array[0:maxi]\n",
    "        \n",
    "    self.num_train = maxi\n",
    "    self.num_test = int(maxi * 0.3)\n",
    "    \n",
    "    idx = torch.randperm(generator=torch.Generator().manual_seed(seed), n=maxi)\n",
    "    \n",
    "    #0 is train, 1 is test, 2 is valid\n",
    "    if (train == 1):\n",
    "        idx = idx[int(maxi * 0.8):]\n",
    "    if (train == 2):\n",
    "        idx = idx[int(maxi * 0.6):int(maxi * 0.8)]\n",
    "    else :\n",
    "        idx = idx[:int(maxi * 0.6)]\n",
    "    return self.get_tensorloader(self.arrays, train, idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = MTFraEng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3163\n",
      "3163\n",
      "9489\n"
     ]
    }
   ],
   "source": [
    "print(len(data.get_dataloader(train=1)))\n",
    "print(len(data.get_dataloader(train=2)))\n",
    "print(len(data.get_dataloader(train=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embedding\n",
    "\n",
    "# We will use three different types of word embeddings:\n",
    "# 1. Word2Vec\n",
    "# 2. GloVe\n",
    "# 3. FastText\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"## Word2Vec\"\"\"\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gille\\AppData\\Local\\Temp\\ipykernel_9936\n",
      "c:\\Users\\gille\\OneDrive\\Desktop\\web\\webtextanalysis\n",
      "c:\\Users\\gille\\miniconda3\\envs\\thesis\\lib\\site-packages\\ipykernel_launcher.py\n"
     ]
    }
   ],
   "source": [
    "#keep in mind you have to launch the notebook inside the git folder to make this work (second one)\n",
    "from inspect import getsourcefile\n",
    "import sys\n",
    "print(os.path.dirname(getsourcefile(lambda:0)))\n",
    "print(sys.path[0])\n",
    "print(os.path.abspath(sys.argv[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: tensor([3204, 6488, 7228,    4, 1608,  568, 4353,    7,   20,   21,   21,   21,\n",
      "          21,   21,   21])\n",
      "decoder input: tensor([   20,  5387,  7071,  7987, 10190,  9145,  7331, 10457,     7,    21,\n",
      "           22,    22,    22,    22,    22])\n"
     ]
    }
   ],
   "source": [
    "src, tgt, src_valid_len, label = next(iter(data.get_dataloader(train=0)))\n",
    "print('source:', src[0].type(torch.int64))\n",
    "print('decoder input:', tgt[0].type(torch.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def save_split():\n",
    "    data = MTFraEng(batch_size=5)\n",
    "    with open(\"samples/source.txt\", \"w\") as f:\n",
    "        for i in range(0, data.num_train):\n",
    "            for word in data.arrays[0][i].numpy() :\n",
    "                f.write(str(word) + \" \")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "    with open(\"samples/target.txt\", \"w\") as f:\n",
    "        for i in range(0, data.num_train):\n",
    "            for word in data.arrays[1][i].numpy() :\n",
    "                f.write(str(word) + \" \")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "def load_source():\n",
    "    return np.loadtxt(\"samples/source.txt\", dtype=str)\n",
    "\n",
    "def load_target():\n",
    "    return np.loadtxt(\"samples/target.txt\", dtype=str)\n",
    "\n",
    "def word_to_token(word, src=True):\n",
    "    if src :\n",
    "        return data.src_vocab[word]\n",
    "    else :\n",
    "        return data.tgt_vocab[word]\n",
    "\n",
    "def token_to_word(token, src=True):\n",
    "    if src :\n",
    "        return data.src_vocab.to_tokens(token)\n",
    "    else :\n",
    "        return data.tgt_vocab.to_tokens(token)\n",
    "\n",
    "def test_similarity(model, word1, word2, model_name, src=True):\n",
    "    print(\"Cosine similarity between '\" + word1 + \"' and '\"+ word2 +\"' - \" + model_name + \" : \" + str(model.similarity(word_to_token(word1, src), word_to_token(word2, src))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', \"'\", 'm', 'working', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['i', \"'\", 'm', 'worried', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['i', \"'\", 've', 'failed', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['ignore', 'them', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['ignore', 'them', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<bos>', 'je', 'suis', 'en', 'train', 'de', 'travailler', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<bos>', 'je', 'me', 'fais', 'du', 'souci', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<bos>', 'j', \"'\", 'ai', 'échoué', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<bos>', 'ignore', '-', 'les', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<bos>', 'ignorez', '-', 'les', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "#print a few samples\n",
    "for i in range(2500,2505):\n",
    "    print(token_to_word(data.arrays[0][i].numpy(), True))\n",
    "\n",
    "for i in range(2500,2505):\n",
    "    print(token_to_word(data.arrays[1][i].numpy(), False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(sys.path[0] + \"/samples/source.txt\") or not os.path.exists(sys.path[0] + \"/samples/target.txt\"):\n",
    "    save_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['2806'], ['7'], ['20'], ['21']]\n",
      "['go', '.', '<eos>', '<pad>']\n",
      "[['20'], ['10312'], ['0'], ['21']]\n",
      "['<bos>', 'va', '!', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "source_text = load_source()\n",
    "one_line_source = source_text.reshape([np.prod(source_text.shape)])\n",
    "\n",
    "#format to be accepted by Word2Vec\n",
    "one_line_source = [str(i).split() for i in one_line_source]\n",
    "\n",
    "print(one_line_source[:4])\n",
    "#print in words \n",
    "print([token_to_word(int(i[0]), src=True) for i in one_line_source[:4]])\n",
    "\n",
    "\n",
    "\n",
    "target_text = load_target()\n",
    "one_line_target = target_text.reshape([np.prod(target_text.shape)])\n",
    "\n",
    "#format to be accepted by Word2Vec\n",
    "one_line_target = [str(i).split() for i in one_line_target]\n",
    "\n",
    "print(one_line_target[:4])\n",
    "#print in words \n",
    "print([token_to_word(int(i[0]), src=False) for i in one_line_target[:4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"## Word2Vec\"\"\"\n",
    "if not os.path.exists(sys.path[0] + \"/models/source_w2v_cbow.txt\"):\n",
    "    # Create CBOW model\n",
    "    source_w2v_model_cbow = gensim.models.Word2Vec(one_line_source, min_count = 1,\n",
    "                                vector_size = 100, window = 5).wv\n",
    "\n",
    "if not os.path.exists(sys.path[0] + \"/models/source_w2v_skip.txt\"):\n",
    "    # Create Skip Gram model\n",
    "    source_w2v_model_skip = gensim.models.Word2Vec(one_line_source, min_count = 1, vector_size = 100,\n",
    "                                                window = 5, sg = 1).wv\n",
    "    \n",
    "if not os.path.exists(sys.path[0] + \"/models/target_w2v_cbow.txt\"):\n",
    "    # Create CBOW model\n",
    "    target_w2v_model_cbow = gensim.models.Word2Vec(one_line_target, min_count = 1,\n",
    "                                vector_size = 100, window = 5).wv\n",
    "\n",
    "if not os.path.exists(sys.path[0] + \"/models/target_w2v_skip.txt\"):\n",
    "    # Create Skip Gram model\n",
    "    target_w2v_model_skip = gensim.models.Word2Vec(one_line_target, min_count = 1, vector_size = 100,\n",
    "                                                window = 5, sg = 1).wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the models\n",
    "if not os.path.exists(sys.path[0] + \"/models/source_w2v_cbow.txt\"):\n",
    "    source_w2v_model_cbow.save_word2vec_format(sys.path[0] + \"/models/source_w2v_cbow.txt\", binary=False)\n",
    "    \n",
    "if not os.path.exists(sys.path[0] + \"/models/source_w2v_skip.txt\"):\n",
    "    source_w2v_model_skip.save_word2vec_format(sys.path[0] + \"/models/source_w2v_skip.txt\", binary=False)\n",
    "    \n",
    "if not os.path.exists(sys.path[0] + \"/models/target_w2v_cbow.txt\"):\n",
    "    target_w2v_model_cbow.save_word2vec_format(sys.path[0] + \"/models/target_w2v_cbow.txt\", binary=False)\n",
    "\n",
    "if not os.path.exists(sys.path[0] + \"/models/target_w2v_skip.txt\"):\n",
    "    target_w2v_model_skip.save_word2vec_format(sys.path[0] + \"/models/target_w2v_skip.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the models\n",
    "source_w2v_model_cbow = gensim.models.KeyedVectors.load_word2vec_format(sys.path[0] + \"/models/source_w2v_cbow.txt\", binary=False)\n",
    "source_w2v_model_skip = gensim.models.KeyedVectors.load_word2vec_format(sys.path[0] + \"/models/source_w2v_skip.txt\", binary=False)\n",
    "target_w2v_model_cbow = gensim.models.KeyedVectors.load_word2vec_format(sys.path[0] + \"/models/target_w2v_cbow.txt\", binary=False)\n",
    "target_w2v_model_skip = gensim.models.KeyedVectors.load_word2vec_format(sys.path[0] + \"/models/target_w2v_skip.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'hi' and '.' - CBOW : -0.0036201924\n",
      "Cosine similarity between 'hi' and 'run' - CBOW : 0.003568519\n",
      "Cosine similarity between 'hi' and '.' - SkipGram : -0.0036201924\n",
      "Cosine similarity between 'hi' and 'run' - SkipGram : 0.003568519\n",
      "Cosine similarity between 'bonjour' and '.' - CBOW : 0.015781732\n",
      "Cosine similarity between 'bonjour' and 'cours' - CBOW : 1.0\n",
      "Cosine similarity between 'bonjour' and '.' - CBOW : 0.015781732\n",
      "Cosine similarity between 'bonjour' and 'cours' - CBOW : 1.0\n"
     ]
    }
   ],
   "source": [
    "test_similarity(source_w2v_model_cbow, 'hi', '.', \"CBOW\")\n",
    "test_similarity(source_w2v_model_cbow, 'hi', 'run', \"CBOW\")\n",
    "\n",
    "test_similarity(source_w2v_model_skip, 'hi', '.', \"SkipGram\")\n",
    "test_similarity(source_w2v_model_skip, 'hi', 'run', \"SkipGram\")\n",
    "\n",
    "test_similarity(target_w2v_model_cbow, 'bonjour', '.', \"CBOW\")\n",
    "test_similarity(target_w2v_model_cbow, 'bonjour', 'cours', \"CBOW\")\n",
    "\n",
    "test_similarity(target_w2v_model_skip, 'bonjour', '.', \"CBOW\")\n",
    "test_similarity(target_w2v_model_skip, 'bonjour', 'cours', \"CBOW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"## GloVe\"\"\"\n",
    "\n",
    "# coding: utf-8\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#once we have the tokenized file, we can call the glove model\n",
    "\n",
    "####CALL FROM BASH glove_run.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only do this once (depends on if windows or linux sometimes)\n",
    "#source_file = sys.path[0] + '\\\\models\\\\source_glove.txt'\n",
    "#target_file = sys.path[0] + '\\\\models\\\\target_glove.txt'\n",
    "\n",
    "source_file = sys.path[0] + '/models/source_glove.txt'\n",
    "target_file = sys.path[0] + '/models/target_glove.txt'\n",
    "# Load the model, can take a bit of time\n",
    "source_glove_model = KeyedVectors.load_word2vec_format(source_file, binary=False, no_header=True)\n",
    "target_glove_model = KeyedVectors.load_word2vec_format(source_file, binary=False, no_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'hi' and '.' - GloVe : -0.19960971\n",
      "Cosine similarity between 'hi' and 'run' - GloVe : 0.10124894\n",
      "Cosine similarity between 'bonjour' and '.' - GloVe : 0.10055532\n",
      "Cosine similarity between 'bonjour' and 'cours' - GloVe : 0.31528467\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test the model\n",
    "test_similarity(source_glove_model, 'hi', '.', \"GloVe\", src=True)\n",
    "test_similarity(source_glove_model, 'hi', 'run', \"GloVe\", src=True)\n",
    "\n",
    "test_similarity(target_glove_model, 'bonjour', '.', \"GloVe\", src=False)\n",
    "test_similarity(target_glove_model, 'bonjour', 'cours', \"GloVe\", src=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"## FastText\"\"\"\n",
    "from gensim.models import FastText\n",
    "\n",
    "#if not saved yet we train it\n",
    "if not os.path.exists(sys.path[0] + \"/models/source_fast.txt\"):\n",
    "    source_fast_model = FastText(vector_size=100, window=5, min_count=1)\n",
    "    source_fast_model.build_vocab(corpus_file=sys.path[0] + '/samples/source.txt')\n",
    "    source_fast_model.train(corpus_file=sys.path[0] + '/samples/source.txt', epochs=10, total_examples=source_fast_model.corpus_count, total_words=source_fast_model.corpus_total_words)\n",
    "    source_fast_model = source_fast_model.wv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(sys.path[0] + \"/models/target_fast.txt\"):\n",
    "    target_fast_model = FastText(vector_size=100, window=5, min_count=1)\n",
    "    target_fast_model.build_vocab(corpus_file=sys.path[0] + '/samples/target.txt')\n",
    "    target_fast_model.train(corpus_file=sys.path[0] + '/samples/target.txt', epochs=10, total_examples=target_fast_model.corpus_count, total_words=target_fast_model.corpus_total_words)\n",
    "    target_fast_model = target_fast_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(sys.path[0] + \"/models/source_fast.txt\"):\n",
    "    source_fast_model.save_word2vec_format(sys.path[0] + \"/models/source_fast.txt\", binary=False)\n",
    "if not os.path.exists(sys.path[0] + \"/models/target_fast.txt\"):\n",
    "    target_fast_model.save_word2vec_format(sys.path[0] + \"/models/target_fast.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if saved we load it\n",
    "source_fast_model = KeyedVectors.load_word2vec_format(sys.path[0] + \"/models/source_fast.txt\", binary=False)\n",
    "target_fast_model = KeyedVectors.load_word2vec_format(sys.path[0] + \"/models/target_fast.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'hi' and '.' - FastText : 0.2860466\n",
      "Cosine similarity between 'hi' and 'run' - FastText : 0.5373634\n",
      "Cosine similarity between 'bonjour' and '.' - FastText : 0.13029826\n",
      "Cosine similarity between 'bonjour' and 'cours' - FastText : 0.38942528\n"
     ]
    }
   ],
   "source": [
    "test_similarity(source_fast_model,'hi', '.', \"FastText\", src=True)\n",
    "test_similarity(source_fast_model,'hi', 'run', \"FastText\", src=True)\n",
    "\n",
    "test_similarity(target_fast_model,'bonjour', '.', \"FastText\", src=False)\n",
    "test_similarity(target_fast_model,'bonjour', 'cours', \"FastText\", src=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can create the RNN model that will translate from english to french using one of the previous embeddings\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNN_encode(nn.Module):\n",
    "    def __init__(self, embedding_model_input, embedding_model_output, hidden_size=200):\n",
    "        super(RNN_encode, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        embedding = torch.tensor(embedding_model_input.vectors).to(device)\n",
    "        self.embedding_in = (embedding / torch.norm(embedding, dim=1, keepdim=True)).to(device)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.embedding_dim_in = self.embedding_in.shape[1]\n",
    "        self.lstm_in = nn.LSTM(self.embedding_dim_in, self.embedding_dim_in, bidirectional=True, num_layers=2, dropout=0.01)\n",
    "        self.hidden_in = nn.Linear(self.embedding_dim_in, self.hidden_size)\n",
    "        self.hidden_in2 = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "\n",
    "\n",
    "    def forward(self, input_sentence):\n",
    "        #words_embeddings is a gensim model\n",
    "        input_sentence = torch.where((input_sentence < self.embedding_in.shape[0]).to(device), input_sentence.to(device), torch.zeros_like(input_sentence).to(device))\n",
    "        one_hot = F.one_hot(input_sentence.type(torch.int64), self.embedding_in.shape[0]).to(device)\n",
    "        embeds = torch.matmul(one_hot.type(torch.float), self.embedding_in.type(torch.float))\n",
    "         \n",
    "        #encoder\n",
    "        output_lstm_1, _ = self.lstm_in(embeds.view(input_sentence.shape[0], input_sentence.shape[1], self.embedding_dim_in))\n",
    "        \n",
    "        #sum the two directions\n",
    "        output_lstm_1 = output_lstm_1[:, :, :self.embedding_dim_in] + output_lstm_1[:, : ,self.embedding_dim_in:]\n",
    "        soft = F.log_softmax(output_lstm_1, dim=2)\n",
    "        \n",
    "        output_hidden_1 = self.hidden_in(soft.view(input_sentence.shape[0], input_sentence.shape[1], self.embedding_dim_in))\n",
    "        output_hidden_2 = self.hidden_in2(F.relu(output_hidden_1.view(input_sentence.shape[0], input_sentence.shape[1], self.hidden_size)))\n",
    "        \n",
    "        return F.log_softmax(output_hidden_2, dim=2)\n",
    "        \n",
    "class RNN_decode(nn.Module):\n",
    "    def __init__(self, embedding_model_input, embedding_model_output, hidden_size=200):\n",
    "        super(RNN_decode, self).__init__()\n",
    "\n",
    "        embedding = torch.tensor(embedding_model_output.vectors).to(device)\n",
    "        self.embedding_out = embedding / torch.norm(embedding, dim=1, keepdim=True).to(device)\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding_dim_out = self.embedding_out.shape[1]\n",
    "        self.lstm_out = nn.LSTM(self.hidden_size, self.hidden_size, bidirectional=True, num_layers=2, dropout=0.01)\n",
    "        self.hidden_out = nn.Linear(self.hidden_size, self.embedding_dim_out)\n",
    "        self.hidden_out2 = nn.Linear(self.embedding_dim_out, self.embedding_dim_out)\n",
    "        \n",
    "    def forward(self, hidden_sentence):\n",
    "        #decoder\n",
    "        output_lstm_2, _ = self.lstm_out(hidden_sentence.view(hidden_sentence.shape[0], hidden_sentence.shape[1], self.hidden_size))\n",
    "        \n",
    "        #sum the two directions\n",
    "        output_lstm_2 = output_lstm_2[:, :, :self.hidden_size] + output_lstm_2[:, : ,self.hidden_size:]\n",
    "        soft1 = F.log_softmax(output_lstm_2, dim=2)\n",
    "        output_hidden_1 = self.hidden_out(soft1.view(hidden_sentence.shape[0], hidden_sentence.shape[1], self.hidden_size))\n",
    "        output_hidden_2 = self.hidden_out2(F.relu(output_hidden_1.view(hidden_sentence.shape[0], hidden_sentence.shape[1], self.embedding_dim_out)))\n",
    "    \n",
    "        #similarity\n",
    "        out = torch.abs(output_hidden_2 @ self.embedding_out.transpose(0,1))\n",
    "        return  F.log_softmax(out, dim=2) #size batch, sentence, size_voc\n",
    "    \n",
    "    def get_embedding(self):\n",
    "        return self.embedding_out\n",
    "    \n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, embedding_model_input, embedding_model_output):\n",
    "        super(RNN, self).__init__()\n",
    "        self.encoder = RNN_encode(embedding_model_input, embedding_model_output)\n",
    "        self.decoder = RNN_decode(embedding_model_input, embedding_model_output)\n",
    "        \n",
    "    def forward(self, input_sentence):\n",
    "        hidden_sentence = self.encoder(input_sentence)\n",
    "        output_sentence = self.decoder(hidden_sentence)\n",
    "        return output_sentence\n",
    "    \n",
    "    def get_embedding(self):\n",
    "        return self.decoder.get_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From this model we can create a loss function and an optimizer\n",
    "\n",
    "def loss_function(predicted_sentence, target_sentence, unk, pad):\n",
    "    #replace out of vocabulary words by unk\n",
    "    mask = (target_sentence < predicted_sentence.shape[2]).to(device)\n",
    "    mask = mask * (target_sentence != pad).to(device)\n",
    "    target_sentence = torch.where(mask, target_sentence.to(device), torch.zeros_like(target_sentence).to(device) + unk)\n",
    "    #returns he class cross entropy loss\n",
    "    return  F.cross_entropy(predicted_sentence.view(-1, predicted_sentence.shape[2]), target_sentence.view(-1), reduction='sum', ignore_index=unk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "data = MTFraEng(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can train the model\n",
    "import torch\n",
    "import time\n",
    "\n",
    "def train(embed_in, embed_out, n_epoch = 15, batch_size = 10, usage_size = 130000, lr = 0.01, neural = RNN):\n",
    "\n",
    "    model = neural(embedding_model_input=embed_in, embedding_model_output=embed_out).to(device)\n",
    "\n",
    "    lr1 = lr\n",
    "    \n",
    "    size_per_epoch = int(usage_size * 0.6 / batch_size) + 1\n",
    "\n",
    "    print(\"Ready\")\n",
    "\n",
    "    unk = word_to_token('<unk>', src=False)\n",
    "    pad = word_to_token('<pad>', src=False)\n",
    "\n",
    "    epoch_loss = np.zeros(n_epoch)\n",
    "    epoch_lr = np.zeros(n_epoch)\n",
    "    for epoch in range(n_epoch):\n",
    "        \n",
    "        if epoch % 5 == 0 and epoch != 0:\n",
    "            lr1 = lr1 / 1.1\n",
    "            \n",
    "        if epoch % 50 == 0 and epoch != 0:\n",
    "            lr1 = lr / 2\n",
    "        \n",
    "        capturable = device == 'cuda'\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr1, capturable=capturable)\n",
    "        \n",
    "        counter = 0\n",
    "        time_avg = 0\n",
    "        for src, tgt, src_valid_len, label in data.get_dataloader(train=0, seed=0, maxi=usage_size):\n",
    "            time_start = time.time()\n",
    "            \n",
    "            src.to(device)\n",
    "            tgt.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            tag_scores = model(src)\n",
    "\n",
    "            loss = loss_function(tag_scores, tgt, unk, pad)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "            #print(\"grad of lstm in\"  + str(model.encoder.lstm_in.weight_hh_l0.grad))\n",
    "\n",
    "            counter += 1\n",
    "            time_avg = time_avg * 0.95 + (time.time() - time_start) * (size_per_epoch - counter) * 0.01\n",
    "            print(\"New step : \", counter, \"/\", size_per_epoch, \" loss : \", loss.item(), \" estimated time :\", time_avg, \"grad :\", torch.abs(model.encoder.lstm_in.weight_hh_l0.grad.mean()).item(), end=\"\\r\")\n",
    "            \n",
    "        #here we can use the test data to evaluate the model\n",
    "        with torch.no_grad() :\n",
    "            losses = torch.zeros(int(usage_size * 0.2 / batch_size) + 1)\n",
    "            counter = 0\n",
    "            for src, tgt, src_valid_len, label in data.get_dataloader(train=1, seed=0, maxi=usage_size):\n",
    "                src.to(device)\n",
    "                tgt.to(device)\n",
    "\n",
    "                tag_scores = model(src)\n",
    "\n",
    "                loss = loss_function(tag_scores, tgt, unk, pad)\n",
    "                \n",
    "                losses[counter] = loss.item()\n",
    "                counter += 1\n",
    "\n",
    "            epoch_loss[epoch] = losses.mean()\n",
    "            epoch_lr[epoch] = lr1\n",
    "            print(\"Epoch: {}/{}.............\".format(epoch, n_epoch), end=\" \")\n",
    "            print(\"Loss: \" + str(epoch_loss[epoch]) + \".............\")\n",
    "            \n",
    "    return epoch_loss / np.max(epoch_loss / 100), epoch_lr / np.max(epoch_lr / 100), model.to('cpu')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, usage_size = 130000, valid=1) :\n",
    "    #sample a sentence from the test set\n",
    "    src, tgt, src_valid_len, label = next(iter(data.get_dataloader(train=valid, seed=0, maxi=usage_size)))\n",
    "\n",
    "    #translate the sentence\n",
    "\n",
    "    sentence = src.to(device)\n",
    "\n",
    "    tag_scores = model(sentence)\n",
    "    \n",
    "    #take most probable word\n",
    "    tag_scores = tag_scores[0].argmax(dim=1)\n",
    "    \n",
    "    print(tag_scores.shape[0])\n",
    "    \n",
    "    #convert to string\n",
    "    sentence = [token_to_word(tag_scores[i].item(), src=False) for i in range(0, tag_scores.shape[0])]\n",
    "    \n",
    "\n",
    "    #print the original sentence\n",
    "    print(\"Original sentence : \")\n",
    "    for word in src[0]:\n",
    "        print(token_to_word(word.item(), src=True), end=\" \")\n",
    "    print()\n",
    "    for word in src[0]:\n",
    "        print(word.item(), end=\" \")\n",
    "    print()\n",
    "\n",
    "    #print the translated sentence\n",
    "    print(\"Translated sentence : \")\n",
    "    print(\" \".join(sentence))\n",
    "    \n",
    "    #print the target sentence\n",
    "    print(\"Target sentence : \")\n",
    "    for word in tgt[0]:\n",
    "        print(token_to_word(word.item(), src=False), end=\" \")\n",
    "    print()\n",
    "    for word in tgt[0]:\n",
    "        print(word.item(), end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show(train_out, usage_size = 130000): \n",
    "    model = train_out[2]\n",
    "    epoch_loss = train_out[0]\n",
    "    epoch_lr = train_out[1]\n",
    "    \n",
    "    model.to(device)\n",
    "    test(model, target_fast_model, usage_size = usage_size)\n",
    "    #print loss and learning rate on the same graph\n",
    "    plt.plot(epoch_loss, label=\"loss\")\n",
    "    plt.plot(epoch_lr, label=\"lr\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.savefig(sys.path[0] + \"/models/\" + \"fast_trained_model_loss.png\")\n",
    "    model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise (Exception(\"Stop cell\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready\n",
      "Epoch: 0/30............. Loss: 17340.462890625.............ime : 0.2717098513050941 grad : 2.759818698905292e-06606\n",
      "Epoch: 1/30............. Loss: 17320.76171875.............e : 0.2740736002508781 grad : -0.000273492711130529649382\n",
      "Epoch: 2/30............. Loss: 17249.2421875............. : 0.2698733350199124 grad : -0.00012856263492722064-0551\n",
      "Epoch: 3/30............. Loss: 16341.5625.............ted time : 0.2861030104825463 grad : -0.000175259396200999625\n",
      "Epoch: 4/30............. Loss: 16231.029296875.............ime : 0.28288199686646415 grad : -0.00012388497998472303\n",
      "Epoch: 5/30............. Loss: 16264.48046875.............ime : 0.2812992060002881 grad : -0.0014253745321184397229\n",
      "Epoch: 6/30............. Loss: 16699.955078125.............ime : 0.2955533116785154 grad : 0.0011524690780788664385\n",
      "Epoch: 7/30............. Loss: 16035.9560546875.............ime : 0.3004344293878584 grad : 0.0002799890062306076305\n",
      "Epoch: 8/30............. Loss: 15930.912109375.............e : 0.29007133046299344 grad : -0.0003802342689596116554\n",
      "Epoch: 9/30............. Loss: 17431.771484375.............time : 0.27922845617197356 grad : 0.00087450433056801562\n",
      "Epoch: 10/30............. Loss: 16725.0703125............. time : 0.2797736454096971 grad : -0.00438604177907109343\n",
      "Epoch: 11/30............. Loss: 15884.412109375.............ime : 0.2885078862938771 grad : -0.000879989587701857135\n",
      "Epoch: 12/30............. Loss: 15894.2802734375.............me : 0.2850549995490376 grad : -0.00081808544928207995\n",
      "Epoch: 13/30............. Loss: 15792.6474609375............. : 0.28798824534482514 grad : -0.000279134226730093363\n",
      "Epoch: 14/30............. Loss: 15748.556640625............. : 0.315607387625815 grad : -0.000887032598257064876493\n",
      "Epoch: 15/30............. Loss: 15736.302734375............. 0.29914043682081265 grad : 0.0007125986157916486029527\n",
      "Epoch: 16/30............. Loss: 15906.6650390625.............e : 0.2816740922021573 grad : -0.001777678495272994483\n",
      "Epoch: 17/30............. Loss: 15710.669921875.............e : 0.2880451596782834 grad : -0.0022264004219323397243\n",
      "Epoch: 18/30............. Loss: 15668.8203125.............me : 0.2832796749950739 grad : -0.00202079280279576828923\n",
      "Epoch: 19/30............. Loss: 15702.7900390625.............e : 0.27957292162846237 grad : -0.00071672059129923585\n",
      "Epoch: 20/30............. Loss: 15613.5478515625.............: 0.2825383166182662 grad : -0.004019355401396751501865\n",
      "Epoch: 21/30............. Loss: 15619.068359375.............ime : 0.29984669961671834 grad : -3.520603058859706e-05\n",
      "Epoch: 22/30............. Loss: 15586.7255859375.............me : 0.4179198305675784 grad : -0.00060993432998657234\n",
      "Epoch: 23/30............. Loss: 15583.4765625............. time : 0.2989011527065307 grad : -2.3399543351843022e-05\n",
      "Epoch: 24/30............. Loss: 15547.548828125.............ime : 0.2977117102519654 grad : 0.0010847636731341481621\n",
      "Epoch: 25/30............. Loss: 15522.4580078125.............me : 0.30236706141010533 grad : 6.778835813747719e-057\n",
      "Epoch: 26/30............. Loss: 15657.16796875.............time : 0.3004388892465404 grad : 0.001263810670934617566\n",
      "Epoch: 27/30............. Loss: 15499.9912109375.............e : 0.2941900866760026 grad : -0.0003622471122071147336\n",
      "Epoch: 28/30............. Loss: 16241.615234375.............e : 0.30891906612364795 grad : 0.0003545243816915899534\n",
      "Epoch: 29/30............. Loss: 15572.4892578125.............me : 0.3213226008514555 grad : -0.00054277456365525727\n",
      "15\n",
      "Original sentence : \n",
      "there is nothing to worry about . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
      "6459 3444 4270 6542 7174 34 7 20 21 21 21 21 21 21 21 \n",
      "Translated sentence : \n",
      "<bos> je ' ' ' ' . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
      "Target sentence : \n",
      "<bos> il n ' y a pas de quoi s ' inquiéter . <eos> <pad> \n",
      "20 4981 6357 4 10697 25 6976 2700 8022 8927 4 5184 7 21 22 \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAioAAAGdCAYAAAA8F1jjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAABGPklEQVR4nO3deXhTdb4G8Dd7m7ZJ96YphRYoaxEREKkLlaWKijggoox3QGYcVHTkuqDIqDhqEVSGGVFcxosoot65I7igsggUEUFWgbJDN6ClUNom3ZI2OfePNIFCgaY9yclJ38/z5EmanKbfHg/2zW9VCIIggIiIiCgAKaUugIiIiOhSGFSIiIgoYDGoEBERUcBiUCEiIqKAxaBCREREAYtBhYiIiAIWgwoREREFLAYVIiIiClhqqQtoDafTiZMnTyIiIgIKhULqcoiIiKgFBEGA1WqF2WyGUtmythJZBpWTJ08iOTlZ6jKIiIioFYqKitChQ4cWHSvLoBIREQHA9YsaDAaJqyEiIqKWsFgsSE5O9vwdbwlZBhV3d4/BYGBQISIikhlvhm1wMC0REREFLAYVIiIiClgMKkRERBSwZDlGhYiIyN8EQUBDQwMcDofUpQQ0jUYDlUol2vsxqBAREV2B3W5HcXExampqpC4l4CkUCnTo0AHh4eGivB+DChER0WU4nU7k5eVBpVLBbDZDq9VysdFLEAQBp0+fxvHjx5GWliZKywqDChER0WXY7XY4nU4kJydDr9dLXU7Ai4uLQ35+Purr60UJKhxMS0RE1AItXfK9vRO7tYlnnYiIiAKW10Flw4YNGDVqFMxmMxQKBZYvX97kdUEQMGvWLJjNZoSGhiIzMxO5ublNjrHZbHjssccQGxuLsLAw3HnnnTh+/HibfhEiIiIKPl4HlerqavTt2xcLFixo9vW5c+di3rx5WLBgAbZu3QqTyYQRI0bAarV6jpk2bRqWLVuGzz//HBs3bkRVVRXuuOMOTvkiIiISSWZmJqZNmyZ1GW3m9WDakSNHYuTIkc2+JggC5s+fj5kzZ2LMmDEAgMWLFyMhIQFLly7FlClTUFlZiQ8//BCffPIJhg8fDgBYsmQJkpOTsWbNGtxyyy1t+HWIiIgomIg66ycvLw8lJSXIysryPKfT6TBkyBBs2rQJU6ZMwfbt21FfX9/kGLPZjPT0dGzatKnZoGKz2WCz2TxfWywWMcs+p+o08NObvnlvCjoCBBw8VQVnz9HoNSjryt9AREReEzWolJSUAAASEhKaPJ+QkICCggLPMVqtFlFRURcd4/7+C82ePRsvvfSSmKU2r64S2LLQ9z+HgoICQA8Ax/LWoihtO5KjOW2RqD0QBAG19dIMVQjVqFo1q6a8vByPP/44vvnmG9hsNgwZMgT//Oc/kZaWBgAoKCjAo48+io0bN8JutyMlJQWvv/46brvtNpSXl+PRRx/FqlWrUFVVhQ4dOuC5557DAw88IPav1yyfrKNy4UkUBOGKJ/Zyx8yYMQNPPPGE52uLxYLk5OS2F3qh0CjgxifFf18KSr8eyMe1p/+DeJzF1OV78dEDA7kIFFE7UFvvQK8XVkrys/f97Rbotd7/6Z40aRIOHz6Mr7/+GgaDAc888wxuu+027Nu3DxqNBlOnToXdbseGDRsQFhaGffv2eVaWff7557Fv3z58//33iI2NxZEjR1BbWyv2r3ZJogYVk8kEwNVqkpiY6Hm+tLTU08piMplgt9tRXl7epFWltLQUGRkZzb6vTqeDTqcTs9RmVamN+FL/B4RqVNBr1dDrVNCf/1irgl6jRqhWBa2aM7vbM0EQ8MK2b/ED/oNwRR02HzqBb3Z3wJ19zVKXRkTUhDug/Pzzz56/s59++imSk5OxfPlyjBs3DoWFhRg7diz69OkDAOjcubPn+wsLC9GvXz8MGDAAAJCSkuLX+kUNKqmpqTCZTFi9ejX69esHwLWiX05ODubMmQMA6N+/PzQaDVavXo177rkHAFBcXIy9e/di7ty5YpbjtVJLHV74KvfKBwLQqBTnAo1W1Rhq1IgIUaNTTBi6xoejS5zrPibc9yGL/OtIaRUOlCtQr1NBo3AgElX42ze5uCktFpF6rdTlEZEPhWpU2Pc3aSZ+hGq8X+l1//79UKvVGDRokOe5mJgYdO/eHfv37wcA/OUvf8HDDz+MVatWYfjw4Rg7diyuuuoqAMDDDz+MsWPHYseOHcjKysJdd911yYYFX/A6qFRVVeHIkSOer/Py8rBr1y5ER0ejY8eOmDZtGrKzs5GWloa0tDRkZ2dDr9djwoQJAACj0Yg//vGPePLJJxETE4Po6Gg89dRT6NOnj2cWkFR0GhVGpptQY3eg1u5ATX0DamwO1NgdqLE3oMbuQINTAADUOwTUOxpgqWu44vtG6TXoGh/eGF7C0SU+HF3jwpEUGQqlkl0FcvTjgVIAClSrDIh0luPqGAdWltmR/d1+zL27r9TlEZEPKRSKVnW/SEUQhEs+7+6u/tOf/oRbbrkFK1aswKpVqzB79my8+eabeOyxxzBy5EgUFBRgxYoVWLNmDYYNG4apU6fijTfe8Ev9CuFSv8ElrF+/HjfffPNFz0+cOBEfffQRBEHASy+9hPfeew/l5eUYNGgQ3n77baSnp3uOraurw9NPP42lS5eitrYWw4YNwzvvvNPicScWiwVGoxGVlZUwGAzelN9m9gbnuRBjdzQGmQbU1LvCTXmNHXmnq3HkdBWOlFbhePml+/FCNSp0bmx16doYYAakRCE+IsSPvxG1xj3v/oJf889iZ+zziKo6ikO3fIqsr1z/4Jc+OAgZXWIlrpCIxFJXV4e8vDykpqYiJEQ+/3/OzMzE1VdfjalTp6Jbt25Nun7KysqQnJyMjz/+GHffffdF3ztjxgysWLECu3fvvui19957D08//fQlZ+Be7ny15u+315EwMzPzkukMcCXNWbNmYdasWZc8JiQkBG+99Rbeeustb3+85LRqJbRqJYzQtOj4WrsDR09X4WhjcDlS6nqcd6YatfUO5J60IPfkuf/Y0WFarHsqE8bQlr0/+V95tR3bCs4CAPTGeKDqKLpF2HH/dWlYsrkQM5ftxfeP34iQVjTREhGJLS0tDaNHj8aDDz6I9957DxEREXj22WeRlJSE0aNHA3AtxDpy5Eh069YN5eXlWLt2LXr27AkAeOGFF9C/f3/07t0bNpsN3377rec1f5BP25VMhWpVSE8yIj3J2OT5BocThWdrXOGlMcRsOHQGZ6ps+OjnfDw+PE2iiulKcg6dhlMAepgioDPEAicA1JRh+q2jsHrfKeSdqcaCtUfw1C3dpS6ViAgAsGjRIjz++OO44447YLfbcdNNN+G7776DRuP6UOxwODB16lQcP34cBoMBt956K/7+978DALRaLWbMmIH8/HyEhobixhtvxOeff+632r3u+gkEUnb9+NI3v53EY5/thCFEjY3PDoUhhK0qgejRpTvw7e5iPJLZBdPrFwLbPwIynwMyn8EPe0vw0JLtUCsVWPGXG9HdFCF1uUTURnLt+pGK2F0/nGMbQG7rk4gucWGw1DXg4035UpdDzah3OJFz6DQAYFjPBEAf43qh1tUVdGu6CVm9EtDgFPDsl7vhcMrucwARUUBhUAkgKqUCjw11dfn8a2MeqmxXnlFE/rU1/yysdQ2IDtPi6uTIc0GlpsxzzN9GpyNcp8bOwgp8uqVAmkKJiIIEg0qAGdXXjM6xYaioqcfHv+RLXQ5dYO3+UgDAzd3joVIqgNBo1wvnBRWTMQTP3OoanzL3h4MorvTfCo5ERMGGQSXAqJQKTL25KwDgXz/loZqtKgHFtX4KMKxnvOuJZlpUAOD3gzrhmo6RqLI14IWvci87U46IiC6NQSUAjb7ajE4xepyttmPJZnYdBIpjjdPKNSoFbkxrXCfFE1TKmxyrVCowe8xVUCsVWL3vFFbmNr/hJhERXR6DSgBSq5SeVpX3NxxDrV2aXTqpqbWNrSmDUmMQ4Z6Rpb+468etuykCDw3pAgB44atcVNbW+6VOIqJgwqASoH7XLwnJ0aEoq7ZzQGaAWLP/FABgaI/4c0+6g0p9NVB/8ViUR4d2RWpsGEqtNsz94YA/yiQiCioMKgFKo1JiaqarVeW9DcdQV89WFSlV1tZja76re8czPgUAdAZA2bhuYs3Zi74vRKNC9u9cu5F+uqUQW/MvPoaIiC6NQSWAjbmmA5IiQ3HaasNnvxZKXU67lnPoNBxOAV3jw9EpJuzcCwrFRWupXGhwlxjcM6ADAGDGl3tga2DoJCLfy8zMxLRp06Quo80YVAKYVq3EIze7xji8m3OUrSoSWtvY7dOkNcXtEjN/zvfcbT0RG67FkdIqvLv+mC9KJCIKSgwqAe7u/h2QaAzBKYsN/7utSOpy2qUGhxPrDjauRtsj4eIDmllL5UKRei1eGNUbAPD2uiM4Ulolep1ERC1lt9ulLqHFGFQCnE6twsOZrlaVheuPsttAAjsKK1BZW49IvQbXdIy8+ADPzJ/Ljz8ZdVUiMrvHwe5w4rkv98DJ5fWJyE9SUlLwyiuvYNKkSTAajXjwwQelLqnFGFRk4J4ByUgw6FBcWYd/bzsudTntzo+N3T6Z3eKgVjXzT8bT9XP5oKJQKPDy6HSEalT4Nf8svmALGZE8CQJgr5bm1obFI19//XWkp6dj+/bteP7550U8Ib6llroAurIQjQoPDemCl77Zh4Xrj+KeAcnQqpkx/eXcarTNdPsALRqj4pYcrceTWd3wyor9yP5uP4b1iEe8gbuxEslKfQ2QbZbmZz93EtCGXfm4ZgwdOhRPPfWUyAX5Hv/aycR913ZEXIQOJypq8eUOtqr4S0FZNY6UVkGlVOCmbnHNH3SZRd+aMykjBX2SjLDWNeClb/aJVCkR0eUNGDBA6hJahS0qMhGiUWHKTZ3xyor9WLDuCMb27wBNc90QMuR0CqiyN8DgXu01gPzYuAnhwJQoGEMvUZ8XLSqAa+Xh2WP6YPTbP2PFnmLcfbAUN3dvZjYREQUmjd7VsiHVz26lsLDWtcRILTj+0rUTvx/UCbHhWhwvr8WynSekLkcUpyx1GPmPnzA4+0f8VlQhdTkXcS+bP/xS3T7AFddRaU56khEPZKQAAOZ8fwAODqwlkg+FwtX9IsVNoZD6t/c7BhUZCdWq8OebOgNwTXFtcDglrqhtTlbUYvx7v+DgKSuq7Q48+e/fAmqtGGtdPbbkuVpJmiybf6EWzvq50KNDuyIiRI0DJVZ8/VtwBE8iIrExqMjM/dd1QnSYFgVlNfhql0RNjyIoOluDe977BfllNegQFYrYcB2OlFZh3upDUpfm8dPhM6h3COgcG4bOceGXPrAF66g0J1Kv9Wxa+OaqQ5x6TkTUDAYVmdFr1XjwRleryoJ1R2TZZZB3phrj3/sFx8trkRKjxxdTBmP2GNd+OB/8dAzbCwJjP5xmNyFsjrvrp76m2Y0JL2fy9amIj9DheHktPtvCbRKISDzr16/H/PnzAQD5+fmyXU6fQUWG/mtwJ0TqNcg7U41vfpNXq8qRUivGv/cLTlbWoUtcGL6YMhhJkaEY0SsBY65JgiAAT/17N2rt0rYuOJwC1rtXo73c+BQA0EUAysaBtl52/4RqVfjLsDQAwFtrj6DK1uB1rUREwYxBRYbCdWr86YZUAMBbaw/LplXlYIkV976/GaVWG7onRODzPw9GwnlriLw4qjdMhhDknanGnB8OSFgpsKuoHGer7YgIUWNAStTlDz5/Y0Ivu38AYPzAZKTE6FFWbceHP+W1oloiouDFoCJTEzNSYAhR4+jpany3p1jqcq5o74lK3Pv+LzhTZUevRAM++/N1iIvQNTnGGKrBa2NdXUAfbcrHL0e9/6MvFve05CHd4lo2DdzLtVTOp1Ep8WRWdwCurq+yKpvX70FEFKwYVGQqIkSDP97gGqvy1trDAb1vzK6iCkz4YDPKa+rRt4MRnz14HaLDtM0em9k9HvddmwwAePr/fkO1RF0h7qBy2WnJ52tDiwoA3N4nEelJBlTZGvD2uqOteg8iomDEoCJjk65PQUSIGodOVeGH3BKpy2nWtvyzuP9fW2Cpa0D/TlFY8qdBMOovv7DbzNt7ISkyFMfLa5H93X4/VXpO0dkaHDxlhVLhalFpEXeLSm15q36mUqnA9Ft6AACWbC7A8fKaVr0PEVGwYVCRMWOoBg9c7xqr8s8fA69V5ZejZfjD//yKKlsDBqVG4+PJ1yKiBavPhuvUeP3uqwAAn24pxE+HT/u61CbWHXS1pgzoFI2oS7T8XKSVU5TPd2NaLDK6xMDucOLvqw+3+n2IyDeENmwI2J6IfZ4YVGTuj9enIlznWjRs1b5TUpfj8dPh03jgo19RY3fgxrRYfPTAtQjTtXzHhoyusfjD4E4AgOn/txuWunpflXqRNY3dPkN7erGsfRu7fgDX7srTb3W1qny58zgOllhb/V5EJB6NxvUBq6aGLZ0tYbfbAQAqlUqU9+NePzJn1GswKSMFC9YdwT9/PIxbeidAIfESy+sOlGLKku2wNzhxc/c4LLy/P0I03l+wz47sgZxDp1FQVoOXv9mH18f19UG1TVXbGrC5cRDvcD8HFQC4OjkSI9NN+H5vCV5feRD/mijPTcSIgolKpUJkZCRKS10fYvR6veT/nw1UTqcTp0+fhl6vh1otTsRgUAkCf7whFYt+zsO+YgtmLt+LET0TMCAlqkXdLGJbmVuCR5fuQL1DQFavBLw1oR906talar1WjTfG9cU97/2Cf28/jpF9TBjao4WDW1vpp8NnYHc40TFajy6XW432Qp6g0vbF6p7M6o6VuSVYs/8UthecRf9O0W1+TyJqG5PJBACesEKXplQq0bFjR9HCHINKEIgK0+KPN6Tin2uPYOmWQizdUgilAuhtNmJQajQGdY7BtSnRVxzE2lbf7j6Jxz/fBYdTwO1XJWL++KvbvMPzwJRo/PH6VPxrYx6e/c8erPrvKETqWzhupBXWHnB1nw3rGe/dP7I2TE++UNf4cNwzIBmfby3CnO8P4osp1/HTG5HEFAoFEhMTER8fj/p6/3VFy5FWq4VSKd7IEgaVIDFteDekJURg4+Ez2JxXhoKyGuw5UYk9Jyrxr415UCiAHiYDruscjUGpMRiU6sVA0SsQBAHLdp7AU//+DU4B+F2/JLx+91VQtzGkuD11S3esO1iKo6erMevrXMy/t58o73shp1PA2gONq9F623LTyo0JL+Xx4WlYtvMEfs0/i/UHT+PmKy3jT0R+oVKpRBt7QS3DoBIklEoFRvU1Y1RfMwCgpLIOW/LKsPnYWWzJK8Ox09XYX2zB/mILFv2cDwDonhCBQe7g0jkaseE61DucqKipR3mNHeXVdpTX1KOi5vx712PXa3ZU1NSjorbeszruPQM6YPaYq6BSitcCEKJR4Y1xfTF24SYs33USt6Yn4tZ0k2jv77b7RCXOVNkQrlPj2lQvu1tEGqPilmgMxaSMFLy34Rjm/HAAQ7rFQSniOSWi4FZla0BJZS26xkdIXUqbMagEKZMxBKOvTsLoq5MAAKWWOmzJc4WWLcfO4nBpFQ6esuLgKSs+/qUAgGtacGv3mlEqgAeuT8XM23r65A9qv45RmDKkCxauP4qZy/ZgYEoUYsJ1V/5GL/zYuAnhTd1ioVV72RrkDioNtYC9BtDq21zPw5ldsPTXQhwoseLr307irn5JbX5PIgp+DqeA//pwC3YWVuDd+6/BremJUpfUJgwq7US8IaRJi0tZlQ2/5p3F5mNl2JJ3FgdKrJ6QolC41miJ0msRqW96H6XXIFKvRXTY+c+5HrdmZo83pg1Pw9r9pTh4yooXvsrF27+/RtT3d69G26oBu9pw18aEznqg9qwoQSVSr8VDQ7rg9ZUH8ebqg7itT6L3AYqI2p3Pfi3EzsIKAMDMZXsxMCVa9A92/sSg0k7FhOswsk8iRvZxJe2KGjvOVtsRpdfCEKoRtetGLDq1Cm/e0xd3vf0zVuwpxq2/nfQEr7YqrqzFvmILFArg5u4tXI32fO6NCatKXN0/xg6i1PXA9Sn4aFM+is7W4rNfCzExI0WU9yWi4FRWZcPrKw8CAMK0KpRV233ywc6f+PGMALg+vXeOC0dUmDYgQ4pbepIRU2/uCgB4/qu9KLXWifK+7taUfsmRrf/kIfI4FcA1Rfsvw9IAuPZ0kmrvIyKSh9e+P4DK2nr0SjRg6YPXQaVUYMWeYny7+6TUpbUagwrJzqNDu6K32YCKmno89+VeUZZrXnvAFVSGtXQTwuaIPPPH7d6ByegUo8eZKjs+3Jgn6nsTUfDYXnAW/95+HADw8l3p6JsciamZXQAAL3yVizMy3ZmdQYVkR6NS4s17+kKjUmDN/lNYtvNEm96v1u7Az0fOAHCtn9JqPgoqGpUST2Z1BwC8v+EYzlbbRX1/IpK/BocTf12eCwAYPyAZ/TtFAQAeHZqGHqYInK224/nl4nyw8zcGFZKlHiYDpg3vBgB48etcFFfWtvq9fj5yBrYGJ5IiQ9E9oQ1T+XzQ9eN2R59E9DYbUGVrwDvrjoj+/kQkb59sLsD+YguMoRpMv7W753mt2vXBTq1U4Pu9Jfh2d7GEVbYOgwrJ1pSbOqNvciSsdQ3IfH09xr/3C95YeRDrD5Z6tYnhj61djfZC7qBSK26LCuBaJ8e9YeHHvxTgREXrgxkRBZdSSx3mrToEAJh+a/eLxtn1Nhvx6FDX2L4XvtqL01Z5dQExqJBsqVVKzLunLzpG62FrcGJL3lksWHcEkxZtRd+XVmHkP37CC1/txde/nbxki4sgCOdNS27j6q8+bFEBgJvSYjG4cwzsDifmrz7kk59BRPIz+/sDsNoa0LeDEfcO7NjsMVNv7opeiQaU19Tjr8v3yKoLiNOTSda6xIUj5+lMHD1djW35Z7E1vxzbCs6ioKzGsxKve0G7pMhQDEyJwoCUaAxMiUZafDhyT1pQarVBr1Xhus4xbSsmVLz9fpqjUCgw/dbu+N07m/CfHcfx55s6I60tXVVEJHubj5Vh2c4TUChcA2gvNWtTo1LijXF9ceeCjViZewpf/3bSsyBooGNQIdlTKBToGh+OrvHhuPda16eJUksdthWUY2v+WWzLL0fuyUqcqKjFiV21WL7LNU3PEKJGbGMT6Q1dY9u+YJ2PW1QA1wq9t/Y24YfcEry+8iDe/8MAn/0sIgps9Q4nXvhqLwBgwrUdcVWHyMse38tswF+GpWHe6kN44atcDO4cg3hDiB8qbRsGFQpK8YYQ3NYnEbc1LmhXZWvArsIKV3ApOIsdBRWw1DXAUudal2R4rzZMS3bzzPopb/t7XcZTt3TDqn0lWLXvFLYXlHtG97sJgoB6h4B6h7Px5nrc4BBgb3yuweFq9u0aH45QbfvYYM3hFPDT4dPIOXQaPU0G3Hm12eerKRP50kc/5+PQqSpEh2nx9C3dr/wNcG3NsWpfCfaesOC5ZXvxwR/6B/zu7Awq1C6E69S4IS0WN6TFAnB9EtlfbMHW/HLU1TvwOzH20fFDiwoAdI2PwLj+yfhiWxH+68MtCNWoYG8MH/UOJxqcLe97VisV6G024JpOUejfeEs0hvqwev/LO1ONf28rwpc7TqDEcm6BwDk/HMDvB3XE/dd1ksWnSqLzlVTWYf4a11i1Z2/tgUi9tkXf5+4CGvXWRqzZfwrLd53A7/qJs5K2rygEOY2oaWSxWGA0GlFZWQmDwSB1OUQuNiswu/Ef/HPFouz3cyknK2pxy983wNrClWq1KiXUKgU0KmXjTQF7gxNlzazJYjaGNAkuPRMN0KjkNe6+ytaA73YX49/bi7A1/1wLV6Reg2E9ErD5WJln5pRGpcCoq8yYfEMq0pOMUpVM5JWpS3dgxe5iXNMxEv/3UIbXm8EuWHsYb6w6BEOIGqufGIIEP4X11vz9ZlAhEosgAK/EAw47MG0vEJns0x932mrDKUudJ3icH0LUKiW0jY9VSkWzTbuCIOBERS22F5RjR0E5theWY3+xFY4LWmRCNEr07RDpCS7XdIxCVFjLPr35kyAI+DXPtTLnd3uKUWN3AHDt7D2kWxzGDUjGsJ7x0KlVaHA4sWrfKfzPxjxsKzgXZK5NicbkG1IwopcpoLeSoPZt4+EzuP/DLVAqgG8euwG9zd4H7AaHE797ZxP2nKjEsB7x+NfEAX7pAmJQIZLamz0AazEwZQOQ2FfqarxWbWvAb8crXMGl8eYex3O+znFh6JloQEJECOINOsRH6JBgCEF8hA7xESEwhKr91u99sqIWX+44jv/bfhz5ZTXnaowNw90DOmBMvw4wGS/9afG3ogos+jkP3+4u9nSbdYgKxaSMFNwzMBmGEI3PfweilrI1ODBy/k84dqYakzJSMOvO3q1+r0OnrLjjnxthdzjxxri+uLu/77uAAiaoWK1WPP/881i2bBlKS0vRr18//OMf/8DAgQMBuD75vPTSS3j//fdRXl6OQYMG4e2330bv3i074QwqFLAWXg+c2gv81zKgy1Cpq2kzp1PAsTNV2JbfGFwKy3HsdPUVv0+nViKuSXjRId4QgrjGx3EROui1amjVrpYfrVoJXePjljRh19U7sHrfKfzvtiJsPHIG7v+LhWlVuOMqM8YN6ID+naK8CkunLHX45JcCfLqlAOU19Z73GzcgGRMzUpAaG9bi9yLylbfXHcHrKw8iNlyHtU8NaXOQfmf9Ecz94SAiQtRY/d9DLhvqxRAwQWX8+PHYu3cvFi5cCLPZjCVLluDvf/879u3bh6SkJMyZMwevvvoqPvroI3Tr1g2vvPIKNmzYgIMHDyIi4srrQjCoUMD66A4g/ydg7IdAn7ulrsYnzlbbsbOwHPllNSi11KHUakOptQ6lFldXVHMtMN5QKxWuAHNeiHE/1jU+PnSqCpW151YfHpQajXEDknFbHxP02rbNEaird2D5zhNY9HM+Dp6yAgAUCmBo93hMviEVGV1iAn6WBAWn4+U1GD4vB3X1Tvx9fF9RBsE2OJwY++4v+K2oAjd3j8P/TBro0+s7IIJKbW0tIiIi8NVXX+H222/3PH/11VfjjjvuwMsvvwyz2Yxp06bhmWeeAQDYbDYkJCRgzpw5mDJlyhV/BoMKBaz/nQjsWw6MnAsMuvK1HIzq6h04fUF4cYWZxpulDmeqbKird8Le4ITd4WzVzzEbQzC2fwfc3b8DOsWI39ohCAI2HS3D/2zMw4+Nu2sDQHJ0KDpG6xEXrkNsuKt1qMktXIcovdbrwY2AqwXLWteA8ho7ztbYUVFjx9nqelTU2FFeY0eDQ0B3UwT6JBnROS6c42jamSmfbMPK3FO4NjUaX/z5OtECxeFTVtz+1kbYG5yYe/dVuGeA78bXtebvt+jTkxsaGuBwOBAS0rT5KDQ0FBs3bkReXh5KSkqQlZXleU2n02HIkCHYtGlTs0HFZrPBZju3N4HFYhG7bCJxeKYoi7/fj1yEaFRIjtYjObpls54EwbW+i73hXHBxP7Zd8LX79Si9FtemRvv0D7VCocD1XWNxfddYHDtdhcWb8vHv7cdRdLYWRWcvv9eSSqlAbLj2XJBpvI8N18HucKK8xo7yajvKa+obw4gdFTX1KK+xo6Wzy/VaFXolGpCeZESfJCP6dDCiC8NL0Fp3sBQrc09BpVTg5dHporZ6pCVE4IkR3fDa9wfw8jf7cGNabEAtUyB6UImIiMDgwYPx8ssvo2fPnkhISMBnn32GLVu2IC0tDSUlJQCAhISmC2wlJCSgoKCg2fecPXs2XnrpJbFLJRKfn9ZSCSYKhQI6tQo6deAuvtY5LhwvjU7Hk7d0x+6iSpypsuG01YbT7nvrua/PVtvhcAo4ZbHhlKV1m7+FaVWI1GsRFaZBlF6LKL0W0WFaCIKAfcUW5J60oMbuwLaC8iazlkI1KvQyG9AnyegJMF3iwqCW2fRyOWtwOLGtoBw/7j+FHw+UotrWgBG9EjD66iT07xjVqpa2unoHZn2dCwCYfH0KupvE3zrjwRs7Y2VuCXYWVuCZ/+zB4gd82wXkDZ8s+PbJJ59g8uTJSEpKgkqlwjXXXIMJEyZgx44dnmMuPAGCIFzypMyYMQNPPPGE52uLxYLkZN9O/SRqFb1v9/shaRlCNJ5FAy+l3uHE2Wr7RQHGfa9TKRGp1yI6TNN4r0WkXuMJI5F6zRVDm8MpIO9MFfacqMSe4xbsPVGJ3JOVqLY7PLO13EI0SvRKdIWX6DAdHIIAp1M4d+8U0OAU4BRcj933DifgcDrhEFxdUgBgCNU01qpBZKir1ki9+971nFbd/kJRZW09cg6dxo/7T2H9wdNNxk8BwJLNhViyuRBJkaEY1deMO/ua0TMxosVB4N2coygoq0GCQYfHh3fzxa8AlVKB1+/ui9v++RM2HDqN/91WhPGX2ODQ33wSVLp06YKcnBxUV1fDYrEgMTER48ePR2pqKkwmEwCgpKQEiYmJnu8pLS29qJXFTafTQafTNfsaUUBhi0q7p1EpkWAI8ekCWiqlAl3jI9A1PgK/6+d6zhVeqrH3RKUrwJyoRO4JV3jZUViBHYUVPqvnfO7WIE940WsRGapBqEYFpVIBhQJQKhRQNt4rFAoocN5zFxyjgGstoEi9BlFhWsSEuVqYYsK1CNWoJPvUn3+mGmv2n8KP+0uxNf9skxWho/Qa3Nw9HkN7xiNcp8a3u4vxw94SnKioxbs5R/FuzlGkxYdj9NVm3Nk3CR1jLt1NWlhWg3fWHwUA/PX2XgjX+W5B+a7x4Xg6qzte/W4/Xv52P25Ii0NSpPRdQD5dQj8sLAxhYWEoLy/HypUrMXfuXE9YWb16Nfr1c/0Ls9vtyMnJwZw5c3xZDpHvuVtUatvvGBWShiu8uDbnvKtxSwinU0BeWXVji4sFVbYGqBSuP/xKhQIqpSsYqJUKqBQKKM+/b/IcIACw1LoG+lbWusbTVDSOsamorUdlbT0EAai2O1Btr/Ws/OtLOrUSMWFaRIc3hpcw7bkwc16oiQrTIlynRniIGuFadau6XxocTuworMCP+09hzf5TOHrBNP2u8eEY1jMew3sm4JqOUU3GCmV2j8crd6Vj3YFSfLXrJNYeLMXh0iq8seoQ3lh1CP06RuLOvmbcflUi4iPOBVxBEDDrm1zYG5y4vmsM7rgqEb42+YZU/JBbgu0F5Xj2P7vx8eRrJe8C8sn05JUrV0IQBHTv3h1HjhzB008/DZ1Oh40bN0Kj0WDOnDmYPXs2Fi1ahLS0NGRnZ2P9+vWcnkzyd2IH8MHNgCEJeGKf1NUQ+Y3DKcBaV+8ZIFxRU4+KWvcg4XrY6h0Q4ApPTgFwCgIEQXA9J7ieEwQBTicg4PxjXN1plbX1KKtyzX4qq7bD3tC62WKAa++vcJ0aESGN4UWnhiFE4wkzESHnXlcoFPjlaBnWHSxFRc25Lh21UoFrU6MxrGcChveM92rmmaWuHiv3luDr307i5yNnPAOolQrg+q6xGNXXjFvTTdh8tAx//mQ7NCoFvn/8JnSND2/17+yNY6erMPIfP6FPkhEfThoIY6h4ix4GxKwfAKisrMSMGTNw/PhxREdHY+zYsXj11Veh0bh+2enTp6O2thaPPPKIZ8G3VatWtSikEAW087t+BMG1AAdRO+DqntE2bo7n28XxBEFAjd2Bs9X2i25l1a4ZVWXVrlBzttrVAmStq0d9467hVbYGVNkaUOLlBNLIxi6dYT3jcVO3uFYvtmYI0WDcgGSMG5CMUmsdVuwuxle7TmJXUQV+OnwGPx0+g78u3wtd43ifP93Y2W8hBXANHv/PwxnomWgIiFlkXEKfSEy2KmB2407Mz50EtFzNlChQ1NU7XCGlrgHWugZYbfWex+7wYqlzPVdlcz1fa3fgqg5GDOuZgGs6Rvp0BlVBWTW++e0klu86iSOlVQBc6wWteXJImxcyDBQB06JC1G5pwwCVDnDYXGupMKgQBYwQjQohGhViwwNzckanmDA8OjQNU2/uiv3FVmw4fBo3d48PmpDSWu37tycSm0LhGlBrLXZ1//h4B2UiCj4KhQK9zAb0MrPHAADa34R3Il/jFGUiItEwqBCJzbPoG6coExG1FYMKkdjcLSpcS4WIqM0YVIjEFspl9ImIxMKgQiQ2jlEhIhINgwqR2BhUiIhEw6BCJDZPUOEYFSKitmJQIRKbPsp1z6BCRNRmDCpEYmPXDxGRaBhUiMR24caERETUagwqRGJzBxWHDaivkbYWIiKZY1AhEptG79qYEGD3DxFRGzGoEIlNoeA4FSIikTCoEPkCgwoRkSgYVIh8wbMxYbm0dRARyRyDCpEv6LnfDxGRGBhUiHyBXT9ERKJgUCHyBQYVIiJRMKgQ+YI7qNRyGX0iorZgUCHyhVCOUSEiEgODCpEveAbTskWFiKgtGFSIfIFjVIiIRMGgQuQLnqBylhsTEhG1AYMKkS+4u34cNsBeLW0tREQyxqBC5AsaPaAOcT1m9w8RUasxqBD5AjcmJCISBYMKka+4pyhzLRUiolZjUCHyFU5RJiJqMwYVIl9h1w8RUZsxqBD5CoMKEVGbMagQ+Qq7foiI2oxBhchX2KJCRNRmDCpEvsKgQkTUZgwqRL7Crh8iojZjUCHyFa6jQkTUZgwqRL5yftcPNyYkImoVBhUiX3EHFYcdsFdJWwsRkUwxqBD5ilYPqENdjzmgloioVRhUiHyJA2qJiNqEQYXIlxhUiIjahEGFyJe4lgoRUZswqBD5EoMKEVGbMKgQ+RLXUiEiahMGFSJfYosKEVGbMKgQ+RKDChFRmzCoEPkSZ/0QEbUJgwqRLzGoEBG1iehBpaGhAX/961+RmpqK0NBQdO7cGX/729/gdDo9xwiCgFmzZsFsNiM0NBSZmZnIzc0VuxQi6bHrh4ioTUQPKnPmzMG7776LBQsWYP/+/Zg7dy5ef/11vPXWW55j5s6di3nz5mHBggXYunUrTCYTRowYAavVKnY5RNLixoRERG0ielD55ZdfMHr0aNx+++1ISUnB3XffjaysLGzbtg2AqzVl/vz5mDlzJsaMGYP09HQsXrwYNTU1WLp0qdjlEEnLPT3ZWQ/YGMSJiLwlelC54YYb8OOPP+LQoUMAgN9++w0bN27EbbfdBgDIy8tDSUkJsrKyPN+j0+kwZMgQbNq0qdn3tNlssFgsTW5EsnD+xoRcS4WIyGtqsd/wmWeeQWVlJXr06AGVSgWHw4FXX30V9913HwCgpKQEAJCQkNDk+xISElBQUNDse86ePRsvvfSS2KUS+Yc+BrAcd3X/RKVIXQ0RkayI3qLyxRdfYMmSJVi6dCl27NiBxYsX44033sDixYubHKdQKJp8LQjCRc+5zZgxA5WVlZ5bUVGR2GUT+Q5n/hARtZroLSpPP/00nn32Wdx7770AgD59+qCgoACzZ8/GxIkTYTKZALhaVhITEz3fV1paelEri5tOp4NOpxO7VCL/4MwfIqJWE71FpaamBkpl07dVqVSe6cmpqakwmUxYvXq153W73Y6cnBxkZGSIXQ6R9NiiQkTUaqK3qIwaNQqvvvoqOnbsiN69e2Pnzp2YN28eJk+eDMDV5TNt2jRkZ2cjLS0NaWlpyM7Ohl6vx4QJE8Quh0h6bFEhImo10YPKW2+9heeffx6PPPIISktLYTabMWXKFLzwwgueY6ZPn47a2lo88sgjKC8vx6BBg7Bq1SpERESIXQ6R9BhUiIhaTSEI8luFymKxwGg0orKyEgaDQepyiC5vy/vA908DPe8Exn8idTVERJJpzd9v7vVD5GvuMSq15dLWQUQkQwwqRL7Grh8iolZjUCHyNQYVIqJWY1Ah8jXP9GRuTEhE5C0GFSJf82xM2MCNCYmIvMSgQuRrWj2g0bses/uHiMgrDCpE/uAZp8LVaYmIvMGgQuQPoVGu+1oGFSIibzCoEPkDZ/4QEbUKgwqRPzCoEBG1CoMKkT8wqBARtQqDCpE/eNZS4RgVIiJvMKgQ+QNbVIiIWoVBhcgf2KJCRNQqDCpE/sAWFSKiVmFQIfIH9zL6XEeFiMgrDCpE/nB+iwo3JiQiajEGFSJ/0J+/MaFF2lqIiGSEQYXIHzShgCbM9ZjjVIiIWoxBhchfPDN/yqWtg4hIRhhUiPzFE1TYokJE1FIMKkT+winKREReY1Ah8hcGFSIirzGoEPkL11IhIvIagwqRv7BFhYjIawwqRP7CwbRERF5jUCHyF0+LCrt+iIhaikGFyF+4gzIRkdcYVIj8hWNUiIi8xqBC5C/cmJCIyGsMKkT+4p6eLDiAukppayEikgkGFSJ/0YSc25iQa6kQEbUIgwqRP3HmDxGRVxhUiPyJa6kQEXmFQYXInxhUiIi8wqBC5E/s+iEi8gqDCpE/cS0VIiKvMKgQ+RODChGRVxhUiPwpNMp1z6BCRNQiDCpE/uRuUaktl7YOIiKZYFAh8id2/RAReYVBhcifGFSIiLzCoELkT551VM5yY0IiohZgUCHyJ25MSETkFQYVIn/ShADacNdjdv8QEV0RgwqRv53f/UNERJfFoELkb6Hc74eIqKUYVIj8zbOWCltUiIiuhEGFyN84RZmIqMUYVIj8jUGFiKjFRA8qKSkpUCgUF92mTp0KABAEAbNmzYLZbEZoaCgyMzORm5srdhlEgUvPMSpERC0lelDZunUriouLPbfVq1cDAMaNGwcAmDt3LubNm4cFCxZg69atMJlMGDFiBKxWq9ilEAUmzvohImox0YNKXFwcTCaT5/btt9+iS5cuGDJkCARBwPz58zFz5kyMGTMG6enpWLx4MWpqarB06VKxSyEKTJ6uHwYVIqIr8ekYFbvdjiVLlmDy5MlQKBTIy8tDSUkJsrKyPMfodDoMGTIEmzZtuuT72Gw2WCyWJjci2eIYFSKiFvNpUFm+fDkqKiowadIkAEBJSQkAICEhoclxCQkJnteaM3v2bBiNRs8tOTnZZzUT+RzXUSEiajGfBpUPP/wQI0eOhNlsbvK8QqFo8rUgCBc9d74ZM2agsrLScysqKvJJvUR+4VlHpRxwOqWthYgowKl99cYFBQVYs2YNvvzyS89zJpMJgKtlJTEx0fN8aWnpRa0s59PpdNDpdL4qlci/9OdtTGirBEKjpK2HiCiA+axFZdGiRYiPj8ftt9/ueS41NRUmk8kzEwhwjWPJyclBRkaGr0ohCixqHaCNcD3mgFoiosvySYuK0+nEokWLMHHiRKjV536EQqHAtGnTkJ2djbS0NKSlpSE7Oxt6vR4TJkzwRSlEgUkfBditrnEqMV2kroaIKGD5JKisWbMGhYWFmDx58kWvTZ8+HbW1tXjkkUdQXl6OQYMGYdWqVYiIiPBFKUSBSR8DVBSyRYWI6AoUgiAIUhfhLYvFAqPRiMrKShgMBqnLIfLekrHAkTXA6HeAfr+XuhoiIr9ozd9v7vVDJAWupUJE1CIMKkRS4FoqREQtwqBCJAXPWioco0JEdDkMKkRS4MaEREQtwqBCJAU9u36IiFqCQYVIChxMS0TUIgwqRFLwBBV2/RARXQ6DCpEUzh9My40JiYguiUGFSAru6cmCE6irkLQUIqJAxqBCJAW1lhsTEhG1AIMKkVTcM3+4lgoR0SUxqBBJhTN/iIiuyCe7JxNRC7hbVL64H1DynyK1QFgcMPFrILqz1JUQ+Q3/70gklU7Xu3ZQdja4bkRXUlkEHPkRuJZBhdoPBhUiqdz4BHD1BMBhl7oSkoP1c4BdSwDLCakrIfIrBhUiKUWYpK6A5CKum+u+8ri0dRD5GQfTEhHJgbGD655BhdoZBhUiIjkwuIMKu36ofWFQISKSA3eLiuUE4HRIWwuRHzGoEBHJQYQJUKgAwQFUnZK6GiK/YVAhIpIDpQowmF2POU6F2hEGFSIiufAMqC2Stg4iP2JQISKSC0OS654DaqkdYVAhIpILTlGmdohBhYhILhhUqB1iUCEikgvPFGUGFWo/GFSIiOSCLSrUDjGoEBHJhTuo1JQB9bXS1kLkJwwqRERyERIJaMJcjznzh9oJBhUiIrlQKLiWCrU7DCpERHJibFxLxcIWFWofGFSIiOSEA2qpnWFQISKSE2Oy655BhdoJBhUiIjnxLKPPoELtA4MKEZGcsOuH2hkGFSIiOfGsTnsCEARpayHyAwYVIiI5MZhd9/U1QG25tLUQ+QGDChGRnGhCgbA412OupULtAIMKEZHceAbUci0VCn4MKkREcsMBtdSOMKgQEcmNey0VC4MKBT8GFSIiuTFyLRVqPxhUiIjkhl0/1I4wqBARyY3BHVQ4mJaCH4MKEZHcuFtUrCcBR4O0tRD5GIMKEZHchCcASg0gOIGqEqmrIfIpBhUiIrlRKgFDousxx6lQkGNQISKSI/cUZQYVCnIMKkREcmTgFGVqH3wSVE6cOIH7778fMTEx0Ov1uPrqq7F9+3bP64IgYNasWTCbzQgNDUVmZiZyc3N9UQoRUXDiFGVqJ0QPKuXl5bj++uuh0Wjw/fffY9++fXjzzTcRGRnpOWbu3LmYN28eFixYgK1bt8JkMmHEiBGwWq1il0NEFJwYVKidUIv9hnPmzEFycjIWLVrkeS4lJcXzWBAEzJ8/HzNnzsSYMWMAAIsXL0ZCQgKWLl2KKVOmiF0SEVHwcQcVLqNPQU70FpWvv/4aAwYMwLhx4xAfH49+/frhgw8+8Lyel5eHkpISZGVleZ7T6XQYMmQINm3a1Ox72mw2WCyWJjcionaNLSrUTogeVI4dO4aFCxciLS0NK1euxEMPPYS//OUv+PjjjwEAJSWuOf8JCQlNvi8hIcHz2oVmz54No9HouSUnJ4tdNhGRvLiDSm05YK+WthYiHxI9qDidTlxzzTXIzs5Gv379MGXKFDz44INYuHBhk+MUCkWTrwVBuOg5txkzZqCystJzKyoqErtsIiJ5CTEC2gjXYy6lT0FM9KCSmJiIXr16NXmuZ8+eKCwsBACYTCYAuKj1pLS09KJWFjedTgeDwdDkRkTU7nm6f/jhjYKX6EHl+uuvx8GDB5s8d+jQIXTq1AkAkJqaCpPJhNWrV3tet9vtyMnJQUZGhtjlEBEFL2PjWioWtqhQ8BJ91s9///d/IyMjA9nZ2bjnnnvw66+/4v3338f7778PwNXlM23aNGRnZyMtLQ1paWnIzs6GXq/HhAkTxC6HiCh4cUAttQOiB5WBAwdi2bJlmDFjBv72t78hNTUV8+fPx+9//3vPMdOnT0dtbS0eeeQRlJeXY9CgQVi1ahUiIiLELoeIKHh5ggpbVCh4KQRBEKQuwlsWiwVGoxGVlZUcr0JE7deuz4DlDwGpQ4CJX0tdDdEVtebvN/f6ISKSK3b9UDvAoEJEJFfnD6aVX+M4UYswqBARyZV7B+WGOqCmTNpaiHyEQYWISK7UOiC8cf0prqVCQYpBhYhIztytKpz5Q0GKQYWISM44oJaCHIMKEZGcGRs3abUwqFBwYlAhIpIz98wftqhQkGJQISKSM3b9UJBjUCEikjMDl9Gn4MagQkQkZ+4WFWsx4KiXthYiH2BQISKSs7A4QKUFILjCClGQYVAhIpIzpRIwmF2POU6FghCDChGR3LmnKDOoUBBiUCEikjsDpyhT8GJQISKSO05RpiDGoEJEJHcMKhTEGFSIiOTOHVQsXEuFgg+DChGR3HlaVIqkrYPIBxhUiIjkzj2Ytq4SsFmlrYVIZAwqRERyF2IAdEbXYy6lT0GGQYWIKBhwQC0FKQYVIqJgYGzs/rEwqFBwYVAhIgoGbFGhIMWgQkQUDDxBhWNUKLgwqBARBQMDpyhTcGJQISIKBuz6oSDFoEJEFAw8g2lPAk6ntLUQiYhBhYgoGESYASgAhw2oOSN1NUSiYVAhIgoGai0QYXI9ZvcPBREGFSKiYOFeSp9BhYIIgwoRUbDggFoKQgwqRETBwh1ULFxLhYIHgwoRUbAwci0VCj4MKkREwYJdPxSEGFSIiIKFZzAtu34oeDCoEBEFC2Oy677qFNBgl7YWIpEwqBARBYuwWEClAyAA1pNSV0MkCgYVIqJgoVCcW0qf41QoSDCoEBEFEw6opSDDoEJEFEwMDCoUXBhUiIiCCVtUKMgwqBARBROuTktBhkGFiCiYcDAtBRkGFSKiYOJeS4VBhYIEgwoRUTBxr05rswB1ldLWQiQCBhUiomCiCwdCIl2PuZQ+BQEGFSKiYMPuHwoiDCpERMHGPaDWwqBC8segQkQUbLiWCgUR0YPKrFmzoFAomtxMJpPndUEQMGvWLJjNZoSGhiIzMxO5ublil0FE1H55ggrHqJD8+aRFpXfv3iguLvbc9uzZ43lt7ty5mDdvHhYsWICtW7fCZDJhxIgRsFqtviiFiKj94TL6FER8ElTUajVMJpPnFhcXB8DVmjJ//nzMnDkTY8aMQXp6OhYvXoyamhosXbrUF6UQEbU/nhaVImnrIBKBT4LK4cOHYTabkZqainvvvRfHjh0DAOTl5aGkpARZWVmeY3U6HYYMGYJNmzZd8v1sNhssFkuTGxERXYJnMO1JwOmUthaiNhI9qAwaNAgff/wxVq5ciQ8++AAlJSXIyMhAWVkZSkpKAAAJCQlNvichIcHzWnNmz54No9HouSUnJ4tdNhFR8IhIBBRKwFkPVJdKXQ1Rm4geVEaOHImxY8eiT58+GD58OFasWAEAWLx4secYhULR5HsEQbjoufPNmDEDlZWVnltREZsziYguSaVxhRWAA2pJ9nw+PTksLAx9+vTB4cOHPbN/Lmw9KS0tvaiV5Xw6nQ4Gg6HJjYiILsO9lD7HqZDM+Tyo2Gw27N+/H4mJiUhNTYXJZMLq1as9r9vtduTk5CAjI8PXpRARtR9cS4WChFrsN3zqqacwatQodOzYEaWlpXjllVdgsVgwceJEKBQKTJs2DdnZ2UhLS0NaWhqys7Oh1+sxYcIEsUshImq/3EHFwq4fkjfRg8rx48dx33334cyZM4iLi8N1112HzZs3o1OnTgCA6dOno7a2Fo888gjKy8sxaNAgrFq1ChEREWKXQkTUfnGKMgUJhSAIgtRFeMtiscBoNKKyspLjVYiImnNgBfD5BMDcD/jzeqmrIQLQur/f3OuHiCgYeQbTsuuH5I1BhYgoGBkb15uqLgUabNLWQtQGDCpERMFIHw2oQ12POaCWZIxBhYgoGCkU55bS5xRlkjEGFSKiYMW1VCgIMKgQEQUrgzuosOuH5ItBhYgoWHEtFQoCDCpERMGKq9NSEGBQISIKVhxMS0GAQYWIKFi511KpPA7IbxFyIgAMKkREwcu9Oq29CqirlLYWolZiUCEiClZaPRAa7XrM7h+SKQYVIqJgxrVUSOYYVIiIgpln5g+DCskTgwoRUTBjiwrJHIMKEVEwM3J1WpI3BhUiomBm4FoqJG8MKkREwez8tVSIZIhBhYgomLlXp7WeBJwOaWshagUGFSKiYBZuAhQqwNkAVJ2SuhoirzGoEBEFM5UaMJhdjzmglmSIQYWIKNh5BtQWSVsHUSswqBARBTuupUIyxqBCRBTsPKvTsuuH5IdBhYgo2LFFhWRMLXUBRETkY+6gcnwb8P2z0tZC8hEWC9z0lNRVMKgQEQW9mK6u+6oSYMtCaWsh+YhJY1AhIiI/iE0Dfvc+cOag1JWQnOhjpK4AAIMKEVH70He81BUQtQoH0xIREVHAYlAhIiKigMWgQkRERAGLQYWIiIgCFoMKERERBSwGFSIiIgpYDCpEREQUsBhUiIiIKGAxqBAREVHAYlAhIiKigMWgQkRERAGLQYWIiIgCFoMKERERBSxZ7p4sCAIAwGKxSFwJERERtZT777b773hLyDKoWK1WAEBycrLElRAREZG3rFYrjEZji45VCN7EmgDhdDpx8uRJREREQKFQiPreFosFycnJKCoqgsFgEPW9gxnPm/d4zlqH5611eN5ah+fNe5c7Z4IgwGq1wmw2Q6ls2egTWbaoKJVKdOjQwac/w2Aw8KJsBZ437/GctQ7PW+vwvLUOz5v3LnXOWtqS4sbBtERERBSwGFSIiIgoYDGoXECn0+HFF1+ETqeTuhRZ4XnzHs9Z6/C8tQ7PW+vwvHlP7HMmy8G0RERE1D6wRYWIiIgCFoMKERERBSwGFSIiIgpYDCpEREQUsBhUzvPOO+8gNTUVISEh6N+/P3766SepSwpos2bNgkKhaHIzmUxSlxVwNmzYgFGjRsFsNkOhUGD58uVNXhcEAbNmzYLZbEZoaCgyMzORm5srTbEB5ErnbdKkSRddf9ddd500xQaI2bNnY+DAgYiIiEB8fDzuuusuHDx4sMkxvN4u1pLzxuvtYgsXLsRVV13lWdht8ODB+P777z2vi3WtMag0+uKLLzBt2jTMnDkTO3fuxI033oiRI0eisLBQ6tICWu/evVFcXOy57dmzR+qSAk51dTX69u2LBQsWNPv63LlzMW/ePCxYsABbt26FyWTCiBEjPHtatVdXOm8AcOuttza5/r777js/Vhh4cnJyMHXqVGzevBmrV69GQ0MDsrKyUF1d7TmG19vFWnLeAF5vF+rQoQNee+01bNu2Ddu2bcPQoUMxevRoTxgR7VoTSBAEQbj22muFhx56qMlzPXr0EJ599lmJKgp8L774otC3b1+py5AVAMKyZcs8XzudTsFkMgmvvfaa57m6ujrBaDQK7777rgQVBqYLz5sgCMLEiROF0aNHS1KPXJSWlgoAhJycHEEQeL211IXnTRB4vbVUVFSU8K9//UvUa40tKgDsdju2b9+OrKysJs9nZWVh06ZNElUlD4cPH4bZbEZqairuvfdeHDt2TOqSZCUvLw8lJSVNrj2dTochQ4bw2muB9evXIz4+Ht26dcODDz6I0tJSqUsKKJWVlQCA6OhoALzeWurC8+bG6+3SHA4HPv/8c1RXV2Pw4MGiXmsMKgDOnDkDh8OBhISEJs8nJCSgpKREoqoC36BBg/Dxxx9j5cqV+OCDD1BSUoKMjAyUlZVJXZpsuK8vXnveGzlyJD799FOsXbsWb775JrZu3YqhQ4fCZrNJXVpAEAQBTzzxBG644Qakp6cD4PXWEs2dN4DX26Xs2bMH4eHh0Ol0eOihh7Bs2TL06tVL1GtNlrsn+4pCoWjytSAIFz1H54wcOdLzuE+fPhg8eDC6dOmCxYsX44knnpCwMvnhtee98ePHex6np6djwIAB6NSpE1asWIExY8ZIWFlgePTRR7F7925s3Ljxotd4vV3apc4br7fmde/eHbt27UJFRQX+85//YOLEicjJyfG8Lsa1xhYVALGxsVCpVBelvNLS0ovSIF1aWFgY+vTpg8OHD0tdimy4Z0nx2mu7xMREdOrUidcfgMceewxff/011q1bhw4dOnie5/V2eZc6b83h9eai1WrRtWtXDBgwALNnz0bfvn3xj3/8Q9RrjUEFrhPdv39/rF69usnzq1evRkZGhkRVyY/NZsP+/fuRmJgodSmykZqaCpPJ1OTas9vtyMnJ4bXnpbKyMhQVFbXr608QBDz66KP48ssvsXbtWqSmpjZ5nddb86503prD6615giDAZrOJe62JNNBX9j7//HNBo9EIH374obBv3z5h2rRpQlhYmJCfny91aQHrySefFNavXy8cO3ZM2Lx5s3DHHXcIERERPGcXsFqtws6dO4WdO3cKAIR58+YJO3fuFAoKCgRBEITXXntNMBqNwpdffins2bNHuO+++4TExETBYrFIXLm0LnferFar8OSTTwqbNm0S8vLyhHXr1gmDBw8WkpKS2vV5e/jhhwWj0SisX79eKC4u9txqamo8x/B6u9iVzhuvt+bNmDFD2LBhg5CXlyfs3r1beO655wSlUimsWrVKEATxrjUGlfO8/fbbQqdOnQStVitcc801Taam0cXGjx8vJCYmChqNRjCbzcKYMWOE3NxcqcsKOOvWrRMAXHSbOHGiIAiuKaMvvviiYDKZBJ1OJ9x0003Cnj17pC06AFzuvNXU1AhZWVlCXFycoNFohI4dOwoTJ04UCgsLpS5bUs2dLwDCokWLPMfwervYlc4br7fmTZ482fM3My4uThg2bJgnpAiCeNeaQhAEoZUtPEREREQ+xTEqREREFLAYVIiIiChgMagQERFRwGJQISIiooDFoEJEREQBi0GFiIiIAhaDChEREQUsBhUiIiIKWAwqREREFLAYVIiIiChgMagQERFRwGJQISIiooD1/3TDlXWjfNBfAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "usage_size = 130000\n",
    "out_fast = train(source_fast_model, target_fast_model, n_epoch = 30, batch_size = batch_size, usage_size = usage_size, lr = 0.001, neural = RNN)\n",
    "torch.save(out_fast[2].state_dict(), sys.path[0] + \"/models/\" + \"fast_trained_model.pt\") \n",
    "show (out_fast, usage_size = usage_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same thing with the glove model\n",
    "\n",
    "usage_size = 130000\n",
    "out_glove = train(source_glove_model, target_glove_model, n_epoch = 1, batch_size = batch_size, usage_size = usage_size, lr = 0.001, neural = RNN)\n",
    "torch.save(out_glove[2].state_dict(), sys.path[0] + \"/models/\" + \"glove_trained_model.pt\")\n",
    "show (out_glove, usage_size = usage_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same thing with the word2vec model\n",
    " \n",
    "usage_size = 130000\n",
    "out_word2vec = train(source_w2v_model_cbow, target_w2v_model_cbow, n_epoch = 1, batch_size = batch_size, usage_size = usage_size, lr = 0.001, neural = RNN)\n",
    "torch.save(out_word2vec[2].state_dict(), sys.path[0] + \"/models/\" + \"word2vec_trained_model.pt\")\n",
    "show (out_word2vec, usage_size = usage_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following is for contextualized embeddings\n",
    "raise (Exception(\"Stop cell\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this model we can now try to add contextual embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for contextual embedding we will use BERT\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "#we use bert and we will train it on the data\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will make a function to get the embeddings of the sentences\n",
    "def get_embeddings(sentences, src = True) :\n",
    "    sentences_in = [token_to_word(sent, src=src) for sent in sentences]\n",
    "    \n",
    "    \n",
    "    #concatenate the sentences\n",
    "    sentences_in = [\" \".join(sent).replace(\"<pad>\", \"\").replace(\"<eos>\", \"\") for sent in sentences_in]\n",
    "    \n",
    "\n",
    "    #we tokenize the sentences\n",
    "    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences_in]\n",
    "    #we get the ids of the tokens\n",
    "    indexed_tokens = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "    \n",
    "    #make sure the sentences are of the same length\n",
    "    max_len = 15\n",
    "    for i in range(len(indexed_tokens)) :\n",
    "        while len(indexed_tokens[i]) < max_len :\n",
    "            indexed_tokens[i].append(0)\n",
    "            \n",
    "    #same for tokenized texts\n",
    "    for i in range(len(tokenized_texts)) :\n",
    "        while len(tokenized_texts[i]) < max_len :\n",
    "            tokenized_texts[i].append(\"<pad>\")\n",
    "    \n",
    "    #we get the attention masks\n",
    "    segments_ids = [[1] * len(sentence) for sentence in tokenized_texts]\n",
    "    #we get the tensors\n",
    "    tokens_tensor = torch.tensor([[indexed_tokens]])\n",
    "    segments_tensors = torch.tensor([[segments_ids]])\n",
    "    \n",
    "    #we get the embeddings\n",
    "    with torch.no_grad():\n",
    "        #no idea why it doesn't work : too many values to unpack (expected 2)\n",
    "        encoded_layers, _ = bert_model(tokens_tensor, segments_tensors)\n",
    "        \n",
    "    #we get the last layer\n",
    "    token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "    \n",
    "    #we get the word embeddings\n",
    "    word_embeddings = []\n",
    "    for token in token_embeddings :\n",
    "        word_embeddings.append(token[-1])\n",
    "    \n",
    "    return word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can modify the model to use the contextual embeddings\n",
    "\n",
    "class BertRNN_encode(nn.Module):\n",
    "    def __init__(self, embedding_model_input, embedding_model_output, hidden_size=50):\n",
    "        super(BertRNN_encode, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        embedding = torch.tensor(embedding_model_input.vectors).to(device)\n",
    "        self.embedding_in = (embedding / torch.norm(embedding, dim=1, keepdim=True)).to(device)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.embedding_dim_in = self.embedding_in.shape[1]\n",
    "        self.lstm_in = nn.LSTM(self.embedding_dim_in, self.embedding_dim_in, bidirectional=True, num_layers=2, dropout=0.01)\n",
    "        self.hidden_in = nn.Linear(self.embedding_dim_in, self.hidden_size)\n",
    "        self.hidden_in2 = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "\n",
    "\n",
    "    def forward(self, input_sentence):\n",
    "        #words_embeddings is a gensim model\n",
    "        input_sentence = torch.where((input_sentence < self.embedding_in.shape[0]).to(device), input_sentence.to(device), torch.zeros_like(input_sentence).to(device))\n",
    "        one_hot = F.one_hot(input_sentence.type(torch.int64), self.embedding_in.shape[0]).to(device)\n",
    "        embeds = torch.matmul(one_hot.type(torch.float), self.embedding_in.type(torch.float))\n",
    "         \n",
    "        #we get the bert embeddings\n",
    "        bert_embed = get_embeddings(input_sentence, src=True)\n",
    "        print(\"bert embed : \", bert_embed)\n",
    "        \n",
    "        embeds = torch.cat((embeds, bert_embed), dim=1)\n",
    "        print(embeds.shape)\n",
    "        \n",
    "        #encoder\n",
    "        output_lstm_1, _ = self.lstm_in(embeds.view(input_sentence.shape[0], input_sentence.shape[1], self.embedding_dim_in))\n",
    "        \n",
    "        #sum the two directions\n",
    "        output_lstm_1 = output_lstm_1[:, :, :self.embedding_dim_in] + output_lstm_1[:, : ,self.embedding_dim_in:] + embeds\n",
    "        soft = F.log_softmax(output_lstm_1, dim=2)\n",
    "        \n",
    "        output_hidden_1 = self.hidden_in(soft.view(input_sentence.shape[0], input_sentence.shape[1], self.embedding_dim_in))\n",
    "        output_hidden_2 = self.hidden_in2(F.relu(output_hidden_1.view(input_sentence.shape[0], input_sentence.shape[1], self.hidden_size)))\n",
    "        \n",
    "        return F.log_softmax(output_hidden_2, dim=2)\n",
    "    \n",
    "class BertRNN(nn.Module) :\n",
    "    def __init__(self, embedding_model_input, embedding_model_output, hidden_size=50):\n",
    "        super(BertRNN, self).__init__()\n",
    "\n",
    "        self.encoder = BertRNN_encode(embedding_model_input, embedding_model_output, hidden_size)\n",
    "        self.decoder = RNN_decode(embedding_model_output, embedding_model_output, hidden_size)\n",
    "    \n",
    "    def forward(self, input_sentence) :\n",
    "        output_hidden_1 = self.encoder(input_sentence)\n",
    "        output_hidden_2 = self.decoder(output_hidden_1)\n",
    "        return output_hidden_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now use the train function to train the model\n",
    "\n",
    "usage_size = 130000\n",
    "out_bert = train(source_fast_model, target_fast_model, n_epoch = 1, batch_size = batch_size, usage_size = usage_size, lr = 0.001, neural = BertRNN)\n",
    "torch.save(out_bert[2].state_dict(), sys.path[0] + \"/models/\" + \"word2vec_trained_model.pt\")\n",
    "show (out_bert, usage_size = usage_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attention\n",
    "raise (Exception(\"Stop cell\"))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#incorporate the attention mechanism in the decoder\n",
    "import math\n",
    "class RNN_decode_attention(nn.Module):\n",
    "    def __init__(self, embedding_model_input, embedding_model_output, hidden_size=100):\n",
    "        super(RNN_decode_attention, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        embedding = torch.tensor(embedding_model_output.vectors).to(device)\n",
    "        self.embedding_out = (embedding / torch.norm(embedding, dim=1, keepdim=True)).to(device)\n",
    "\n",
    "        self.embedding_dim_out = self.embedding_out.shape[1]\n",
    "        self.lstm_out = nn.LSTM(self.hidden_size, self.hidden_size, bidirectional=True)\n",
    "        self.hidden_out = nn.Linear(self.hidden_size, self.embedding_dim_out)\n",
    "        self.hidden_out2 = nn.Linear(self.embedding_dim_out, self.embedding_dim_out)\n",
    "        \n",
    "        #attention\n",
    "        self.attn = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.attn2 = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.v = nn.Linear(self.hidden_size, self.hidden_size, bias = False)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, input_hidden):\n",
    "        #decoder\n",
    "        output_lstm_1, _ = self.lstm_out(input_hidden.view(input_hidden.shape[0], input_hidden.shape[1], self.hidden_size))\n",
    "        \n",
    "        #sum the two directions\n",
    "        output_lstm_1 = output_lstm_1[:, :, :self.hidden_size] + output_lstm_1[:, : ,self.hidden_size:] + input_hidden\n",
    "        \n",
    "        #attention\n",
    "        query = self.attn(output_lstm_1)\n",
    "        key = self.attn2(input_hidden)\n",
    "        value = self.v(input_hidden)\n",
    "        output_lstm_1, attention = self.attention(query, key, value, dropout=self.dropout)\n",
    "        \n",
    "        output_hidden_1 = self.hidden_out(output_lstm_1.view(input_hidden.shape[0], input_hidden.shape[1], self.hidden_size))\n",
    "        output_hidden_2 = self.hidden_out2(F.relu(output_hidden_1.view(input_hidden.shape[0], input_hidden.shape[1], self.embedding_dim_out)))\n",
    "        \n",
    "        #similarity\n",
    "        out = torch.abs(output_hidden_2 @ self.embedding_out.transpose(0,1))\n",
    "        return  F.log_softmax(out, dim=2) #size batch, sentence, size_voc\n",
    "    \n",
    "    def attention(self, query, key, value, dropout=None):\n",
    "        d_k = query.size(-1)\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        p_attn = F.softmax(scores, dim = -1)\n",
    "        if dropout is not None:\n",
    "            p_attn = dropout(p_attn)\n",
    "        return torch.matmul(p_attn, value), p_attn\n",
    "    \n",
    "    def get_embedding(self):\n",
    "        return self.embedding_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_attention(nn.Module) :\n",
    "    def __init__(self, embedding_model_input, embedding_model_output, hidden_size=100):\n",
    "        super(RNN_attention, self).__init__()\n",
    "\n",
    "        self.encoder = RNN_encode(embedding_model_input, embedding_model_output, hidden_size)\n",
    "        self.decoder = RNN_decode_attention(embedding_model_output, embedding_model_output, hidden_size)\n",
    "    \n",
    "    def forward(self, input_sentence) :\n",
    "        output_hidden_1 = self.encoder(input_sentence)\n",
    "        output_hidden_2 = self.decoder(output_hidden_1)\n",
    "        return output_hidden_2\n",
    "    \n",
    "    def get_embedding(self) :\n",
    "        return self.decoder.get_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready\n",
      "Epoch: 0/200............. Loss: 18101.279296875.............e : 0.21316683738460224 grad : 1.6037911336752586e-055\n",
      "Epoch: 1/200............. Loss: 16593.744140625.............me : 0.22611064738336437 grad : 0.00618642568588256865\n",
      "Epoch: 2/200............. Loss: 16460.751953125.............e : 0.23099506200310593 grad : 1.2924050679430366e-054\n",
      "Epoch: 3/200............. Loss: 16307.9560546875.............me : 0.22773816416230597 grad : 0.0049037574790418155\n",
      "Epoch: 4/200............. Loss: 16163.98046875.............time : 0.24189592110287736 grad : 0.0011069481261074543\n",
      "Epoch: 5/200............. Loss: 15994.4609375............. time : 0.22308035716858096 grad : 0.0003675624029710889\n",
      "Epoch: 6/200............. Loss: 16023.58203125.............time : 0.23445465056404188 grad : 0.00366178131662309173\n",
      "Epoch: 7/200............. Loss: 15899.767578125.............me : 0.2652188412330388 grad : 0.000633115123491734305\n",
      "Epoch: 8/200............. Loss: 15830.7880859375............. : 0.27182997738281506 grad : 0.005401853471994477374\n",
      "Epoch: 9/200............. Loss: 15765.888671875.............ime : 0.25439902954686394 grad : 0.0010060311760753393\n",
      "Epoch: 10/200............. Loss: 15758.1328125.............time : 0.23033776721770727 grad : 0.0108691630885005805\n",
      "Epoch: 11/200............. Loss: 15663.826171875.............me : 0.22954890161105238 grad : 0.0044427188113331795\n",
      "Epoch: 12/200............. Loss: 15637.724609375.............me : 0.26996843959136985 grad : 0.0012117042206227784\n",
      "Epoch: 13/200............. Loss: 16122.1572265625.............: 0.22740743111035613 grad : 0.000347988796420395494\n",
      "Epoch: 14/200............. Loss: 16099.1220703125.............e : 0.23551223123817364 grad : 0.0038805194199085236\n",
      "Epoch: 15/200............. Loss: 15520.318359375.............0.2607277263879957 grad : 0.0034682655241340458331269\n",
      "Epoch: 16/200............. Loss: 15576.9814453125.............e : 0.23164350781700643 grad : 0.0082897124812006953\n",
      "Epoch: 17/200............. Loss: 15446.8095703125............. : 0.2379960720163613 grad : 0.019853154197335243633\n",
      "Epoch: 18/200............. Loss: 15075.662109375.............e : 0.2330309910398104 grad : 0.001425807247869670456\n",
      "Epoch: 19/200............. Loss: 15221.044921875.............me : 0.22981000153814476 grad : 0.0011367397382855415\n",
      "Epoch: 20/200............. Loss: 15108.787109375.............e : 0.23643721949927074 grad : 0.00725322403013706298\n",
      "Epoch: 21/200............. Loss: 14898.7119140625.............e : 0.22614958803694665 grad : 0.0059241307899355897\n",
      "Epoch: 22/200............. Loss: 15000.9921875.............time : 0.22719926294120926 grad : 0.0178774483501911165\n",
      "Epoch: 23/200............. Loss: 14833.0439453125............. : 0.226304400431894 grad : 0.0009949951199814677297\n",
      "Epoch: 24/200............. Loss: 14806.779296875.............me : 0.24604072610327965 grad : 0.0030831489711999893\n",
      "Epoch: 25/200............. Loss: 14748.6064453125............. 0.25475866486675786 grad : 0.0035417270846664906215\n",
      "Epoch: 26/200............. Loss: 14713.9365234375.............e : 0.23243005502197808 grad : 0.0029712142422795296\n",
      "Epoch: 27/200............. Loss: 15034.3115234375.............e : 0.23718260143823452 grad : 0.0056134397163987167\n",
      "Epoch: 28/200............. Loss: 14647.384765625.............me : 0.2574739032982263 grad : 0.00127836223691701894\n",
      "Epoch: 29/200............. Loss: 14629.2646484375.............e : 0.2514523719844677 grad : 0.00248971511609852324\n",
      "Epoch: 30/200............. Loss: 15769.7060546875.............e : 0.24405749907068588 grad : 0.008999519050121307\n",
      "Epoch: 31/200............. Loss: 14842.0009765625.............e : 0.22882213712889454 grad : 0.0008407931891269982\n",
      "Epoch: 32/200............. Loss: 14696.8603515625............. : 0.23643896299684888 grad : 0.00211921986192464838\n",
      "Epoch: 33/200............. Loss: 14692.544921875.............me : 0.2271318943168907 grad : 0.00293082231655716939\n",
      "Epoch: 34/200............. Loss: 15199.24609375.............ime : 0.23070283856619098 grad : 0.0022495009470731025\n",
      "Epoch: 35/200............. Loss: 14608.7802734375............. : 0.2312648550831221 grad : 0.006271694786846638484\n",
      "Epoch: 36/200............. Loss: 14562.5732421875.............e : 0.23200890537969313 grad : 0.0086264917626976974\n",
      "Epoch: 37/200............. Loss: 14846.8427734375.............0.23601297319103376 grad : 0.00570757081732153909062\n",
      "Epoch: 38/200............. Loss: 14501.7685546875.............e : 0.23192487654919255 grad : 0.0055682179518043995\n",
      "Epoch: 39/200............. Loss: 14460.880859375.............me : 0.22996653629932004 grad : 9.428043267689645e-05\n",
      "Epoch: 40/200............. Loss: 14422.6669921875............. : 0.22787629302616663 grad : 0.00092519383179023863\n",
      "Epoch: 41/200............. Loss: 15525.37109375.............me : 0.23078979049018578 grad : 0.00645536324009299386\n",
      "Epoch: 42/200............. Loss: 14639.7353515625.............e : 0.2296859385947627 grad : 0.00735309487208724254\n",
      "Epoch: 43/200............. Loss: 14485.8935546875.............e : 0.23464355289818986 grad : 0.0004240493581164628\n",
      "Epoch: 44/200............. Loss: 14416.962890625.............me : 0.2275900080197244 grad : 0.00559975765645504395\n",
      "Epoch: 45/200............. Loss: 14376.5439453125.............e : 0.23780189162347815 grad : 0.0049987914972007275\n",
      "Epoch: 46/200............. Loss: 14346.3125.............d time : 0.24293356848917327 grad : 0.00103548960760235796\n",
      "Epoch: 47/200............. Loss: 14311.576171875.............me : 0.22741812320004914 grad : 0.00036886625457555056\n",
      "Epoch: 48/200............. Loss: 14328.9072265625............. : 0.2276067785715341 grad : 0.001914748689159751941\n",
      "Epoch: 49/200............. Loss: 14274.326171875.............e : 0.23747851767810968 grad : 0.00426790397614240653\n",
      "Epoch: 50/200............. Loss: 14453.0126953125............. : 0.2259143467402583 grad : 0.031826462596654897984\n",
      "Epoch: 51/200............. Loss: 14251.99609375.............ime : 0.23136699051011084 grad : 0.0060541774146258835\n",
      "Epoch: 52/200............. Loss: 14226.880859375.............e : 0.2539465998836544 grad : 0.005412722006440163354\n",
      "Epoch: 53/200............. Loss: 14219.3359375.............ime : 0.23322169899124637 grad : 0.00712187960743904163\n",
      "Epoch: 54/200............. Loss: 14216.529296875.............e : 0.2257024435143082 grad : 0.004628935828804977738\n",
      "Epoch: 55/200............. Loss: 14210.703125.............time : 0.22341558140997267 grad : 0.00853193644434213655\n",
      "Epoch: 56/200............. Loss: 14185.2998046875.............0.23680572892879134 grad : 0.00194525520782917741053\n",
      "Epoch: 57/200............. Loss: 14164.66015625.............me : 0.231845108493329 grad : 0.0046036955900490285496\n",
      "Epoch: 58/200............. Loss: 14151.9326171875............. : 0.22429171239447906 grad : 0.005655417684465647824\n",
      "Epoch: 59/200............. Loss: 14143.7275390625............. : 0.22177483773732698 grad : 0.00483717489987611876\n",
      "Epoch: 60/200............. Loss: 14336.0673828125.............: 0.24411043134968116 grad : 0.009121864102780819546\n",
      "Epoch: 61/200............. Loss: 14124.77734375.............e : 0.22187352407591462 grad : 0.0079945484176278111903\n",
      "Epoch: 62/200............. Loss: 14096.8720703125.............: 0.25150142824825306 grad : 0.001951566431671381083\n",
      "Epoch: 63/200............. Loss: 14093.2109375.............me : 0.21909565226282177 grad : 0.003253340488299727454\n",
      "Epoch: 64/200............. Loss: 14074.244140625............. : 0.22359885987865774 grad : 0.005846314132213592587\n",
      "Epoch: 65/200............. Loss: 14059.0478515625............. 0.24430214135646236 grad : 0.0007705925381742418766\n",
      "Epoch: 66/200............. Loss: 14121.5810546875............. : 0.22800480473159465 grad : 0.00411828467622399325\n",
      "Epoch: 67/200............. Loss: 14046.322265625............. : 0.22004219661128363 grad : 0.000349995330907404465\n",
      "Epoch: 68/200............. Loss: 14042.0439453125.............: 0.24754645146919393 grad : 0.0067179012112319472234\n",
      "Epoch: 69/200............. Loss: 14083.3095703125............. 0.2660759233851656 grad : 0.00373124121688306334864\n",
      "Epoch: 70/200............. Loss: 14013.296875.............time : 0.22878321321831296 grad : 0.00301466835662722654\n",
      "Epoch: 71/200............. Loss: 14021.3134765625............. : 0.2393269580522208 grad : 0.003244673134759068545\n",
      "Epoch: 72/200............. Loss: 14002.41015625.............me : 0.2985093505599785 grad : 0.00577212637290358542\n",
      "Epoch: 73/200............. Loss: 14009.533203125.............e : 0.2592448910103326 grad : 0.003730137832462787668\n",
      "Epoch: 74/200............. Loss: 13982.9296875............. : 0.22567638280612076 grad : 0.00325350370258092923823\n",
      "Epoch: 75/200............. Loss: 13988.326171875............. 0.23841077338197494 grad : 0.00186745938844978802446\n",
      "Epoch: 76/200............. Loss: 14000.306640625.............e : 0.22425969856907782 grad : 0.00176782568451017145\n",
      "Epoch: 77/200............. Loss: 13979.119140625............. : 0.2237709689559979 grad : 0.0101521499454975131984\n",
      "Epoch: 78/200............. Loss: 13991.89453125.............e : 0.22103185884233337 grad : 0.010466846637427807175\n",
      "Epoch: 79/200............. Loss: 13964.6279296875.............: 0.2859606062861346 grad : 0.0036845351569354534235\n",
      "Epoch: 80/200............. Loss: 13964.599609375.............e : 0.31207429220187594 grad : 0.00863635540008545346\n",
      "Epoch: 81/200............. Loss: 13949.0478515625.............: 0.22957966080739683 grad : 0.000801856978796422576\n",
      "Epoch: 82/200............. Loss: 13935.7802734375............. : 0.2516846932137593 grad : 0.006690777372568846794\n",
      "Epoch: 83/200............. Loss: 13943.60546875............. : 0.22041964834159344 grad : 0.0014067649608477953533\n",
      "Epoch: 84/200............. Loss: 13941.736328125............. : 0.2233730551947665 grad : 0.0005239623715169728157\n",
      "Epoch: 85/200............. Loss: 13942.7548828125............. : 0.22607634051769518 grad : 0.00388195109553635169\n",
      "Epoch: 86/200............. Loss: 13922.7646484375.............: 0.23091943001467863 grad : 0.002264633541926741626\n",
      "Epoch: 87/200............. Loss: 13921.822265625............. : 0.2526676773318652 grad : 0.0025901850312948227726\n",
      "Epoch: 88/200............. Loss: 13921.453125.............ime : 0.2678301889300069 grad : 0.0070860655978322034763\n",
      "Epoch: 89/200............. Loss: 13920.443359375............. : 0.23594033719368612 grad : 0.005341928452253342046\n",
      "Epoch: 90/200............. Loss: 13912.5078125.............ime : 0.2619511970914724 grad : 0.004859076347202063553\n",
      "Epoch: 91/200............. Loss: 13897.845703125............. : 0.27964228031850724 grad : 0.001415840932168066553\n",
      "Epoch: 92/200............. Loss: 13899.33203125.............e : 0.252098216250346 grad : 0.00232371711172163504717\n",
      "Epoch: 93/200............. Loss: 13891.380859375.............e : 0.25429832504402394 grad : 0.00312657770700752745\n",
      "Epoch: 94/200............. Loss: 13897.5927734375............. : 0.22872618778587894 grad : 0.010200393386185177447\n",
      "Epoch: 95/200............. Loss: 13885.65234375.............me : 0.24982460849869179 grad : 0.00566440727561712356\n",
      "Epoch: 96/200............. Loss: 13888.5712890625.............0.2234228786400496 grad : 0.00252003036439418846e-05\n",
      "Epoch: 97/200............. Loss: 13873.8623046875............. : 0.22278686834552178 grad : 0.00585118029266595814\n",
      "Epoch: 98/200............. Loss: 13866.0419921875............. : 0.22471297250850308 grad : 0.00332478946074843487\n",
      "Epoch: 99/200............. Loss: 13869.984375.............ime : 0.21956398630042945 grad : 0.006685846485197544637\n",
      "Epoch: 100/200............. Loss: 13891.52734375............. : 0.22179378486662704 grad : 0.002529637422412634664\n",
      "Epoch: 101/200............. Loss: 13914.671875.............ime : 0.22596875430106417 grad : 0.00341819063760340281\n",
      "Epoch: 102/200............. Loss: 13909.16015625............. : 0.2205617944976659 grad : 0.0059869969263672838233\n",
      "Epoch: 103/200............. Loss: 13865.8427734375.............: 0.22300788965549181 grad : 0.00079635391011834142\n",
      "Epoch: 104/200............. Loss: 13874.5029296875.............22856845504769405 grad : 0.01282087061554193597028\n",
      "Epoch: 105/200............. Loss: 13849.3212890625.............: 0.23945272849343838 grad : 0.00258864322677254684\n",
      "Epoch: 106/200............. Loss: 13849.5244140625............. 0.2777043598441077 grad : 0.0019984373357146987997\n",
      "Epoch: 107/200............. Loss: 13844.3154296875............. 0.22643860565132473 grad : 0.0044216425158083447228\n",
      "Epoch: 108/200............. Loss: 13832.2841796875.............: 0.22081573819265984 grad : 0.00013321303413249552\n",
      "Epoch: 109/200............. Loss: 14474.1044921875.............: 0.22598223292476166 grad : 0.01345081627368927545\n",
      "Epoch: 110/200............. Loss: 13831.77734375.............: 0.23094617474693388 grad : 0.0002498693356756121177\n",
      "Epoch: 111/200............. Loss: 13846.7275390625.............: 0.2216346103570726 grad : 0.001160320825874805565\n",
      "Epoch: 112/200............. Loss: 13810.380859375............. 0.23394121303978574 grad : 0.0048619634471833716445\n",
      "Epoch: 113/200............. Loss: 13824.8974609375.............: 0.26238441892762443 grad : 0.0048591461963951594\n",
      "Epoch: 114/200............. Loss: 13815.828125.............ime : 0.25774290687923807 grad : 0.000584022316616028563\n",
      "Epoch: 115/200............. Loss: 13806.0634765625............. 0.24053462060713748 grad : 0.001614520209841430244\n",
      "Epoch: 116/200............. Loss: 13796.9169921875..............2857660523982309 grad : 0.002838890999555587868966\n",
      "Epoch: 117/200............. Loss: 13785.0927734375.............: 0.2979250153314867 grad : 0.002293561119586229313\n",
      "Epoch: 118/200............. Loss: 13785.1796875.............me : 0.23470193917056625 grad : 0.00121290108654648076\n",
      "Epoch: 119/200............. Loss: 13807.576171875.............: 0.23553649460701054 grad : 0.000732789572793990424\n",
      "Epoch: 120/200............. Loss: 13779.412109375.............: 0.22868371601340143 grad : 0.003618686692789197538\n",
      "Epoch: 121/200............. Loss: 13788.87109375.............: 0.22667566728683913 grad : 0.003436628961935639436\n",
      "Epoch: 122/200............. Loss: 13796.4326171875.............0.23193957369185617 grad : 0.0082809105515480040534\n",
      "Epoch: 123/200............. Loss: 13771.9033203125.............: 0.2602444106678444 grad : 0.005850197747349739198\n",
      "Epoch: 124/200............. Loss: 13760.3271484375.............22401395814846356 grad : 0.000644234474748373728664\n",
      "Epoch: 125/200............. Loss: 13773.8349609375............. 0.22549174326311117 grad : 0.0033015669323503974826\n",
      "Epoch: 126/200............. Loss: 13769.474609375............. : 0.23018003059547984 grad : 0.00330421351827681067\n",
      "Epoch: 127/200............. Loss: 13787.455078125.............: 0.2715102379188571 grad : 0.0017145143356174237955\n",
      "Epoch: 128/200............. Loss: 13762.4150390625............. 0.2847891474329871 grad : 0.0060919579118490227455\n",
      "Epoch: 129/200............. Loss: 13766.6279296875.............0.30483391484896777 grad : 0.0008362939697690308752\n",
      "Epoch: 130/200............. Loss: 13749.298828125.............: 0.22666471245849704 grad : 0.003859357442706823303\n",
      "Epoch: 131/200............. Loss: 13753.7919921875.............: 0.26169879511609057 grad : 0.00175506935920566326\n",
      "Epoch: 132/200............. Loss: 13754.337890625............. : 0.25491579237914225 grad : 0.004673962481319904367\n",
      "Epoch: 133/200............. Loss: 13750.181640625............. : 0.28572694934754794 grad : 0.01212846767157316255\n",
      "Epoch: 134/200............. Loss: 13749.8740234375.............: 0.30771980332961785 grad : 0.0009663720848038793\n",
      "Epoch: 135/200............. Loss: 13752.953125.............ime : 0.2874750100243644 grad : 0.012493415735661983418\n",
      "Epoch: 136/200............. Loss: 13746.154296875.............: 0.34986601547961144 grad : 0.005754464771598577566\n",
      "Epoch: 137/200............. Loss: 13748.5185546875.............: 0.26187421260748317 grad : 0.00560609251260757453\n",
      "Epoch: 138/200............. Loss: 13727.8896484375..............23762031435823502 grad : 0.004807120189070702916794\n",
      "Epoch: 139/200............. Loss: 13745.615234375............. : 0.23700895798398994 grad : 0.01458312943577766453\n",
      "Epoch: 140/200............. Loss: 13715.3984375.............me : 0.2702674570709995 grad : 0.01626665517687797576\n",
      "Epoch: 141/200............. Loss: 13719.8720703125............. 0.25095914046350165 grad : 0.015834340825676918648\n",
      "Epoch: 142/200............. Loss: 13724.9560546875.............2504615943772837 grad : 0.0092815058305859572057446\n",
      "Epoch: 143/200............. Loss: 13724.0625.............time : 0.2332972845981509 grad : 0.0085520353168249134856\n",
      "Epoch: 144/200............. Loss: 13721.6396484375.............: 0.2884552933359398 grad : 0.012233129702508456169\n",
      "Epoch: 145/200............. Loss: 13714.6787109375............. 0.23053435924691285 grad : 0.012927275151014328743\n",
      "Epoch: 146/200............. Loss: 13716.068359375............. 0.2437398230872448 grad : 0.0135190179571509365374\n",
      "Epoch: 147/200............. Loss: 13717.9755859375.............: 0.23379366602293283 grad : 0.0231103263795375827\n",
      "Epoch: 148/200............. Loss: 13705.8759765625.............: 0.23024227421494425 grad : 0.0093650966882705693\n",
      "Epoch: 149/200............. Loss: 13712.7705078125.............: 0.25716979333906514 grad : 0.00921648368239402845\n",
      "Epoch: 150/200............. Loss: 13834.0576171875.............: 0.2602885869064392 grad : 0.005986017175018787132\n",
      "Epoch: 151/200............. Loss: 13711.4453125.............me : 0.22332743276618913 grad : 0.00338228605687618262\n",
      "Epoch: 152/200............. Loss: 13768.181640625.............0.22570457988443193 grad : 0.0073910243809223175797\n",
      "Epoch: 153/200............. Loss: 13722.9365234375.............: 0.2786107665631169 grad : 0.003062434261664748383\n",
      "Epoch: 154/200............. Loss: 13716.8935546875.............0.29687626745555284 grad : 0.0009419250418432057214\n",
      "Epoch: 155/200............. Loss: 13702.9677734375.............: 0.3183057118641048 grad : 0.006299026310443878055\n",
      "Epoch: 156/200............. Loss: 13729.4384765625.............: 0.24966943826233556 grad : 0.01301306858658790677\n",
      "Epoch: 157/200............. Loss: 13715.6396484375.............: 0.2480194270452502 grad : 0.00333725893869996074\n",
      "Epoch: 158/200............. Loss: 13716.265625.............ime : 0.334482496042185 grad : 0.0036182792391628027174\n",
      "Epoch: 159/200............. Loss: 13704.8515625.............me : 0.2593083240674719 grad : 0.010674382559955122483\n",
      "Epoch: 160/200............. Loss: 13711.16015625............. : 0.2991753295474749 grad : 0.0102219544351100923535\n",
      "Epoch: 161/200............. Loss: 13721.9287109375.............: 0.2689548999756493 grad : 0.00026149902259930978\n",
      "Epoch: 162/200............. Loss: 13710.783203125............. 0.30497872778734036 grad : 0.000279028317891061392\n",
      "Epoch: 163/200............. Loss: 13705.62109375.............e : 0.2874119349451751 grad : 0.008118668571114547678\n",
      "Epoch: 164/200............. Loss: 13721.7822265625.............: 0.23803807448297185 grad : 0.00588005781173706056\n",
      "Epoch: 165/200............. Loss: 13707.0654296875.............: 0.24670857058021448 grad : 0.0094483140856027693\n",
      "Epoch: 166/200............. Loss: 13716.0478515625............. 0.2460157570395112 grad : 0.0058342930860817439967\n",
      "Epoch: 167/200............. Loss: 13699.2060546875.............: 0.2580811126460251 grad : 0.002048846799880266856\n",
      "Epoch: 168/200............. Loss: 13686.1845703125.............: 0.2413571977669119 grad : 0.0027349062729626894195\n",
      "Epoch: 169/200............. Loss: 13719.0087890625.............: 0.3658408076112729 grad : 0.010868356563150883648\n",
      "Epoch: 170/200............. Loss: 13709.6748046875.............: 0.27731980773073167 grad : 0.0042850472964346417\n",
      "Epoch: 171/200............. Loss: 13687.40234375............. 0.22467835318467452 grad : 0.01128171943128109547637\n",
      "Epoch: 172/200............. Loss: 13687.3232421875.............: 0.23662996039771325 grad : 0.01475259102880954708\n",
      "Epoch: 173/200............. Loss: 13692.2734375.............e : 0.24958351757284328 grad : 0.010785410180687904941\n",
      "Epoch: 174/200............. Loss: 13696.4462890625.............: 0.23065302461386997 grad : 0.00810723099857568738\n",
      "Epoch: 175/200............. Loss: 13693.4169921875.............06316021884365 grad : 0.011410177685320377215287767\n",
      "Epoch: 176/200............. Loss: 13700.068359375............. : 0.2395430718118209 grad : 0.012759362347424031535\n",
      "Epoch: 177/200............. Loss: 13683.1240234375.............: 0.22606424554269106 grad : 0.01727223396301269556\n",
      "Epoch: 178/200............. Loss: 13686.3251953125.............: 0.220810234846265 grad : 0.0097897546365857125086\n",
      "Epoch: 179/200............. Loss: 13696.6650390625.............0.23647680677108415 grad : 0.010492497123777866731\n",
      "Epoch: 180/200............. Loss: 13712.76953125.............e : 0.22340068467825255 grad : 0.01418322138488292746\n",
      "Epoch: 181/200............. Loss: 13713.203125.............e : 0.26081124126826155 grad : 0.0160932578146457674487\n",
      "Epoch: 182/200............. Loss: 13720.1962890625..............22600343405245152 grad : 0.0203054845333099370349\n",
      "Epoch: 183/200............. Loss: 13707.3359375.............me : 0.22443842612352893 grad : 0.02545722201466560457\n",
      "Epoch: 184/200............. Loss: 13711.98046875............. : 0.264153443111698 grad : 0.02336863987147808219928\n",
      "Epoch: 185/200............. Loss: 13673.9296875.............e : 0.2352419964096776 grad : 0.0158507190644741069964\n",
      "Epoch: 186/200............. Loss: 13707.6162109375.............0.2744978760431158 grad : 0.01946214213967323341386\n",
      "Epoch: 187/200............. Loss: 13718.197265625............. : 0.24880824191710835 grad : 0.0196910519152879773\n",
      "Epoch: 188/200............. Loss: 13703.4384765625............. 0.3604898690408362 grad : 0.0185662172734737463604\n",
      "Epoch: 189/200............. Loss: 13686.85546875.............e : 0.251977564256776 grad : 0.0220869146287441254313\n",
      "Epoch: 190/200............. Loss: 13669.982421875............. : 0.27691627863416635 grad : 0.02020508982241153757\n",
      "Epoch: 191/200............. Loss: 13672.462890625.............: 0.29140754938192553 grad : 0.023627718910574913407\n",
      "Epoch: 192/200............. Loss: 13679.9775390625.............: 0.2509949429856384 grad : 0.016008749604225167334\n",
      "Epoch: 193/200............. Loss: 13691.0478515625............. 0.2932624196163111 grad : 0.0198373012244701441347\n",
      "Epoch: 194/200............. Loss: 13673.8134765625.............: 0.2525975392038461 grad : 0.028304360806941986945\n",
      "Epoch: 195/200............. Loss: 13668.859375.............e : 0.31448508884453086 grad : 0.0171318333595991137644\n",
      "Epoch: 196/200............. Loss: 13674.8203125.............me : 0.2467137433803317 grad : 0.0159531980752944959617\n",
      "Epoch: 197/200............. Loss: 13675.8115234375............. 0.22766105648147858 grad : 0.021079309284687042684\n",
      "Epoch: 198/200............. Loss: 13681.5390625.............e : 0.22790013045288313 grad : 0.016115849837660791167\n",
      "Epoch: 199/200............. Loss: 13665.8134765625............. 0.23188808435699665 grad : 0.012884684838354588165\n",
      "15\n",
      "Original sentence : \n",
      "there is nothing to worry about . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
      "6459 3444 4270 6542 7174 34 7 20 21 21 21 21 21 21 21 \n",
      "Translated sentence : \n",
      "<bos> il n ' pas pas pas pas . . . <eos> . <eos> <eos>\n",
      "Target sentence : \n",
      "<bos> il n ' y a pas de quoi s ' inquiéter . <eos> <pad> \n",
      "20 4981 6357 4 10697 25 6976 2700 8022 8927 4 5184 7 21 22 \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#train it\n",
    "\n",
    "usage_size = 130000\n",
    "out_attention = train(source_fast_model, target_fast_model, n_epoch = 200, batch_size = batch_size, usage_size = usage_size, lr = 0.001, neural = RNN_attention)\n",
    "torch.save(out_attention[2].state_dict(), sys.path[0] + \"/models/\" + \"word2vec_trained_model.pt\")\n",
    "show (out_attention, usage_size = usage_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tests on the validation set\n",
    "raise (Exception(\"Stop cell\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "Original sentence : \n",
      "i am looking forward to your letter . <eos> <pad> <pad> <pad> <pad> <pad> <pad> \n",
      "3204 227 3790 2644 6542 7232 3691 7 20 21 21 21 21 21 21 \n",
      "Translated sentence : \n",
      "<bos> je - ai ' . . . . <eos> <eos> <eos> <eos> <eos> <eos>\n",
      "Target sentence : \n",
      "<bos> j ' attends <unk> ta lettre . <eos> <pad> <pad> <pad> <pad> <pad> <pad> \n",
      "20 5363 4 892 23 9716 5592 7 21 22 22 22 22 22 22 \n"
     ]
    }
   ],
   "source": [
    "test(out_attention[2].to(device), usage_size = usage_size, valid=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usage_size = 162000\n",
    "batch_size = 256\n",
    "data = MTFraEng(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready\n",
      "Epoch: 0/200............. Loss: 19503.6953125.............time : 0.2250648839345461 grad : 2.4948989448603243e-056\n",
      "Epoch: 1/200............. Loss: 18882.365234375.............me : 0.2214906264658393 grad : 0.002983324695378542427\n",
      "Epoch: 2/200............. Loss: 17751.220703125.............me : 0.2237808671799615 grad : 0.000557243707589805157\n",
      "Epoch: 3/200............. Loss: 17877.2578125............. time : 0.22289336809004906 grad : 0.0030283890664577484\n",
      "Epoch: 4/200............. Loss: 17613.51171875.............e : 0.22848686148942798 grad : 0.0006337444065138698337\n",
      "Epoch: 5/200............. Loss: 17435.224609375.............me : 0.22364969539119658 grad : 0.0064531848765909676\n",
      "Epoch: 6/200............. Loss: 17473.341796875.............ime : 0.22468250726800024 grad : 6.191577995195985e-05\n",
      "Epoch: 7/200............. Loss: 17386.24609375.............time : 0.22549617326561305 grad : 0.0006480496376752853\n",
      "Epoch: 8/200............. Loss: 17405.7890625.............: 0.22491689322983865 grad : 0.003082543844357133659577\n",
      "Epoch: 9/200............. Loss: 17363.11328125.............time : 0.2264647844655775 grad : 0.0011388581478968263\n",
      "Epoch: 10/200............. Loss: 17329.451171875.............me : 0.2340304193091518 grad : 0.0105567267164587976\n",
      "Epoch: 11/200............. Loss: 17270.255859375.............me : 0.23155667490439757 grad : 0.005808305460959673\n",
      "Epoch: 12/200............. Loss: 18661.064453125............. : 0.23739666519394434 grad : 0.000562411267310380935\n",
      "Epoch: 13/200............. Loss: 17455.1015625.............me : 0.22630641705126534 grad : 0.00428528338670730682\n",
      "Epoch: 14/200............. Loss: 17232.662109375.............me : 0.22764784967304136 grad : 0.0005928974132984877\n",
      "Epoch: 15/200............. Loss: 17172.51171875.............e : 0.23089862753040732 grad : 0.00523329898715019273\n",
      "Epoch: 16/200............. Loss: 17135.5703125.............time : 0.23464330766649022 grad : 0.004647369030863047\n",
      "Epoch: 17/200............. Loss: 17066.568359375.............me : 0.22793197434005003 grad : 0.004801720846444368\n",
      "Epoch: 18/200............. Loss: 17000.978515625.............me : 0.23278889362535604 grad : 0.0027490670327097178\n",
      "Epoch: 19/200............. Loss: 17454.33203125.............ime : 0.227713272090964 grad : 0.00302234455011785035\n",
      "Epoch: 20/200............. Loss: 17322.5625.............ed time : 0.22670505825515935 grad : 0.0004044606175739318\n",
      "Epoch: 21/200............. Loss: 17365.74609375.............ime : 0.23364756239410853 grad : 0.0009231597650796175\n",
      "Epoch: 22/200............. Loss: 17211.08203125.............ime : 0.23195920436514025 grad : 0.0015391727210953832\n",
      "Epoch: 23/200............. Loss: 17133.05859375.............me : 0.23920044753772432 grad : 0.00038983410922810435\n",
      "Epoch: 24/200............. Loss: 17188.849609375............. : 0.22682674208307171 grad : 0.00158858625218272237\n",
      "Epoch: 25/200............. Loss: 17105.42578125.............: 0.22971122234714547 grad : 0.0009144266950897872e-05\n",
      "Epoch: 26/200............. Loss: 17871.47265625.............e : 0.23285821193704778 grad : 0.000692656089086085605\n",
      "Epoch: 27/200............. Loss: 18052.21875.............time : 0.2358092160125803 grad : 0.001542223268188536283\n",
      "Epoch: 28/200............. Loss: 16805.841796875.............me : 0.23299429571341143 grad : 0.0003938041627407074\n",
      "Epoch: 29/200............. Loss: 16744.8984375.............ime : 0.22948141477275427 grad : 0.00067079364089295277\n",
      "Epoch: 30/200............. Loss: 16736.275390625.............e : 0.23859013679662772 grad : 0.00245967251248657763\n",
      "Epoch: 31/200............. Loss: 16694.173828125.............e : 0.22908170347784518 grad : 0.00148019543848931876\n",
      "Epoch: 32/200............. Loss: 16649.97265625.............e : 0.22785607192122567 grad : 0.001261511933989822943\n",
      "Epoch: 33/200............. Loss: 16630.916015625............. : 0.23162200638866992 grad : 0.001148112700320780365\n",
      "Epoch: 34/200............. Loss: 16597.349609375.............: 0.22625679396937445 grad : 0.0022978901397436857535\n",
      "Epoch: 35/200............. Loss: 16563.205078125............. : 0.22850061110407752 grad : 0.00084533385233953605\n",
      "Epoch: 36/200............. Loss: 16527.345703125.............e : 0.23685105561645314 grad : 0.00221669650636613374\n",
      "Epoch: 37/200............. Loss: 16509.65234375............. : 0.23135621907794227 grad : 0.0003202529915142804435\n",
      "Epoch: 38/200............. Loss: 16434.021484375............. : 0.22785138472065872 grad : 4.775523848365992e-0566\n",
      "Epoch: 39/200............. Loss: 16393.537109375.............me : 0.23106023596667588 grad : 0.0044004525989294053\n",
      "Epoch: 40/200............. Loss: 16343.439453125.............me : 0.2339264273389082 grad : 0.00329700834117829846\n",
      "Epoch: 41/200............. Loss: 16316.869140625.............0.2283790871603529 grad : 0.0027121608145534992084934\n",
      "Epoch: 42/200............. Loss: 16285.728515625.............me : 0.23285297683989278 grad : 0.0009238860802724957\n",
      "Epoch: 43/200............. Loss: 16420.98046875............. : 0.23269532309243282 grad : 0.0001096594787668436815\n",
      "Epoch: 44/200............. Loss: 16233.5078125.............ime : 0.2317051209418691 grad : 0.000559654610697180852\n",
      "Epoch: 45/200............. Loss: 16204.8525390625.............e : 0.2272790522288633 grad : 0.002084045205265283656\n",
      "Epoch: 46/200............. Loss: 16209.1953125.............time : 0.2259165880550311 grad : 0.00190400960855185995\n",
      "Epoch: 47/200............. Loss: 16168.0302734375.............e : 0.22630873486778874 grad : 0.0022977434564381847\n",
      "Epoch: 48/200............. Loss: 16145.9052734375.............: 0.22821608469792298 grad : 0.001382374553941190285\n",
      "Epoch: 49/200............. Loss: 16124.9541015625.............e : 0.2294948993710845 grad : 0.00211672973819077215\n",
      "Epoch: 50/200............. Loss: 16108.4970703125.............e : 0.22729873296893566 grad : 0.00103048840537667276\n",
      "Epoch: 51/200............. Loss: 16085.0458984375............. : 0.22909691279302372 grad : 0.00089081382611766468\n",
      "Epoch: 52/200............. Loss: 16056.951171875.............: 0.22935302610236005 grad : 0.0012477918062359095577\n",
      "Epoch: 53/200............. Loss: 15988.45703125.............ime : 0.22618610619013105 grad : 0.0016901324270293117\n",
      "Epoch: 54/200............. Loss: 15960.0673828125.............e : 0.23055995532967186 grad : 9.464887989452109e-05\n",
      "Epoch: 55/200............. Loss: 16052.03515625.............me : 0.2318101771994567 grad : 0.002003116533160209798\n",
      "Epoch: 56/200............. Loss: 15882.30078125.............ime : 0.226484848686381 grad : 0.001138760126195848554\n",
      "Epoch: 57/200............. Loss: 15823.5048828125.............e : 0.23072666857284274 grad : 0.0004886531969532371\n",
      "Epoch: 58/200............. Loss: 15833.923828125.............me : 0.24106811027208394 grad : 0.0194210447371006267\n",
      "Epoch: 59/200............. Loss: 15792.265625............. time : 0.22965316381486908 grad : 0.0018301871605217457\n",
      "Epoch: 60/200............. Loss: 15764.4541015625.............: 0.23216948200745907 grad : 0.004005096387118101053\n",
      "Epoch: 61/200............. Loss: 15787.87890625.............me : 0.22695407047349128 grad : 0.00363383861258626782\n",
      "Epoch: 62/200............. Loss: 15778.748046875.............me : 0.22965750952882558 grad : 0.0016321028815582395\n",
      "Epoch: 63/200............. Loss: 15700.779296875.............me : 0.23041468226557332 grad : 0.00018900298164226115\n",
      "Epoch: 64/200............. Loss: 15732.04296875.............me : 0.2338037975433885 grad : 0.002078542718663811722\n",
      "Epoch: 65/200............. Loss: 15728.1572265625.............e : 0.23711779685175757 grad : 0.0030558796133846045\n",
      "Epoch: 66/200............. Loss: 15681.7353515625............. : 0.22889487839867803 grad : 0.00039465902955271304\n",
      "Epoch: 67/200............. Loss: 15682.5439453125.............e : 0.23682435541443744 grad : 0.0024497844278812415\n",
      "Epoch: 68/200............. Loss: 15643.9873046875.............e : 0.2284541454182875 grad : 0.00076061056461185223\n",
      "Epoch: 69/200............. Loss: 15623.5341796875............. : 0.22644999101124122 grad : 0.001692842459306120924\n",
      "Epoch: 70/200............. Loss: 15607.951171875............. : 0.23423194508301132 grad : 0.0006754613714292645284\n",
      "Epoch: 71/200............. Loss: 15630.96875.............time : 0.2340299007354373 grad : 0.0010044925147667527598\n",
      "Epoch: 72/200............. Loss: 15579.8583984375.............e : 0.22609610400351896 grad : 0.00311956694349646574\n",
      "Epoch: 73/200............. Loss: 15615.2568359375.............: 0.22950945665173392 grad : 0.002860267413780093642\n",
      "Epoch: 74/200............. Loss: 15559.4228515625.............0.22871575501802058 grad : 0.007588967680931091809576\n",
      "Epoch: 75/200............. Loss: 15556.78125.............ime : 0.2340967648520448 grad : 0.00186940154526382682764\n",
      "Epoch: 76/200............. Loss: 15530.5751953125............. : 0.23677712829079606 grad : 0.00173027336131781345\n",
      "Epoch: 77/200............. Loss: 15563.9453125.............time : 0.22856047291064738 grad : 0.0006161996861919761\n",
      "Epoch: 78/200............. Loss: 15530.28125............. time : 0.23328754675245936 grad : 0.00503667723387479886\n",
      "Epoch: 79/200............. Loss: 15535.7890625.............time : 0.24668295751738786 grad : 0.0044294083490967754\n",
      "Epoch: 80/200............. Loss: 15516.1103515625............. : 0.22784079504835103 grad : 0.003185277571901679461\n",
      "Epoch: 81/200............. Loss: 15496.9951171875.............: 0.23133001673571496 grad : 0.007480123545974493685\n",
      "Epoch: 82/200............. Loss: 15492.197265625.............e : 0.22867423367241801 grad : 0.00535722170025110243\n",
      "Epoch: 83/200............. Loss: 15489.8037109375............. : 0.23331937859030727 grad : 0.00856980588287115125\n",
      "Epoch: 84/200............. Loss: 15487.1005859375..............22928661008022316 grad : 0.006849681027233601276176\n",
      "Epoch: 85/200............. Loss: 15461.08984375.............ime : 0.22881201125276468 grad : 0.0016149892471730711\n",
      "Epoch: 86/200............. Loss: 15514.6005859375............. : 0.22579968776661638 grad : 0.00756506342440843614\n",
      "Epoch: 87/200............. Loss: 15483.4111328125............. : 0.22707892116347334 grad : 0.00369345163926482222\n",
      "Epoch: 88/200............. Loss: 15462.7001953125..............2291692427561361 grad : 0.0063131870701909065698101\n",
      "Epoch: 89/200............. Loss: 15458.142578125.............me : 0.23141150791525675 grad : 0.0083282608538866046\n",
      "Epoch: 90/200............. Loss: 15442.9052734375.............0.22791297670148694 grad : 0.00457077007740736278531\n",
      "Epoch: 91/200............. Loss: 15440.0068359375.............e : 0.22675990358607806 grad : 0.0033299659844487906\n",
      "Epoch: 92/200............. Loss: 15442.099609375............. : 0.22524428110222786 grad : 0.004107384942471981382\n",
      "Epoch: 93/200............. Loss: 15444.1591796875.............e : 0.23093431867265035 grad : 0.0032997049856930977\n",
      "Epoch: 94/200............. Loss: 15429.4951171875.............e : 0.23297061052200238 grad : 0.0068389293737709526\n",
      "Epoch: 95/200............. Loss: 15480.958984375.............me : 0.22995035023473756 grad : 0.0053486023098230364\n",
      "Epoch: 96/200............. Loss: 15422.0810546875............. : 0.2304511424421201 grad : 0.006019089370965958767\n",
      "Epoch: 97/200............. Loss: 15457.533203125.............me : 0.23148473270292913 grad : 0.0026858472265303135\n",
      "Epoch: 98/200............. Loss: 15403.3203125.............time : 0.23471877112585732 grad : 0.0047644553706049923\n",
      "Epoch: 99/200............. Loss: 15406.400390625.............me : 0.24149343746762947 grad : 0.0077362842857837685\n",
      "Epoch: 100/200............. Loss: 15491.6708984375.............0.23046782799600465 grad : 0.0123258708044886595434\n",
      "Epoch: 101/200............. Loss: 15413.916015625.............22818845997914367 grad : 0.0019906081724911936962366\n",
      "Epoch: 102/200............. Loss: 15436.57421875.............me : 0.22612456747136403 grad : 0.0032580120023339987\n",
      "Epoch: 103/200............. Loss: 15394.8935546875............. : 0.23226802351562573 grad : 0.0043553723953664346\n",
      "Epoch: 104/200............. Loss: 15448.3154296875............. : 0.22963750994088275 grad : 0.0030397032387554646\n",
      "Epoch: 105/200............. Loss: 15395.578125.............ime : 0.22579122527141834 grad : 0.004506041761487722801\n",
      "Epoch: 106/200............. Loss: 20540.341796875............. : 0.23050299631573912 grad : 0.01073277741670608552\n",
      "Epoch: 107/200............. Loss: 15368.6884765625............. : 0.22726495452650433 grad : 0.0003150725387968123\n",
      "Epoch: 108/200............. Loss: 15380.3515625.............e : 0.23397410696324497 grad : 0.000339779275236651363\n",
      "Epoch: 109/200............. Loss: 15386.0419921875.............0.23422752583721534 grad : 0.0067894184030592442775\n",
      "Epoch: 110/200............. Loss: 15366.791015625............. : 0.23275547847815928 grad : 0.00391809782013297187\n",
      "Epoch: 111/200............. Loss: 15368.990234375............. : 0.23640610059395734 grad : 0.00235093804076313977\n",
      "Epoch: 112/200............. Loss: 15359.9423828125.............: 0.22555223716073797 grad : 0.00222496758215129384\n",
      "Epoch: 113/200............. Loss: 15366.4169921875.............: 0.22445919284040755 grad : 0.00098238792270421985\n",
      "Epoch: 114/200............. Loss: 15379.9853515625.............: 0.22640089655694845 grad : 0.00015435104432981467\n",
      "Epoch: 115/200............. Loss: 15350.1455078125............. 0.22733918231857925 grad : 0.002772739389911294338\n",
      "Epoch: 116/200............. Loss: 15337.7890625.............e : 0.22678095934840814 grad : 0.0048344419337809094113\n",
      "Epoch: 117/200............. Loss: 15325.8173828125............. : 0.22654251537862127 grad : 0.0015098612057045102\n",
      "Epoch: 118/200............. Loss: 15323.384765625............. 0.23279909243848493 grad : 0.0059657222591340544438\n",
      "Epoch: 119/200............. Loss: 15306.98046875.............me : 0.22851324137690796 grad : 0.00150125275831669577\n",
      "Epoch: 120/200............. Loss: 15307.04296875.............e : 0.23297410709438907 grad : 0.00428828597068786647\n",
      "Epoch: 121/200............. Loss: 15307.53125............. time : 0.22802955747998507 grad : 0.0038167934399098168\n",
      "Epoch: 122/200............. Loss: 15282.3212890625............. 0.23271636561327821 grad : 0.004494899418205023754\n",
      "Epoch: 123/200............. Loss: 15309.658203125............. : 0.22475861051802964 grad : 0.00330969085916876856\n",
      "Epoch: 124/200............. Loss: 15283.376953125.............e : 0.23519003674275057 grad : 0.0016134467441588645\n",
      "Epoch: 125/200............. Loss: 15289.265625.............time : 0.22644863116840958 grad : 0.0025356134865432978\n",
      "Epoch: 126/200............. Loss: 15286.4990234375.............0.23572346564562288 grad : 0.0018169858958572156897\n",
      "Epoch: 127/200............. Loss: 15277.9560546875.............: 0.22976109269541317 grad : 0.00328971934504807996\n",
      "Epoch: 128/200............. Loss: 15285.578125.............time : 0.22620245541792752 grad : 0.0024589484091848135\n",
      "Epoch: 129/200............. Loss: 15280.5859375.............me : 0.22974528521625448 grad : 0.00025417708093300465\n",
      "Epoch: 130/200............. Loss: 15271.548828125.............e : 0.23441267132703253 grad : 0.00221019983291625988\n",
      "Epoch: 131/200............. Loss: 15302.5751953125............. 0.2256881855346946 grad : 0.00811139401048421916144\n",
      "Epoch: 132/200............. Loss: 15294.3115234375............. 0.23433556120638432 grad : 0.003850778099149465626\n",
      "Epoch: 133/200............. Loss: 15258.59375............. time : 0.22465825056483424 grad : 0.0004083641979377717\n",
      "Epoch: 134/200............. Loss: 15276.6171875.............me : 0.23283118764321903 grad : 0.00214496534317731868\n",
      "Epoch: 135/200............. Loss: 15241.8759765625............. : 0.22884012739395876 grad : 0.0075074811466038236\n",
      "Epoch: 136/200............. Loss: 15244.640625.............time : 0.2246548942153703 grad : 0.00185904931277036677\n",
      "Epoch: 137/200............. Loss: 15302.59375............. time : 0.23110085722402196 grad : 0.0077236709184944635\n",
      "Epoch: 138/200............. Loss: 15241.6240234375............. : 0.22463724726339884 grad : 0.0018445391906425357\n",
      "Epoch: 139/200............. Loss: 15246.767578125.............: 0.22802706770768053 grad : 0.001861895550973713477\n",
      "Epoch: 140/200............. Loss: 15242.3857421875..............224909359053914 grad : 0.0055639557540416725913256\n",
      "Epoch: 141/200............. Loss: 15240.2607421875.............0.23609017673510801 grad : 0.0093983681872487077077\n",
      "Epoch: 142/200............. Loss: 15223.58203125.............e : 0.2285099130018188 grad : 0.003273066598922014243\n",
      "Epoch: 143/200............. Loss: 15237.5732421875............. 0.22709209936438346 grad : 0.002069087931886315361\n",
      "Epoch: 144/200............. Loss: 15233.345703125.............e : 0.22683596289312097 grad : 0.0010844705393537885\n",
      "Epoch: 145/200............. Loss: 15221.9736328125.............: 0.22611151037106475 grad : 0.00228066998533904555\n",
      "Epoch: 146/200............. Loss: 15214.0693359375.............: 0.23066213792913204 grad : 0.00306625291705131535\n",
      "Epoch: 147/200............. Loss: 15224.9560546875.............: 0.24306281385512196 grad : 0.00396522600203752567\n",
      "Epoch: 148/200............. Loss: 15205.0927734375............. : 0.2400292674590555 grad : 0.00879175215959549495\n",
      "Epoch: 149/200............. Loss: 15202.6103515625............. : 0.24203604162306325 grad : 0.0007745776674710214\n",
      "Epoch: 150/200............. Loss: 15235.361328125.............: 0.23589862543001827 grad : 0.009383625350892544846\n",
      "Epoch: 151/200............. Loss: 15242.484375.............time : 0.23425019922322843 grad : 0.007482794113457203\n",
      "Epoch: 152/200............. Loss: 15234.6162109375.............0.2365969539364565 grad : 0.00126401684246957343715\n",
      "Epoch: 153/200............. Loss: 16904.650390625.............: 0.2542636005992373 grad : 0.0059870262630283837683\n",
      "Epoch: 154/200............. Loss: 15275.830078125.............e : 0.24016616241229186 grad : 0.0019353339448571205\n",
      "Epoch: 155/200............. Loss: 15210.8095703125.............29458964065546 grad : 0.000805112067610025408253484\n",
      "Epoch: 156/200............. Loss: 15207.4453125.............e : 0.23055699019990633 grad : 0.002687657950446009624\n",
      "Epoch: 157/200............. Loss: 15225.283203125.............e : 0.22982986924351892 grad : 0.0056863329373300076\n",
      "Epoch: 158/200............. Loss: 15211.5712890625............. : 0.23055893739568176 grad : 0.00302634271793067465\n",
      "Epoch: 159/200............. Loss: 15183.8369140625............. 0.2326299038322586 grad : 0.0129228159785270692394\n",
      "Epoch: 160/200............. Loss: 15213.8369140625.............: 0.22647438245229617 grad : 0.00714972661808133162\n",
      "Epoch: 161/200............. Loss: 15188.1083984375.............0.22456485387292818 grad : 0.00061950529925525193893\n",
      "Epoch: 162/200............. Loss: 15216.3095703125............. : 0.23245478367029324 grad : 0.0013781188754364848\n",
      "Epoch: 163/200............. Loss: 15179.1328125.............ime : 0.2238264684198331 grad : 0.005946842022240162825\n",
      "Epoch: 164/200............. Loss: 15180.8427734375.............: 0.2252379597924807 grad : 0.0049701710231602190957\n",
      "Epoch: 165/200............. Loss: 15178.220703125.............e : 0.23339805102050448 grad : 0.0032335284631699324\n",
      "Epoch: 166/200............. Loss: 15193.326171875.............e : 0.23107282844179033 grad : 9.835815035330597e-061\n",
      "Epoch: 167/200............. Loss: 15257.0830078125.............: 0.25267352504679846 grad : 0.00072107772575691346\n",
      "Epoch: 168/200............. Loss: 15186.4384765625............. : 0.22387340869443753 grad : 0.0062999236397445267\n",
      "Epoch: 169/200............. Loss: 15219.2841796875.............: 0.22152789294738304 grad : 0.00152850756421685222\n",
      "Epoch: 170/200............. Loss: 15199.0908203125.............: 0.22447281755217935 grad : 0.00043841579463332896\n",
      "Epoch: 171/200............. Loss: 15200.53125.............time : 0.22641654929406888 grad : 0.00014075316721573472\n",
      "Epoch: 172/200............. Loss: 15192.033203125............. : 0.21719542610436027 grad : 0.00328602781519293832\n",
      "Epoch: 173/200............. Loss: 15188.67578125.............e : 0.3971608883461065 grad : 0.008589694276452065533\n",
      "Epoch: 174/200............. Loss: 15196.3251953125.............: 0.33527621591131584 grad : 0.00249068834818899637\n",
      "Epoch: 175/200............. Loss: 15188.5458984375.............0.2977565896775805 grad : 0.00317476410418748860393\n",
      "Epoch: 176/200............. Loss: 15192.12109375.............me : 0.25427123551032205 grad : 0.0011057127267122269\n",
      "Epoch: 177/200............. Loss: 15210.6298828125............. : 0.24342979198412637 grad : 0.0043216245248913765\n",
      "Epoch: 178/200............. Loss: 15215.8564453125............. : 0.23410332218735894 grad : 0.0043178997002542025\n",
      "Epoch: 179/200............. Loss: 15170.7470703125............. : 0.2242908988774412 grad : 0.00616975082084536555\n",
      "Epoch: 180/200............. Loss: 15167.251953125............. : 0.2237232780059818 grad : 0.009023667313158512753\n",
      "Epoch: 181/200............. Loss: 15167.0654296875............. : 0.22353202783929357 grad : 0.00955355446785688454\n",
      "Epoch: 182/200............. Loss: 15182.64453125............. : 0.2260317955554765 grad : 0.0035982611589133745371\n",
      "Epoch: 183/200............. Loss: 15155.177734375.............e : 0.2350659561634541 grad : 0.011098088696599007455\n",
      "Epoch: 184/200............. Loss: 15170.2158203125............. : 0.23181759275992816 grad : 0.0025835661217570305\n",
      "Epoch: 185/200............. Loss: 15155.8720703125............. 0.24265011412814166 grad : 0.005399323068559176292\n",
      "Epoch: 186/200............. Loss: 15173.447265625............. 0.2451732842198503 grad : 0.01038383040577173226724\n",
      "Epoch: 187/200............. Loss: 15166.029296875.............e : 0.23012661468951104 grad : 0.0121058244258165361\n",
      "Epoch: 188/200............. Loss: 15159.6396484375............. : 0.22025978152810194 grad : 0.0098587172105908424\n",
      "Epoch: 189/200............. Loss: 15157.060546875.............: 0.22436967400856545 grad : 0.013307736255228529654\n",
      "Epoch: 190/200............. Loss: 15148.4296875.............me : 0.21612270129122374 grad : 0.00901032052934169887\n",
      "Epoch: 191/200............. Loss: 15178.388671875.............e : 0.23027416413712967 grad : 0.0060893096961081035\n",
      "Epoch: 192/200............. Loss: 15162.0107421875.............0.21844179001541525 grad : 0.0012914687395095825337\n",
      "Epoch: 193/200............. Loss: 15178.349609375............. 0.2188134607761392 grad : 0.00089105067308992159-05\n",
      "Epoch: 194/200............. Loss: 15151.908203125.............e : 0.22513580359295263 grad : 0.0019166786223649979\n",
      "Epoch: 195/200............. Loss: 15149.8642578125............. : 0.21773225998078002 grad : 0.0114825936034321782\n",
      "Epoch: 196/200............. Loss: 15172.7294921875............. : 0.21767560941045255 grad : 0.0008698491728864614\n",
      "Epoch: 197/200............. Loss: 15162.5380859375............. : 0.21939740280872533 grad : 0.0060431896708905716\n",
      "Epoch: 198/200............. Loss: 15150.208984375.............e : 0.22124070048008634 grad : 0.00370431062765419558\n",
      "Epoch: 199/200............. Loss: 15184.4638671875.............0.2231877213326818 grad : 0.002792121842503547731704\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "test() got multiple values for argument 'usage_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [102], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m out_attention \u001b[39m=\u001b[39m train(source_fast_model, target_fast_model, n_epoch \u001b[39m=\u001b[39m \u001b[39m200\u001b[39m, batch_size \u001b[39m=\u001b[39m batch_size, usage_size \u001b[39m=\u001b[39m usage_size, lr \u001b[39m=\u001b[39m \u001b[39m0.001\u001b[39m, neural \u001b[39m=\u001b[39m RNN_attention)\n\u001b[0;32m      2\u001b[0m torch\u001b[39m.\u001b[39msave(out_attention[\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mstate_dict(), sys\u001b[39m.\u001b[39mpath[\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/models/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mword2vec_trained_model.pt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m show (out_attention, usage_size \u001b[39m=\u001b[39m usage_size)\n",
      "Cell \u001b[1;32mIn [79], line 9\u001b[0m, in \u001b[0;36mshow\u001b[1;34m(train_out, usage_size)\u001b[0m\n\u001b[0;32m      6\u001b[0m epoch_lr \u001b[39m=\u001b[39m train_out[\u001b[39m1\u001b[39m]\n\u001b[0;32m      8\u001b[0m model\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m----> 9\u001b[0m test(model, target_fast_model, usage_size \u001b[39m=\u001b[39;49m usage_size)\n\u001b[0;32m     10\u001b[0m \u001b[39m#print loss and learning rate on the same graph\u001b[39;00m\n\u001b[0;32m     11\u001b[0m plt\u001b[39m.\u001b[39mplot(epoch_loss, label\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: test() got multiple values for argument 'usage_size'"
     ]
    }
   ],
   "source": [
    "out_attention = train(source_fast_model, target_fast_model, n_epoch = 200, batch_size = batch_size, usage_size = usage_size, lr = 0.001, neural = RNN_attention)\n",
    "torch.save(out_attention[2].state_dict(), sys.path[0] + \"/models/\" + \"word2vec_trained_model.pt\")\n",
    "show (out_attention, usage_size = usage_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('thesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "fefcaccf17e39639418275c4a17d5ca0413e9c7c1af2b5e38e9064532ca76b63"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
