{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will use an RNN architecture to build a Machine Translation model.\n",
    "\n",
    "It will use as a corpus wikipedia dumps.\n",
    "\n",
    "Either the source or the target will be English. We will, in our case, try English to French Translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test samples location and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embedding\n",
    "\n",
    "# We will use three different types of word embeddings:\n",
    "# 1. Word2Vec\n",
    "# 2. GloVe\n",
    "# 3. FastText\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"## Word2Vec\"\"\"\n",
    "\n",
    "import io\n",
    "import os\n",
    "\n",
    "# Python program to generate word vectors using Word2Vec\n",
    " \n",
    "# importing all necessary modules\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import download\n",
    "import warnings\n",
    " \n",
    "warnings.filterwarnings(action = 'ignore')\n",
    " \n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gille\\AppData\\Local\\Temp\\ipykernel_10576\n",
      "c:\\Users\\gille\\OneDrive\\Desktop\\web\\webtextanalysis\n",
      "c:\\Users\\gille\\miniconda3\\envs\\thesis\\lib\\site-packages\\ipykernel_launcher.py\n"
     ]
    }
   ],
   "source": [
    "#keep in mind you have to launch the notebook inside the git folder to make this work (second one)\n",
    "from inspect import getsourcefile\n",
    "import sys\n",
    "print(os.path.dirname(getsourcefile(lambda:0)))\n",
    "print(sys.path[0])\n",
    "print(os.path.abspath(sys.argv[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to D:\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We need to download the punkt package for tokenizing sentences\n",
    "download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(path = sys.path[0] + \"\\\\samples\\\\alice.txt\"):\n",
    "    #  Reads ‘alice.txt’ file\n",
    "    with io.open(path, 'r',encoding='utf8') as sample :\n",
    "        s = sample.read()\n",
    "        \n",
    "        # Replaces escape character with space\n",
    "        f = s.replace(\"\\n\", \" \")\n",
    "        \n",
    "        data = []\n",
    "        \n",
    "        # iterate through each sentence in the file\n",
    "        for i in sent_tokenize(f):\n",
    "            temp = []\n",
    "            \n",
    "            # tokenize the sentence into words\n",
    "            for j in word_tokenize(i):\n",
    "                temp.append(j.lower())\n",
    "        \n",
    "            data.append(temp)\n",
    "\n",
    "    return data\n",
    "\n",
    "def test_similarity(model, word1, word2, model_name):\n",
    "    print(\"Cosine similarity between '\" + word1 + \"' and '\"+ word2 +\"' - \" + model_name + \" : \" + str(model.similarity(word1, word2)))\n",
    "\n",
    "embedding_data = tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(sys.path[0] + \"\\\\samples\\\\alice_tokenised.txt\", 'w',encoding='utf8') as f:\n",
    "    for line in embedding_data:\n",
    "        f.write(\" \".join(line) + \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'alice' and 'wonderland' - CBOW : 0.97200704\n",
      "Cosine similarity between 'alice' and 'machines' - CBOW : 0.8724395\n",
      "Cosine similarity between 'alice' and 'wonderland' - SkipGram : 0.66590977\n",
      "Cosine similarity between 'alice' and 'machines' - SkipGram : 0.83266664\n"
     ]
    }
   ],
   "source": [
    "\"\"\"## Word2Vec\"\"\"\n",
    " \n",
    "# Create CBOW model\n",
    "w2v_model_cbow = gensim.models.Word2Vec(embedding_data, min_count = 1,\n",
    "                              vector_size = 100, window = 5)\n",
    " \n",
    "# Print results\n",
    "test_similarity(w2v_model_cbow.wv, 'alice', 'wonderland', \"CBOW\")\n",
    "     \n",
    "test_similarity(w2v_model_cbow.wv, 'alice', 'machines', \"CBOW\")\n",
    " \n",
    "# Create Skip Gram model\n",
    "w2v_model_skip = gensim.models.Word2Vec(embedding_data, min_count = 1, vector_size = 100,\n",
    "                                             window = 5, sg = 1)\n",
    " \n",
    "# Print results\n",
    "test_similarity(w2v_model_skip.wv, 'alice', 'wonderland', \"SkipGram\")\n",
    "     \n",
    "test_similarity(w2v_model_skip.wv, 'alice', 'machines', \"SkipGram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"## GloVe\"\"\"\n",
    "\n",
    "# coding: utf-8\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3343, 300)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Only do this once\n",
    "input_file = sys.path[0] + '\\\\models\\\\alice_glove.txt'\n",
    "output_file = sys.path[0] + '\\\\models\\\\gensim_alice_glove.txt'\n",
    "glove2word2vec(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='python3.10 c:\\\\Users\\\\gille\\\\OneDrive\\\\Desktop\\\\web\\\\webtextanalysis/glove_run.py', returncode=1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#once we have the tokenized file, we can call the glove model\n",
    "\n",
    "####CALL FROM BASH glove_run.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model, can take a bit of time\n",
    "output_file = sys.path[0] + '\\\\models\\\\gensim_alice_glove.txt'\n",
    "glove_model = KeyedVectors.load_word2vec_format(output_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'alice' and 'wonderland' - GloVe : -0.21529882\n",
      "Cosine similarity between 'alice' and 'machines' - GloVe : -0.8179076\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test the model\n",
    "test_similarity(glove_model, 'alice', 'wonderland', \"GloVe\")\n",
    "test_similarity(glove_model, 'alice', 'machines', \"GloVe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"## FastText\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can create the RNN model that will translate from english to french using one of the previous embeddings\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From this model we can create a loss function and an optimizer\n",
    "\n",
    "def loss_function(tag_scores, gold_tags):\n",
    "    loss_function = nn.NLLLoss()\n",
    "    loss = loss_function(tag_scores, gold_tags)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can train the model\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = RNN(100, 128, 100, 100).to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "n_epoch = 100\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    for sentence, tags in data:\n",
    "        sentence = torch.tensor(sentence, dtype=torch.long).to(device)\n",
    "        tags = torch.tensor(tags, dtype=torch.long).to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        tag_scores = model(sentence)\n",
    "\n",
    "        loss = loss_function(tag_scores, tags)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    #here we can use the test data to evaluate the model\n",
    "    \n",
    "    losses = torch.zeros(len(test_data))\n",
    "    \n",
    "    for sentence, tags in test_data:\n",
    "        sentence = torch.tensor(sentence, dtype=torch.long).to(device)\n",
    "        tags = torch.tensor(tags, dtype=torch.long).to(device)\n",
    "\n",
    "        tag_scores = model(sentence)\n",
    "\n",
    "        loss = loss_function(tag_scores, tags)\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "    print(\"Epoch \" + str(epoch) + \" : \" + str(losses.mean()))\n",
    "    print(\"Std : \" + str(losses.std()))\n",
    "        \n",
    "\n",
    "    print(\"Epoch: {}/{}.............\".format(epoch, n_epoch), end=\" \")\n",
    "    print(\"Loss: {:.4f}\".format(loss.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('thesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fefcaccf17e39639418275c4a17d5ca0413e9c7c1af2b5e38e9064532ca76b63"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
