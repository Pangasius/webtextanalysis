{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will use an RNN architecture to build a Machine Translation model.\n",
    "\n",
    "It will use as a corpus wikipedia dumps.\n",
    "\n",
    "Either the source or the target will be English. We will, in our case, try English to French Translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\gille\\miniconda3\\envs\\thesis\\lib\\site-packages (1.10.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\gille\\miniconda3\\envs\\thesis\\lib\\site-packages (1.23.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\gille\\miniconda3\\envs\\thesis\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: gensim in c:\\users\\gille\\miniconda3\\envs\\thesis\\lib\\site-packages (4.2.0)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
      "     ---------------------------------------- 5.5/5.5 MB 7.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\gille\\miniconda3\\envs\\thesis\\lib\\site-packages (from torch) (4.3.0)\n",
      "Requirement already satisfied: click in c:\\users\\gille\\miniconda3\\envs\\thesis\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\gille\\miniconda3\\envs\\thesis\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\gille\\miniconda3\\envs\\thesis\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: joblib in c:\\users\\gille\\miniconda3\\envs\\thesis\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\gille\\miniconda3\\envs\\thesis\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\gille\\miniconda3\\envs\\thesis\\lib\\site-packages (from gensim) (1.8.1)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.8.0-py3-none-any.whl (10 kB)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp310-cp310-win_amd64.whl (151 kB)\n",
      "     -------------------------------------- 151.7/151.7 kB 4.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\gille\\miniconda3\\envs\\thesis\\lib\\site-packages (from transformers) (21.3)\n",
      "Collecting requests\n",
      "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
      "     ---------------------------------------- 62.8/62.8 kB 3.5 MB/s eta 0:00:00\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp310-cp310-win_amd64.whl (3.3 MB)\n",
      "     ---------------------------------------- 3.3/3.3 MB 7.5 MB/s eta 0:00:00\n",
      "Collecting huggingface-hub<1.0,>=0.10.0\n",
      "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
      "     -------------------------------------- 163.5/163.5 kB 4.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\gille\\miniconda3\\envs\\thesis\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\gille\\miniconda3\\envs\\thesis\\lib\\site-packages (from tqdm->nltk) (0.4.5)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
      "     -------------------------------------- 140.4/140.4 kB 4.2 MB/s eta 0:00:00\n",
      "Collecting charset-normalizer<3,>=2\n",
      "  Downloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gille\\miniconda3\\envs\\thesis\\lib\\site-packages (from requests->transformers) (2022.9.24)\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
      "     ---------------------------------------- 61.5/61.5 kB 3.4 MB/s eta 0:00:00\n",
      "Installing collected packages: tokenizers, urllib3, pyyaml, idna, filelock, charset-normalizer, requests, huggingface-hub, transformers\n",
      "Successfully installed charset-normalizer-2.1.1 filelock-3.8.0 huggingface-hub-0.10.1 idna-3.4 pyyaml-6.0 requests-2.28.1 tokenizers-0.13.2 transformers-4.24.0 urllib3-1.26.12\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "!pip3 install torch numpy nltk gensim transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'fra.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfra.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 5\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[0;32m      8\u001b[0m source \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'fra.txt'"
     ]
    }
   ],
   "source": [
    "#Test samples location and preprocessing\n",
    "import numpy as np\n",
    "\n",
    "data_path = \"fra.txt\"\n",
    "data = open(data_path, \"r\", encoding='utf-8')\n",
    "\n",
    "# Load data\n",
    "source = []\n",
    "target = []\n",
    "\n",
    "for line in data:\n",
    "    # Discard empty lines if any\n",
    "    if line:\n",
    "        l = line.split(\"\\t\")\n",
    "        source.append(l[0])\n",
    "        target.append(l[1])\n",
    "\n",
    "# Close file\n",
    "data.close()\n",
    "\n",
    "def remove_duplicates(source, target):\n",
    "    new_source = [source[0]]\n",
    "    new_target = [target[0]]\n",
    "    for i in range(1, len(source)):\n",
    "        if source[i] != source[i-1]:\n",
    "            new_source.append(source[i])\n",
    "            new_target.append(target[i])\n",
    "            \n",
    "    return new_source, new_target\n",
    "\n",
    "source, target = remove_duplicates(source, target)\n",
    "print(\"Some dataset statistics:\\n\")\n",
    "\n",
    "print(\"Number of sentences: {}\\n\".format(len(source)))\n",
    "\n",
    "source_word_count = [len(sentence) for sentence in source]\n",
    "target_word_count = [len(sentence) for sentence in target]\n",
    "\n",
    "print(\"Total number of words (source): {}\".format(np.sum(source_word_count)))\n",
    "print(\"Total number of words (target): {}\\n\".format(np.sum(target_word_count)))\n",
    "\n",
    "print(\"Average number of words per sentence (source): {}\".format(np.mean(source_word_count)))\n",
    "print(\"Average number of words per sentence (target): {}\\n\".format(np.mean(target_word_count)))\n",
    "\n",
    "unique_source_words = []\n",
    "for sentence in source:\n",
    "    for word in sentence:\n",
    "        if word not in unique_source_words:\n",
    "            unique_source_words.append(word)\n",
    "            \n",
    "unique_target_words = []\n",
    "for sentence in target:\n",
    "    for word in sentence:\n",
    "        if word not in unique_target_words:\n",
    "            unique_target_words.append(word)\n",
    "            \n",
    "print(\"Number of unique words (source): {}\".format(len(unique_source_words)))\n",
    "print(\"Number of unique words (target): {}\".format(len(unique_target_words)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embedding\n",
    "\n",
    "# We will use three different types of word embeddings:\n",
    "# 1. Word2Vec\n",
    "# 2. GloVe\n",
    "# 3. FastText\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"## Word2Vec\"\"\"\n",
    "\n",
    "import io\n",
    "import os\n",
    "\n",
    "# Python program to generate word vectors using Word2Vec\n",
    " \n",
    "# importing all necessary modules\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import download\n",
    "import warnings\n",
    " \n",
    "warnings.filterwarnings(action = 'ignore')\n",
    " \n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gille\\AppData\\Local\\Temp\\ipykernel_9472\n",
      "c:\\Users\\gille\\OneDrive\\Desktop\\web\\webtextanalysis\n",
      "c:\\Users\\gille\\miniconda3\\envs\\thesis\\lib\\site-packages\\ipykernel_launcher.py\n"
     ]
    }
   ],
   "source": [
    "#keep in mind you have to launch the notebook inside the git folder to make this work (second one)\n",
    "from inspect import getsourcefile\n",
    "import sys\n",
    "print(os.path.dirname(getsourcefile(lambda:0)))\n",
    "print(sys.path[0])\n",
    "print(os.path.abspath(sys.argv[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to D:\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We need to download the punkt package for tokenizing sentences\n",
    "download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(path = sys.path[0] + \"\\\\samples\\\\alice.txt\"):\n",
    "    #  Reads ‘alice.txt’ file\n",
    "    with io.open(path, 'r',encoding='utf8') as sample :\n",
    "        s = sample.read()\n",
    "        \n",
    "        # Replaces escape character with space\n",
    "        f = s.replace(\"\\n\", \" \")\n",
    "        \n",
    "        data = []\n",
    "        \n",
    "        # iterate through each sentence in the file\n",
    "        for i in sent_tokenize(f):\n",
    "            temp = []\n",
    "            \n",
    "            # tokenize the sentence into words\n",
    "            for j in word_tokenize(i):\n",
    "                temp.append(j.lower())\n",
    "        \n",
    "            data.append(temp)\n",
    "\n",
    "    return data\n",
    "\n",
    "def test_similarity(model, word1, word2, model_name):\n",
    "    print(\"Cosine similarity between '\" + word1 + \"' and '\"+ word2 +\"' - \" + model_name + \" : \" + str(model.similarity(word1, word2)))\n",
    "    \n",
    "def Fast_similarity(model, word1, word2):\n",
    "    print(\"Cosine similarity between '\" + word1 + \"' and '\"+ word2 +\"' - \" + \"FastText\" + \" : \" + str(model.wv.similarity(word1, word2)))\n",
    "\n",
    "embedding_data = tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(sys.path[0] + \"\\\\samples\\\\alice_tokenised.txt\", 'w',encoding='utf8') as f:\n",
    "    for line in embedding_data:\n",
    "        f.write(\" \".join(line) + \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'alice' and 'wonderland' - CBOW : 0.9727886\n",
      "Cosine similarity between 'alice' and 'machines' - CBOW : 0.8968569\n",
      "Cosine similarity between 'alice' and 'wonderland' - SkipGram : 0.6006849\n",
      "Cosine similarity between 'alice' and 'machines' - SkipGram : 0.81720406\n"
     ]
    }
   ],
   "source": [
    "\"\"\"## Word2Vec\"\"\"\n",
    " \n",
    "# Create CBOW model\n",
    "w2v_model_cbow = gensim.models.Word2Vec(embedding_data, min_count = 1,\n",
    "                              vector_size = 100, window = 5)\n",
    " \n",
    "# Print results\n",
    "test_similarity(w2v_model_cbow.wv, 'alice', 'wonderland', \"CBOW\")\n",
    "     \n",
    "test_similarity(w2v_model_cbow.wv, 'alice', 'machines', \"CBOW\")\n",
    " \n",
    "# Create Skip Gram model\n",
    "w2v_model_skip = gensim.models.Word2Vec(embedding_data, min_count = 1, vector_size = 100,\n",
    "                                             window = 5, sg = 1)\n",
    " \n",
    "# Print results\n",
    "test_similarity(w2v_model_skip.wv, 'alice', 'wonderland', \"SkipGram\")\n",
    "     \n",
    "test_similarity(w2v_model_skip.wv, 'alice', 'machines', \"SkipGram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"## GloVe\"\"\"\n",
    "\n",
    "# coding: utf-8\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3390, 300)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Only do this once\n",
    "input_file = sys.path[0] + '\\\\models\\\\alice_glove.txt'\n",
    "output_file = sys.path[0] + '\\\\models\\\\gensim_alice_glove.txt'\n",
    "glove2word2vec(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#once we have the tokenized file, we can call the glove model\n",
    "\n",
    "####CALL FROM BASH glove_run.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model, can take a bit of time\n",
    "output_file = sys.path[0] + '\\\\models\\\\gensim_alice_glove.txt'\n",
    "glove_model = KeyedVectors.load_word2vec_format(output_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'alice' and 'wonderland' - GloVe : -0.18298031\n",
      "Cosine similarity between 'alice' and 'machines' - GloVe : -0.8092102\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test the model\n",
    "test_similarity(glove_model, 'alice', 'wonderland', \"GloVe\")\n",
    "test_similarity(glove_model, 'alice', 'machines', \"GloVe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'alice' and 'wonderland' - FastText : 0.18689592\n",
      "Cosine similarity between 'alice' and 'machine' - FastText : -0.12982063\n"
     ]
    }
   ],
   "source": [
    "\"\"\"## FastText\"\"\"\n",
    "from gensim.models import FastText\n",
    "from gensim.test.utils import common_texts  # some example sentences\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "model = FastText(vector_size=100, window=5, min_count=1)  # instantiate\n",
    "model.build_vocab(corpus_iterable=embedding_data)\n",
    "model.train(corpus_iterable=embedding_data, total_examples=len(embedding_data), epochs=100)  \n",
    "Fast_similarity(model,'alice','wonderland')\n",
    "Fast_similarity(model,'alice','machine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can create the RNN model that will translate from english to french using one of the previous embeddings\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From this model we can create a loss function and an optimizer\n",
    "\n",
    "def loss_function(tag_scores, gold_tags):\n",
    "    loss_function = nn.NLLLoss()\n",
    "    loss = loss_function(tag_scores, gold_tags)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can train the model\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = RNN(100, 128, 100, 100).to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "n_epoch = 100\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    for sentence, tags in data:\n",
    "        sentence = torch.tensor(sentence, dtype=torch.long).to(device)\n",
    "        tags = torch.tensor(tags, dtype=torch.long).to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        tag_scores = model(sentence)\n",
    "\n",
    "        loss = loss_function(tag_scores, tags)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    #here we can use the test data to evaluate the model\n",
    "    \n",
    "    losses = torch.zeros(len(test_data))\n",
    "    \n",
    "    for sentence, tags in test_data:\n",
    "        sentence = torch.tensor(sentence, dtype=torch.long).to(device)\n",
    "        tags = torch.tensor(tags, dtype=torch.long).to(device)\n",
    "\n",
    "        tag_scores = model(sentence)\n",
    "\n",
    "        loss = loss_function(tag_scores, tags)\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "    print(\"Epoch \" + str(epoch) + \" : \" + str(losses.mean()))\n",
    "    print(\"Std : \" + str(losses.std()))\n",
    "        \n",
    "\n",
    "    print(\"Epoch: {}/{}.............\".format(epoch, n_epoch), end=\" \")\n",
    "    print(\"Loss: {:.4f}\".format(loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this model we can now try to add contextual embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for contextual embedding we will use BERT\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "#we use bert and we will train it on the data\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "#we will use the tokenizer to tokenize the sentences\n",
    "sentences = [\"I love machine learning\", \"I love coding in python\"]\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "fefcaccf17e39639418275c4a17d5ca0413e9c7c1af2b5e38e9064532ca76b63"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
