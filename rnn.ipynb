{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will use an RNN architecture to build a Machine Translation model.\n",
    "\n",
    "It will use as a corpus wikipedia dumps.\n",
    "\n",
    "Either the source or the target will be English. We will, in our case, try English to French Translation."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'fra.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfra.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 5\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[0;32m      8\u001b[0m source \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'fra.txt'"
     ]
    }
   ],
   "source": [
    "#Test samples location and preprocessing\n",
    "import numpy as np\n",
    "\n",
    "data_path = \"fra.txt\"\n",
    "data = open(data_path, \"r\", encoding='utf-8')\n",
    "\n",
    "# Load data\n",
    "source = []\n",
    "target = []\n",
    "\n",
    "for line in data:\n",
    "    # Discard empty lines if any\n",
    "    if line:\n",
    "        l = line.split(\"\\t\")\n",
    "        source.append(l[0])\n",
    "        target.append(l[1])\n",
    "\n",
    "# Close file\n",
    "data.close()\n",
    "\n",
    "def remove_duplicates(source, target):\n",
    "    new_source = [source[0]]\n",
    "    new_target = [target[0]]\n",
    "    for i in range(1, len(source)):\n",
    "        if source[i] != source[i-1]:\n",
    "            new_source.append(source[i])\n",
    "            new_target.append(target[i])\n",
    "            \n",
    "    return new_source, new_target\n",
    "\n",
    "source, target = remove_duplicates(source, target)\n",
    "print(\"Some dataset statistics:\\n\")\n",
    "\n",
    "print(\"Number of sentences: {}\\n\".format(len(source)))\n",
    "\n",
    "source_word_count = [len(sentence) for sentence in source]\n",
    "target_word_count = [len(sentence) for sentence in target]\n",
    "\n",
    "print(\"Total number of words (source): {}\".format(np.sum(source_word_count)))\n",
    "print(\"Total number of words (target): {}\\n\".format(np.sum(target_word_count)))\n",
    "\n",
    "print(\"Average number of words per sentence (source): {}\".format(np.mean(source_word_count)))\n",
    "print(\"Average number of words per sentence (target): {}\\n\".format(np.mean(target_word_count)))\n",
    "\n",
    "unique_source_words = []\n",
    "for sentence in source:\n",
    "    for word in sentence:\n",
    "        if word not in unique_source_words:\n",
    "            unique_source_words.append(word)\n",
    "            \n",
    "unique_target_words = []\n",
    "for sentence in target:\n",
    "    for word in sentence:\n",
    "        if word not in unique_target_words:\n",
    "            unique_target_words.append(word)\n",
    "            \n",
    "print(\"Number of unique words (source): {}\".format(len(unique_source_words)))\n",
    "print(\"Number of unique words (target): {}\".format(len(unique_target_words)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embedding\n",
    "\n",
    "# We will use three different types of word embeddings:\n",
    "# 1. Word2Vec\n",
    "# 2. GloVe\n",
    "# 3. FastText\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"## Word2Vec\"\"\"\n",
    "\n",
    "import io\n",
    "import os\n",
    "\n",
    "# Python program to generate word vectors using Word2Vec\n",
    " \n",
    "# importing all necessary modules\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import download\n",
    "import warnings\n",
    " \n",
    "warnings.filterwarnings(action = 'ignore')\n",
    " \n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrie\\AppData\\Local\\Temp\\ipykernel_10632\n",
      "C:\\Users\\adrie\\OneDrive\\Bureau\\webtextanalysis\n",
      "C:\\Users\\adrie\\miniconda3\\envs\\fondation\\lib\\site-packages\\ipykernel_launcher.py\n"
     ]
    }
   ],
   "source": [
    "#keep in mind you have to launch the notebook inside the git folder to make this work (second one)\n",
    "from inspect import getsourcefile\n",
    "import sys\n",
    "print(os.path.dirname(getsourcefile(lambda:0)))\n",
    "print(sys.path[0])\n",
    "print(os.path.abspath(sys.argv[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\adrie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We need to download the punkt package for tokenizing sentences\n",
    "download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(path = sys.path[0] + \"\\\\samples\\\\alice.txt\"):\n",
    "    #  Reads ‘alice.txt’ file\n",
    "    with io.open(path, 'r',encoding='utf8') as sample :\n",
    "        s = sample.read()\n",
    "        \n",
    "        # Replaces escape character with space\n",
    "        f = s.replace(\"\\n\", \" \")\n",
    "        \n",
    "        data = []\n",
    "        \n",
    "        # iterate through each sentence in the file\n",
    "        for i in sent_tokenize(f):\n",
    "            temp = []\n",
    "            \n",
    "            # tokenize the sentence into words\n",
    "            for j in word_tokenize(i):\n",
    "                temp.append(j.lower())\n",
    "        \n",
    "            data.append(temp)\n",
    "\n",
    "    return data\n",
    "\n",
    "def test_similarity(model, word1, word2, model_name):\n",
    "    print(\"Cosine similarity between '\" + word1 + \"' and '\"+ word2 +\"' - \" + model_name + \" : \" + str(model.similarity(word1, word2)))\n",
    "    \n",
    "def Fast_similarity(model, word1, word2):\n",
    "    print(\"Cosine similarity between '\" + word1 + \"' and '\"+ word2 +\"' - \" + \"FastText\" + \" : \" + str(model.wv.similarity(word1, word2)))\n",
    "\n",
    "embedding_data = tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(sys.path[0] + \"\\\\samples\\\\alice_tokenised.txt\", 'w',encoding='utf8') as f:\n",
    "    for line in embedding_data:\n",
    "        f.write(\" \".join(line) + \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'alice' and 'wonderland' - CBOW : 0.9727886\n",
      "Cosine similarity between 'alice' and 'machines' - CBOW : 0.8968569\n",
      "Cosine similarity between 'alice' and 'wonderland' - SkipGram : 0.6006849\n",
      "Cosine similarity between 'alice' and 'machines' - SkipGram : 0.81720406\n"
     ]
    }
   ],
   "source": [
    "\"\"\"## Word2Vec\"\"\"\n",
    " \n",
    "# Create CBOW model\n",
    "w2v_model_cbow = gensim.models.Word2Vec(embedding_data, min_count = 1,\n",
    "                              vector_size = 100, window = 5)\n",
    " \n",
    "# Print results\n",
    "test_similarity(w2v_model_cbow.wv, 'alice', 'wonderland', \"CBOW\")\n",
    "     \n",
    "test_similarity(w2v_model_cbow.wv, 'alice', 'machines', \"CBOW\")\n",
    " \n",
    "# Create Skip Gram model\n",
    "w2v_model_skip = gensim.models.Word2Vec(embedding_data, min_count = 1, vector_size = 100,\n",
    "                                             window = 5, sg = 1)\n",
    " \n",
    "# Print results\n",
    "test_similarity(w2v_model_skip.wv, 'alice', 'wonderland', \"SkipGram\")\n",
    "     \n",
    "test_similarity(w2v_model_skip.wv, 'alice', 'machines', \"SkipGram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"## GloVe\"\"\"\n",
    "\n",
    "# coding: utf-8\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3390, 300)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Only do this once\n",
    "input_file = sys.path[0] + '\\\\models\\\\alice_glove.txt'\n",
    "output_file = sys.path[0] + '\\\\models\\\\gensim_alice_glove.txt'\n",
    "glove2word2vec(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#once we have the tokenized file, we can call the glove model\n",
    "\n",
    "####CALL FROM BASH glove_run.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model, can take a bit of time\n",
    "output_file = sys.path[0] + '\\\\models\\\\gensim_alice_glove.txt'\n",
    "glove_model = KeyedVectors.load_word2vec_format(output_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'alice' and 'wonderland' - GloVe : -0.18298031\n",
      "Cosine similarity between 'alice' and 'machines' - GloVe : -0.8092102\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test the model\n",
    "test_similarity(glove_model, 'alice', 'wonderland', \"GloVe\")\n",
    "test_similarity(glove_model, 'alice', 'machines', \"GloVe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'alice' and 'machine' - FastText : -0.117291\n"
     ]
    }
   ],
   "source": [
    "\"\"\"## FastText\"\"\"\n",
    "from gensim.models import FastText\n",
    "from gensim.test.utils import common_texts  # some example sentences\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "model = FastText(vector_size=100, window=5, min_count=1)  # instantiate\n",
    "model.build_vocab(corpus_iterable=embedding_data)\n",
    "model.train(corpus_iterable=embedding_data, total_examples=len(embedding_data), epochs=100)  \n",
    "Fast_similarity(model,'alice','machine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can create the RNN model that will translate from english to french using one of the previous embeddings\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From this model we can create a loss function and an optimizer\n",
    "\n",
    "def loss_function(tag_scores, gold_tags):\n",
    "    loss_function = nn.NLLLoss()\n",
    "    loss = loss_function(tag_scores, gold_tags)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can train the model\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = RNN(100, 128, 100, 100).to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "n_epoch = 100\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    for sentence, tags in data:\n",
    "        sentence = torch.tensor(sentence, dtype=torch.long).to(device)\n",
    "        tags = torch.tensor(tags, dtype=torch.long).to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        tag_scores = model(sentence)\n",
    "\n",
    "        loss = loss_function(tag_scores, tags)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    #here we can use the test data to evaluate the model\n",
    "    \n",
    "    losses = torch.zeros(len(test_data))\n",
    "    \n",
    "    for sentence, tags in test_data:\n",
    "        sentence = torch.tensor(sentence, dtype=torch.long).to(device)\n",
    "        tags = torch.tensor(tags, dtype=torch.long).to(device)\n",
    "\n",
    "        tag_scores = model(sentence)\n",
    "\n",
    "        loss = loss_function(tag_scores, tags)\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "    print(\"Epoch \" + str(epoch) + \" : \" + str(losses.mean()))\n",
    "    print(\"Std : \" + str(losses.std()))\n",
    "        \n",
    "\n",
    "    print(\"Epoch: {}/{}.............\".format(epoch, n_epoch), end=\" \")\n",
    "    print(\"Loss: {:.4f}\".format(loss.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "fefcaccf17e39639418275c4a17d5ca0413e9c7c1af2b5e38e9064532ca76b63"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
